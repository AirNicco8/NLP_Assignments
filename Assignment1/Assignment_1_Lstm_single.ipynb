{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1_Lstm_single.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Due to**: 23/12/2021 (dd/mm/yyyy)\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Part-of Speech (POS) tagging as Sequence Labelling using Recurrent Neural Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4_wqPdlBcKS"
      },
      "source": [
        "# Intro\n",
        "\n",
        "In this assignment  we will ask you to perform POS tagging using neural architectures\n",
        "\n",
        "You are asked to follow these steps:\n",
        "*   Download the corpora and split it in training and test sets, structuring a dataframe.\n",
        "*   Embed the words using GloVe embeddings\n",
        "*   Create a baseline model, using a simple neural architecture\n",
        "*   Experiment doing small modifications to the baseline model, choose hyperparameters using the validation set\n",
        "*   Evaluate your two best model\n",
        "*   Analyze the errors of your model\n",
        "\n",
        "\n",
        "**Task**: given a corpus of documents, predict the POS tag for each word\n",
        "\n",
        "**Corpus**:\n",
        "Ignore the numeric value in the third column, use only the words/symbols and its label. \n",
        "The corpus is available at:\n",
        "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
        "\n",
        "**Splits**: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
        "\n",
        "\n",
        "**Features**: you MUST use GloVe embeddings as the only input features to the model.\n",
        "\n",
        "**Splitting**: you can decide to split documents into sentences or not, the choice is yours.\n",
        "\n",
        "**I/O structure**: The input data will have three dimensions: 1-documents/sentences, 2-token, 3-features; for the output there are 2 possibilities: if you use one-hot encoding it will be 1-documents/sentences, 2-token labels, 3-classes, if you use a single integer that indicates the number of the class it will be 1-documents/sentences, 2-token labels.\n",
        "\n",
        "**Baseline**: two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top; the choice of hyper-parameters is yours.\n",
        "\n",
        "**Architectures**: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and adding an additional dense layer; do not mix these variantions.\n",
        "\n",
        "\n",
        "**Training and Experiments**: all the experiments must involve only the training and validation sets.\n",
        "\n",
        "**Evaluation**: in the end, only the two best models of your choice (according to the validation set) must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech. DO NOT CONSIDER THE PUNCTUATION CLASSES.\n",
        "\n",
        "**Metrics**: the metric you must use to evaluate your final model is the F1-macro, WITHOUT considering punctuation/symbols classes; during the training process you can use accuracy because you can't use the F1 metric unless you use a single (gigantic) batch because there is no way to aggregate \"partial\" F1 scores computed on mini-batches.\n",
        "\n",
        "**Discussion and Error Analysis** : verify and discuss if the results on the test sets are coherent with those on the validation set; analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
        "\n",
        "**Report**: you are asked to deliver the code of your experiments and a small pdf report of about 2 pages; the pdf must begin with the names of the people of your team and a small abstract (4-5 lines) that sums up your findings.\n",
        "\n",
        "# Out Of Vocabulary (OOV) terms\n",
        "\n",
        "How to handle words that are not in GloVe vocabulary?\n",
        "You can handle them as you want (random embedding, placeholder, whatever!), but they must be STATIC embeddings (you cannot train them).\n",
        "\n",
        "But there is a very important caveat! As usual, the element of the test set must not influence the elements of the other splits!\n",
        "\n",
        "So, when you compute new embeddings for train+validation, you must forget about test documents.\n",
        "The motivation is to emulate a real-world scenario, where you select and train a model in the first stage, without knowing nothing about the testing environment.\n",
        "\n",
        "For implementation convenience, you CAN use a single vocabulary file/matrix/whatever. The principle of the previous point is that the embeddings inside that file/matrix must be generated independently for train and test splits.\n",
        "\n",
        "Basically in a real-world scenario, this is what would happen:\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Training of the model(s)\n",
        "5. Compute embeddings for terms OOV2 of the validation split \n",
        "6. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "7. Validation of the model(s)\n",
        "8. Compute embeddings for terms OOV3 of the test split \n",
        "9. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2+OOV3\n",
        "10. Testing of the final model\n",
        "\n",
        "In this case, where we already have all the documents, we can simplify the process a bit, but the procedure must remain rigorous.\n",
        "\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Compute embeddings for terms OOV2 of the validation split \n",
        "5. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "6. Compute embeddings for terms OOV3 of the test split \n",
        "7. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2\n",
        "8. Training of the model(s)\n",
        "9. Validation of the model(s)\n",
        "10. Testing of the final model\n",
        "\n",
        "Step 2 and step 6 must be completely independent of each other, for what concerns the method and the documents. But they can rely on the previous vocabulary (V1 for step 2 and V3 for step 6)\n",
        "THEREFORE if a word is present both in the training set and the test split and not in the starting vocabulary, its embedding is computed in step 2) and it is not considered OOV anymore in step 6).\n",
        "\n",
        "# Report\n",
        "The report must not be just a copy and paste of graphs and tables!\n",
        "\n",
        "The report must not be longer than 2 pages and must contain:\n",
        "* The names of the member of your team\n",
        "* A short abstract (4-5 lines) that sum ups everything\n",
        "* A general description of the task you have addressed and how you have addressed it\n",
        "* A short description of the models you have used\n",
        "* Some tables that sum up your findings in validation and test and a discussion of those results\n",
        "* The most relevant findings of your error analysis\n",
        "\n",
        "# Evaluation Criterion\n",
        "\n",
        "The goal of this assignment is not to prove you can find best model ever, but to face a common task, structure it correctly, and follow a correct and rigorous experimental procedure.\n",
        "In other words, we don't care if you final models are awful as long as you have followed the correct procedure and wrote a decent report.\n",
        "\n",
        "The score of the assignment will be computed roughly as follows\n",
        "* 1 point for the general setting of the problem\n",
        "* 1 point for the handling of OOV terms\n",
        "* 1 point for the models\n",
        "* 1 point for train-validation-test procedure\n",
        "* 2 point for the discussion of the results, error analysis, and report\n",
        "\n",
        "This distribution of scores is tentative and we may decide to alter it at any moment.\n",
        "We also reserve the right to assign a small bonus (0.5 points) to any assignment that is particularly worthy. Similarly, in case of grave errors, we may decide to assign an equivalent malus (-0.5 points).\n",
        "\n",
        "# Contacts\n",
        "\n",
        "In case of any doubt, question, issue, or help we highly recommend you to check the [course useful material](https://virtuale.unibo.it/pluginfile.php/1036039/mod_resource/content/2/NLP_Course_Useful_Material.pdf) for additional information, and to use the Virtuale forums to discuss with other students.\n",
        "\n",
        "You can always contact us at the following email addresses. To increase the probability of a prompt response, we reccomend you to write to both the teaching assistants.\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Andrea Galassi -> a.galassi@unibo.it\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it\n",
        "\n",
        "\n",
        "# FAQ\n",
        "* You can use a non-trainable Embedding layer to load the glove embeddings\n",
        "* You can use any library of your choice to implement the networks. Two options are tensorflow/keras or pythorch. Both these libraries have all the classes you need to implement these simple architectures and there are plenty of tutorials around, where you can learn how to use them."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Dataset"
      ],
      "metadata": {
        "id": "COWq7PIOWtYr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emdubW66fnlD"
      },
      "source": [
        "import os, shutil  #  file management\n",
        "import sys \n",
        "import pandas as pd  #  dataframe management\n",
        "import numpy as np  #  data manipulation\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request  #  download files\n",
        "import zipfile  #  unzip files\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B-w2pqyfpF1"
      },
      "source": [
        "from keras import metrics \n",
        "from tensorflow.python.keras.metrics import Metric\n",
        "from sklearn.metrics import f1_score\n",
        "import keras.backend as K"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8prqSTRX-TK",
        "outputId": "8db88e1c-5b44-4859-be35-7c404cf0e59b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3xn1MZPgA_v",
        "outputId": "97bb1c28-85de-43e8-917d-69a7cec747ae"
      },
      "source": [
        "dataset_folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\")\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "    os.makedirs(dataset_folder)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
        "\n",
        "dataset_path = os.path.join(dataset_folder, \"treebank.zip\")\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    urllib.request.urlretrieve(url, dataset_path)\n",
        "    print(\"Successful download\")\n",
        "\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_folder)\n",
        "print(\"Successful extraction\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful extraction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre Processing\n",
        "- splitting"
      ],
      "metadata": {
        "id": "iXl3LpVxW2h5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKbUsrVHhGAG"
      },
      "source": [
        "dataset_name = \"dependency_treebank\"\n",
        "\n",
        "\n",
        "folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\", dataset_name)\n",
        "\n",
        "pre_train = []\n",
        "pre_valid = []\n",
        "pre_test = []\n",
        "i = 1\n",
        "\n",
        "file_list = sorted(os.listdir(folder))\n",
        "\n",
        "for filename in file_list:\n",
        "  file_path = os.path.join(folder, filename)\n",
        "  if os.path.isfile(file_path):\n",
        "    # open the file\n",
        "      text = []\n",
        "      with open(file_path, mode='r', encoding='utf-8') as text_file:\n",
        "        text = text_file.read()\n",
        "        if i <= 100:\n",
        "          pre_train.append(text)\n",
        "        elif i <= 150:\n",
        "          pre_valid.append(text)\n",
        "        else:\n",
        "          pre_test.append(text)  \n",
        "  i+=1\n",
        "\n",
        "tr = []\n",
        "val = []\n",
        "tes = []\n",
        "\n",
        "for paragraph in pre_train:\n",
        "   tr.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_valid:\n",
        "   val.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_test:\n",
        "   tes.append(paragraph.split('\\n'))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUgbTXi-F7Sa"
      },
      "source": [
        "train = []\n",
        "valid = []\n",
        "test = []\n",
        "\n",
        "for i in tr:\n",
        "  for j in i:\n",
        "    train.append(j.split('\\t'))\n",
        "\n",
        "for i in val:\n",
        "  for j in i:\n",
        "    valid.append(j.split('\\t'))\n",
        "\n",
        "for i in tes:\n",
        "  for j in i:\n",
        "    test.append(j.split('\\t'))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmzVaRuQFp0M"
      },
      "source": [
        "train_sentences = []\n",
        "valid_sentences = []\n",
        "test_sentences = []\n",
        "train_tags = []\n",
        "valid_tags = []\n",
        "test_tags = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in train:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    train_sentences.append(s)\n",
        "    train_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in valid:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    valid_sentences.append(s)\n",
        "    valid_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in test:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    test_sentences.append(s)\n",
        "    test_tags.append(t)\n",
        "    s = []\n",
        "    t = []"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB65cUFLIeEr"
      },
      "source": [
        "flat_train = [item for sublist in train_sentences for item in sublist]\n",
        "flat_valid = [item for sublist in valid_sentences for item in sublist]\n",
        "flat_test = [item for sublist in test_sentences for item in sublist]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "- vocabularies\n",
        "- GloVe download"
      ],
      "metadata": {
        "id": "VQsxPZcXW9EF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZO6ga1EGHdh"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "train_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "train_tokenizer.fit_on_texts(train_sentences)                    # fit tokeniser on data\n",
        "train_encoded = train_tokenizer.texts_to_sequences(train_sentences)\n",
        "\n",
        "valid_tokenizer = Tokenizer()       \n",
        "valid_tokenizer.fit_on_texts(valid_sentences)                  # instantiate tokeniser\n",
        "valid_encoded = valid_tokenizer.texts_to_sequences(valid_sentences)\n",
        "\n",
        "test_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "test_tokenizer.fit_on_texts(test_sentences)                    # fit tokeniser on data\n",
        "test_encoded = test_tokenizer.texts_to_sequences(test_sentences)  # use the tokeniser to encode input sequence"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(train_tags)\n",
        "tags_encoded = tag_tokenizer.texts_to_sequences(train_tags)"
      ],
      "metadata": {
        "id": "1nC8ma_jXYtc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMsG0Yw1l50L",
        "outputId": "40e37217-560d-4579-895d-3e4798ede495"
      },
      "source": [
        "voc = list(set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(valid_tokenizer.word_index.keys()) - set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(test_tokenizer.word_index.keys()) - set(voc))\n",
        "print(len(voc))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7404\n",
            "9901\n",
            "10947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS9RwnOtJuas"
      },
      "source": [
        "word_index = dict(zip(voc, range(2, len(voc)+3)))\n",
        "word_index['-PAD-'] = 0\n",
        "word_index['-OOV-'] = 1\n",
        "\n",
        "tags = set([item for sublist in train_tags for item in sublist])\n",
        "\n",
        "tag2index = {t: i + 1 for i, t in enumerate(tags)}\n",
        "tag2index['-PAD-'] = 0 "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoj4Zx51J2MU",
        "outputId": "cd2217a3-9c71-4747-ebaa-0a77c91a4d58"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-16 11:21:40--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-12-16 11:21:40--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-12-16 11:21:40--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  5.15MB/s    in 2m 40s  \n",
            "\n",
            "2021-12-16 11:24:21 (5.13 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc-T6FcLKtS9",
        "outputId": "0def9509-9787-48a0-cd38-294a11c9cb24"
      },
      "source": [
        "path_to_glove_file = os.path.join(os.getcwd(), \"glove.6B.100d.txt\")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding\n",
        "- matrix creation\n",
        "- padding\n",
        "- one hot encoding of tags\n",
        "- punctuation identification"
      ],
      "metadata": {
        "id": "_x9MpHE6XB2l"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0mOStEPMeXt",
        "outputId": "d87fe509-04e9-4f29-ac97-7e5225acbc76"
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        embedding_matrix += np.random.uniform(low=-0.05, high=0.05, size=embedding_dim)\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 10271 words (678 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwoJ4hbYXFPq"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpYrHs0RMtQJ"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8EZM31Js06-",
        "outputId": "c280b68c-e8e1-42d0-9fe8-5b1e304b8e6e"
      },
      "source": [
        "MAX_LENGTH = len(max(train_sentences, key=len))\n",
        "print(MAX_LENGTH)\n",
        "MAX_LENGTH2 = len(max(valid_sentences, key=len))\n",
        "print(MAX_LENGTH2)\n",
        "MAX_LENGTH3 = len(max(test_sentences, key=len))\n",
        "print(MAX_LENGTH3)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "249\n",
            "81\n",
            "58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSRa9SwTuNrX"
      },
      "source": [
        "train_sentences_X, valid_sentences_X, train_tags_y, valid_tags_y = [], [], [], []\n",
        " \n",
        "for s in train_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        "\n",
        "for s in valid_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    valid_sentences_X.append(s_int)\n",
        " \n",
        "for s in train_tags:\n",
        "    train_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "for s in valid_tags:\n",
        "    valid_tags_y.append([tag2index[t] for t in s])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf-XlSrStQx3"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_sentences_X = pad_sequences(valid_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_tags_y = pad_sequences(valid_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)"
      ],
      "metadata": {
        "id": "yc_u-8itYXQB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "point = [tag2index['.']]\n",
        "virg = [tag2index[',']]\n",
        "weird_apex = [tag2index['``']]\n",
        "single_apex = [tag2index[\"''\"]]\n",
        "two_dots = [tag2index[':']]\n",
        "pad = [tag2index['-PAD-']]\n",
        "\n",
        "punct_cat_classes = to_categorical([point, virg, weird_apex, single_apex, two_dots, pad], len(tag2index))\n",
        "punct_cat_classes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tvb1ZbVIYZ4Y",
        "outputId": "fe4e83ae-d5b5-40e0-f506-434b9ee1162f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 1, 46)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cum_tags = np.zeros(46)\n",
        "for i in punct_cat_classes:\n",
        "  print(i[0])\n",
        "  cum_tags += i[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7oHM_vUYf4c",
        "outputId": "4bc58730-14b7-4ead-d4a0-b21a398954c9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "where_tags = np.where(np.logical_not(cum_tags))\n",
        "where_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5uT2QJrYi15",
        "outputId": "f292a82a-9bf0-4c04-9e47-fb01bd5d146e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 2,  4,  5,  6,  7,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
              "        21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 35, 36, 37, 38, 39,\n",
              "        40, 41, 42, 43, 44, 45]),)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_punct_indexes = where_tags[0]\n",
        "n_classes = len(no_punct_indexes)"
      ],
      "metadata": {
        "id": "nPBHVI2HYlxs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "cat_val_tags_y = to_categorical(valid_tags_y, len(tag2index))\n",
        "print(len(cat_train_tags_y), len(cat_val_tags_y))\n",
        "print(len(train_sentences_X), len(valid_sentences_X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SPgQ9YmYnbG",
        "outputId": "3210263d-b355-462a-9ca5-927d31545b97"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1963 1299\n",
            "1963 1299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mini Data Vizualization Inset"
      ],
      "metadata": {
        "id": "426ajTPoYtaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_tags = np.zeros(46)\n",
        "for i in cat_train_tags_y:\n",
        "  for t in i:\n",
        "    distribution_tags += t"
      ],
      "metadata": {
        "id": "Jf_djyP3YvhV"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = sum(distribution_tags[1:])\n",
        "norm = [float(i)/s for i in distribution_tags[1:]]"
      ],
      "metadata": {
        "id": "8jqHRmS_Yyde"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt "
      ],
      "metadata": {
        "id": "-ksL6xl1Y2Sw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(np.arange(len(norm)),norm)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "o4EY5adaY3Z6",
        "outputId": "fa9caaf4-173e-42a7-c15b-9d50cc460e64"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#F1 Metric"
      ],
      "metadata": {
        "id": "yUvMJPihY4nK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric.\n",
        "\n",
        "    Only computes a batch-wise average of precision.\n",
        "\n",
        "    Computes the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric.\n",
        "\n",
        "    Only computes a batch-wise average of recall.\n",
        "\n",
        "    Computes the recall, a metric for multi-label classification of\n",
        "    how many relevant items are selected.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def f1_binary(y_true, y_pred):\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
        "\n",
        "def metric_wrapper(index):\n",
        "  def f1_for_class(y_true, y_pred):\n",
        "    #get only the desired class\n",
        "    true = y_true[:,:,index]\n",
        "    pred = y_pred[:,:,index]\n",
        "    #return dice per class\n",
        "    tmp = f1_binary(true,pred)\n",
        "    return tmp\n",
        "  f1_for_class.__name__ = 'f1_' + str(index)\n",
        "  return f1_for_class\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    result = 0.0\n",
        "    for class_id in no_punct_indexes:\n",
        "        y_true_single_class = y_true[:,:,class_id]\n",
        "        y_pred_single_class = y_pred[:,:,class_id]\n",
        "        f1_single = f1_binary(y_true_single_class, y_pred_single_class)\n",
        "        result += f1_single / float(n_classes)\n",
        "    return result"
      ],
      "metadata": {
        "id": "_4dwoTOuZCmi"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsVGByW_sdec",
        "outputId": "20f289bf-6576-4085-8a82-9cf32e83de18"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(embedding_layer)\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(Dense(len(tag2index)))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy', f1, [metric_wrapper(i) for i in no_punct_indexes]])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 249, 100)          1094900   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 249, 512)         731136    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 249, 46)           23598     \n",
            "                                                                 \n",
            " activation (Activation)     (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,849,634\n",
            "Trainable params: 754,734\n",
            "Non-trainable params: 1,094,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "history = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=40, validation_data=(valid_sentences_X, cat_val_tags_y), callbacks=[callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n45419-PZSR7",
        "outputId": "06a86b5d-43ab-4eb2-84d4-76e380718076"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "16/16 [==============================] - 94s 5s/step - loss: 0.7678 - accuracy: 0.8473 - f1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.3314 - val_accuracy: 0.9198 - val_f1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.3074 - accuracy: 0.9214 - f1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2857 - val_accuracy: 0.9255 - val_f1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.2756 - accuracy: 0.9298 - f1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2650 - val_accuracy: 0.9369 - val_f1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.2546 - accuracy: 0.9408 - f1: 2.6416e-05 - f1_2: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0011 - f1_33: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2453 - val_accuracy: 0.9429 - val_f1: 1.5101e-05 - val_f1_2: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 6.0405e-04 - val_f1_33: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.2337 - accuracy: 0.9446 - f1: 0.0021 - f1_2: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0821 - f1_33: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2234 - val_accuracy: 0.9466 - val_f1: 0.0035 - val_f1_2: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.1399 - val_f1_33: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - 69s 4s/step - loss: 0.2107 - accuracy: 0.9490 - f1: 0.0090 - f1_2: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0284 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.3214 - f1_33: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0092 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2001 - val_accuracy: 0.9512 - val_f1: 0.0156 - val_f1_2: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 7.0497e-04 - val_f1_20: 0.2166 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.3279 - val_f1_33: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0785 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.1875 - accuracy: 0.9534 - f1: 0.0346 - f1_2: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0195 - f1_20: 0.5355 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.4764 - f1_33: 0.0455 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.3088 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1782 - val_accuracy: 0.9553 - val_f1: 0.0495 - val_f1_2: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0697 - val_f1_20: 0.8651 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.4155 - val_f1_33: 0.1350 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.4945 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.1664 - accuracy: 0.9577 - f1: 0.0631 - f1_2: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.1401 - f1_19: 0.1507 - f1_20: 0.8821 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.5345 - f1_33: 0.2222 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.5960 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1593 - val_accuracy: 0.9581 - val_f1: 0.0810 - val_f1_2: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 7.4211e-04 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.5001 - val_f1_19: 0.3307 - val_f1_20: 0.9018 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.5345 - val_f1_33: 0.3211 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.6514 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - 78s 5s/step - loss: 0.1482 - accuracy: 0.9616 - f1: 0.0954 - f1_2: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0927 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0120 - f1_16: 7.1023e-04 - f1_17: 0.0000e+00 - f1_18: 0.7710 - f1_19: 0.3274 - f1_20: 0.9065 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.6045 - f1_33: 0.3694 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.7327 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1436 - val_accuracy: 0.9621 - val_f1: 0.1099 - val_f1_2: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.2769 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0405 - val_f1_16: 0.0177 - val_f1_17: 0.0000e+00 - val_f1_18: 0.9373 - val_f1_19: 0.4387 - val_f1_20: 0.9146 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.6041 - val_f1_33: 0.4367 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0018 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.7293 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - 69s 4s/step - loss: 0.1330 - accuracy: 0.9656 - f1: 0.1245 - f1_2: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.5301 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.1016 - f1_16: 0.0578 - f1_17: 0.0000e+00 - f1_18: 0.9751 - f1_19: 0.4482 - f1_20: 0.9168 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.6451 - f1_33: 0.4666 - f1_35: 0.0000e+00 - f1_36: 0.0346 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.8036 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1305 - val_accuracy: 0.9658 - val_f1: 0.1380 - val_f1_2: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.6857 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.1989 - val_f1_16: 0.1854 - val_f1_17: 0.0000e+00 - val_f1_18: 0.9889 - val_f1_19: 0.5286 - val_f1_20: 0.9241 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.5994 - val_f1_33: 0.5111 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0743 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.8226 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.1205 - accuracy: 0.9694 - f1: 0.1504 - f1_2: 0.0096 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.7820 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.2368 - f1_16: 0.1672 - f1_17: 0.0000e+00 - f1_18: 0.9909 - f1_19: 0.5391 - f1_20: 0.9258 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0592 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.6780 - f1_33: 0.5687 - f1_35: 0.0000e+00 - f1_36: 0.2137 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.8433 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1193 - val_accuracy: 0.9691 - val_f1: 0.1677 - val_f1_2: 0.0333 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.7503 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.2098 - val_f1_16: 0.2633 - val_f1_17: 0.0040 - val_f1_18: 0.9956 - val_f1_19: 0.5744 - val_f1_20: 0.9275 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.3946 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.6394 - val_f1_33: 0.6185 - val_f1_35: 0.0000e+00 - val_f1_36: 0.4642 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.8343 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.1100 - accuracy: 0.9724 - f1: 0.1888 - f1_2: 0.1298 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.8100 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.3265 - f1_16: 0.2826 - f1_17: 0.0284 - f1_18: 0.9968 - f1_19: 0.5878 - f1_20: 0.9273 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.8048 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0031 - f1_32: 0.6985 - f1_33: 0.6356 - f1_35: 0.0000e+00 - f1_36: 0.4514 - f1_37: 0.0000e+00 - f1_38: 0.0020 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.8664 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1101 - val_accuracy: 0.9714 - val_f1: 0.1934 - val_f1_2: 0.0910 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.7690 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.4002 - val_f1_16: 0.3408 - val_f1_17: 0.0382 - val_f1_18: 0.9976 - val_f1_19: 0.5854 - val_f1_20: 0.9293 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.8750 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0095 - val_f1_32: 0.6742 - val_f1_33: 0.6387 - val_f1_35: 0.0000e+00 - val_f1_36: 0.5114 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0045 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.8703 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.1010 - accuracy: 0.9746 - f1: 0.2225 - f1_2: 0.2962 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.8235 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.4633 - f1_16: 0.3706 - f1_17: 0.1492 - f1_18: 0.9972 - f1_19: 0.6227 - f1_20: 0.9324 - f1_21: 0.3355 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0022 - f1_26: 0.0000e+00 - f1_27: 0.9527 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0571 - f1_32: 0.7304 - f1_33: 0.6805 - f1_35: 0.0000e+00 - f1_36: 0.5801 - f1_37: 0.0000e+00 - f1_38: 0.0293 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.8776 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1024 - val_accuracy: 0.9735 - val_f1: 0.2392 - val_f1_2: 0.4050 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.7725 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.3683 - val_f1_16: 0.3821 - val_f1_17: 0.2887 - val_f1_18: 0.9981 - val_f1_19: 0.6647 - val_f1_20: 0.9309 - val_f1_21: 0.6462 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0103 - val_f1_26: 0.0000e+00 - val_f1_27: 0.9641 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.2813 - val_f1_32: 0.6869 - val_f1_33: 0.6846 - val_f1_35: 0.0000e+00 - val_f1_36: 0.5767 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0433 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.8653 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.0936 - accuracy: 0.9767 - f1: 0.2603 - f1_2: 0.4293 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.8208 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.5008 - f1_16: 0.4159 - f1_17: 0.3961 - f1_18: 0.9987 - f1_19: 0.6617 - f1_20: 0.9365 - f1_21: 0.7017 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0638 - f1_26: 0.0275 - f1_27: 0.9886 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.3408 - f1_32: 0.7452 - f1_33: 0.7281 - f1_35: 0.0000e+00 - f1_36: 0.6880 - f1_37: 0.0000e+00 - f1_38: 0.0829 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.8849 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0024 - val_loss: 0.0959 - val_accuracy: 0.9751 - val_f1: 0.2713 - val_f1_2: 0.4335 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.7737 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.5020 - val_f1_16: 0.3860 - val_f1_17: 0.5215 - val_f1_18: 0.9994 - val_f1_19: 0.6806 - val_f1_20: 0.9322 - val_f1_21: 0.8236 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.1288 - val_f1_26: 0.0805 - val_f1_27: 0.9869 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.5853 - val_f1_32: 0.7246 - val_f1_33: 0.6958 - val_f1_35: 0.0000e+00 - val_f1_36: 0.6322 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0823 - val_f1_39: 0.0044 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.8800 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - 69s 4s/step - loss: 0.0872 - accuracy: 0.9781 - f1: 0.2954 - f1_2: 0.5264 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.8263 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.5751 - f1_16: 0.4595 - f1_17: 0.5665 - f1_18: 0.9987 - f1_19: 0.6829 - f1_20: 0.9363 - f1_21: 0.8514 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.2295 - f1_26: 0.1577 - f1_27: 0.9943 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.5995 - f1_32: 0.7563 - f1_33: 0.7347 - f1_35: 0.0000e+00 - f1_36: 0.7177 - f1_37: 0.0000e+00 - f1_38: 0.1739 - f1_39: 0.1242 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.8916 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0135 - val_loss: 0.0901 - val_accuracy: 0.9766 - val_f1: 0.3002 - val_f1_2: 0.5123 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.7737 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.4892 - val_f1_16: 0.4794 - val_f1_17: 0.5698 - val_f1_18: 1.0000 - val_f1_19: 0.7063 - val_f1_20: 0.9339 - val_f1_21: 0.8707 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.3614 - val_f1_26: 0.2711 - val_f1_27: 0.9952 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.6990 - val_f1_32: 0.7150 - val_f1_33: 0.7162 - val_f1_35: 0.0000e+00 - val_f1_36: 0.6758 - val_f1_37: 0.0000e+00 - val_f1_38: 0.1693 - val_f1_39: 0.1814 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.8819 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0049\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.0816 - accuracy: 0.9796 - f1: 0.3254 - f1_2: 0.5825 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.8292 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.6058 - f1_16: 0.5040 - f1_17: 0.6562 - f1_18: 0.9986 - f1_19: 0.7031 - f1_20: 0.9365 - f1_21: 0.8985 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.3444 - f1_26: 0.3657 - f1_27: 0.9950 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.7143 - f1_32: 0.7713 - f1_33: 0.7528 - f1_35: 0.0000e+00 - f1_36: 0.7430 - f1_37: 0.0000e+00 - f1_38: 0.2436 - f1_39: 0.4409 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.8966 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0329 - val_loss: 0.0851 - val_accuracy: 0.9781 - val_f1: 0.3297 - val_f1_2: 0.5591 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.7737 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.5764 - val_f1_16: 0.4865 - val_f1_17: 0.6644 - val_f1_18: 1.0000 - val_f1_19: 0.7026 - val_f1_20: 0.9363 - val_f1_21: 0.9083 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.4851 - val_f1_26: 0.3970 - val_f1_27: 0.9975 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.7782 - val_f1_32: 0.7371 - val_f1_33: 0.7333 - val_f1_35: 0.0000e+00 - val_f1_36: 0.7178 - val_f1_37: 0.0000e+00 - val_f1_38: 0.2723 - val_f1_39: 0.5251 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.8890 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0481\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.0770 - accuracy: 0.9805 - f1: 0.3487 - f1_2: 0.6463 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.8235 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.6452 - f1_16: 0.5289 - f1_17: 0.7196 - f1_18: 0.9987 - f1_19: 0.7136 - f1_20: 0.9394 - f1_21: 0.9234 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.3981 - f1_26: 0.5327 - f1_27: 0.9950 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.7646 - f1_32: 0.7866 - f1_33: 0.7806 - f1_35: 0.0000e+00 - f1_36: 0.7735 - f1_37: 0.0000e+00 - f1_38: 0.3331 - f1_39: 0.6628 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.8995 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0833 - val_loss: 0.0811 - val_accuracy: 0.9788 - val_f1: 0.3435 - val_f1_2: 0.5459 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.7745 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6437 - val_f1_16: 0.4984 - val_f1_17: 0.7207 - val_f1_18: 1.0000 - val_f1_19: 0.7417 - val_f1_20: 0.9412 - val_f1_21: 0.9400 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.5408 - val_f1_26: 0.4844 - val_f1_27: 0.9975 - val_f1_28: 0.0069 - val_f1_30: 0.0000e+00 - val_f1_31: 0.7810 - val_f1_32: 0.7146 - val_f1_33: 0.7448 - val_f1_35: 0.0000e+00 - val_f1_36: 0.7305 - val_f1_37: 0.0000e+00 - val_f1_38: 0.3336 - val_f1_39: 0.6436 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.8955 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0624\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - 69s 4s/step - loss: 0.0730 - accuracy: 0.9816 - f1: 0.3626 - f1_2: 0.6595 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.8305 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.6642 - f1_16: 0.5432 - f1_17: 0.7656 - f1_18: 0.9986 - f1_19: 0.7354 - f1_20: 0.9403 - f1_21: 0.9336 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.4735 - f1_26: 0.5677 - f1_27: 0.9930 - f1_28: 0.0177 - f1_30: 0.0000e+00 - f1_31: 0.8082 - f1_32: 0.7910 - f1_33: 0.7872 - f1_35: 0.0000e+00 - f1_36: 0.7888 - f1_37: 0.0000e+00 - f1_38: 0.4163 - f1_39: 0.7788 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.9049 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.1061 - val_loss: 0.0772 - val_accuracy: 0.9797 - val_f1: 0.3569 - val_f1_2: 0.6482 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.7771 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6157 - val_f1_16: 0.5100 - val_f1_17: 0.7544 - val_f1_18: 1.0000 - val_f1_19: 0.7409 - val_f1_20: 0.9421 - val_f1_21: 0.9501 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.5478 - val_f1_26: 0.5982 - val_f1_27: 0.9975 - val_f1_28: 0.0150 - val_f1_30: 0.0000e+00 - val_f1_31: 0.8145 - val_f1_32: 0.7563 - val_f1_33: 0.7739 - val_f1_35: 0.0000e+00 - val_f1_36: 0.7496 - val_f1_37: 0.0000e+00 - val_f1_38: 0.3439 - val_f1_39: 0.7628 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.8988 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0780\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - 69s 4s/step - loss: 0.0692 - accuracy: 0.9824 - f1: 0.3783 - f1_2: 0.7005 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.8329 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.7047 - f1_16: 0.5691 - f1_17: 0.8064 - f1_18: 0.9988 - f1_19: 0.7476 - f1_20: 0.9411 - f1_21: 0.9425 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.5144 - f1_26: 0.6923 - f1_27: 0.9944 - f1_28: 0.0362 - f1_30: 0.0000e+00 - f1_31: 0.8258 - f1_32: 0.8052 - f1_33: 0.8064 - f1_35: 0.0000e+00 - f1_36: 0.8068 - f1_37: 0.0000e+00 - f1_38: 0.4474 - f1_39: 0.8704 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.9102 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.1802 - val_loss: 0.0740 - val_accuracy: 0.9806 - val_f1: 0.3692 - val_f1_2: 0.6588 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.7906 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6519 - val_f1_16: 0.5995 - val_f1_17: 0.7521 - val_f1_18: 1.0000 - val_f1_19: 0.7519 - val_f1_20: 0.9427 - val_f1_21: 0.9501 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.5866 - val_f1_26: 0.6031 - val_f1_27: 0.9975 - val_f1_28: 0.0426 - val_f1_30: 0.0000e+00 - val_f1_31: 0.8329 - val_f1_32: 0.7570 - val_f1_33: 0.7770 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8074 - val_f1_37: 0.0000e+00 - val_f1_38: 0.3745 - val_f1_39: 0.8299 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.8983 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.1632\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.0661 - accuracy: 0.9832 - f1: 0.3889 - f1_2: 0.7263 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.8427 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.7241 - f1_16: 0.5916 - f1_17: 0.8175 - f1_18: 0.9986 - f1_19: 0.7633 - f1_20: 0.9414 - f1_21: 0.9409 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.5583 - f1_26: 0.7606 - f1_27: 0.9926 - f1_28: 0.0752 - f1_30: 0.0000e+00 - f1_31: 0.8487 - f1_32: 0.8003 - f1_33: 0.8173 - f1_35: 0.0000e+00 - f1_36: 0.8205 - f1_37: 0.0000e+00 - f1_38: 0.5021 - f1_39: 0.8943 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.9131 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.2264 - val_loss: 0.0711 - val_accuracy: 0.9814 - val_f1: 0.3785 - val_f1_2: 0.6784 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.7913 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6634 - val_f1_16: 0.6097 - val_f1_17: 0.7675 - val_f1_18: 1.0000 - val_f1_19: 0.7524 - val_f1_20: 0.9432 - val_f1_21: 0.9501 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.6128 - val_f1_26: 0.6762 - val_f1_27: 0.9975 - val_f1_28: 0.0606 - val_f1_30: 0.0000e+00 - val_f1_31: 0.8318 - val_f1_32: 0.7649 - val_f1_33: 0.7831 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8144 - val_f1_37: 0.0000e+00 - val_f1_38: 0.4904 - val_f1_39: 0.8595 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9106 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.1839\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.0632 - accuracy: 0.9839 - f1: 0.3987 - f1_2: 0.7483 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.8571 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.7415 - f1_16: 0.6216 - f1_17: 0.8448 - f1_18: 0.9985 - f1_19: 0.7668 - f1_20: 0.9429 - f1_21: 0.9444 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.5811 - f1_26: 0.7908 - f1_27: 0.9939 - f1_28: 0.1363 - f1_30: 0.0000e+00 - f1_31: 0.8636 - f1_32: 0.8188 - f1_33: 0.8282 - f1_35: 0.0000e+00 - f1_36: 0.8293 - f1_37: 0.0000e+00 - f1_38: 0.5512 - f1_39: 0.9193 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.9179 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.2525 - val_loss: 0.0686 - val_accuracy: 0.9820 - val_f1: 0.3879 - val_f1_2: 0.7091 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.8261 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6724 - val_f1_16: 0.6065 - val_f1_17: 0.7987 - val_f1_18: 1.0000 - val_f1_19: 0.7684 - val_f1_20: 0.9454 - val_f1_21: 0.9501 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.5959 - val_f1_26: 0.7475 - val_f1_27: 0.9975 - val_f1_28: 0.0714 - val_f1_30: 0.0000e+00 - val_f1_31: 0.8774 - val_f1_32: 0.7695 - val_f1_33: 0.7826 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8255 - val_f1_37: 0.0000e+00 - val_f1_38: 0.4855 - val_f1_39: 0.9041 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9130 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.2681\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - 69s 4s/step - loss: 0.0605 - accuracy: 0.9847 - f1: 0.4068 - f1_2: 0.7605 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.8824 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.7589 - f1_16: 0.6316 - f1_17: 0.8661 - f1_18: 0.9985 - f1_19: 0.7821 - f1_20: 0.9455 - f1_21: 0.9465 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.5936 - f1_26: 0.8382 - f1_27: 0.9941 - f1_28: 0.1836 - f1_30: 0.0000e+00 - f1_31: 0.8748 - f1_32: 0.8227 - f1_33: 0.8331 - f1_35: 0.0000e+00 - f1_36: 0.8374 - f1_37: 0.0000e+00 - f1_38: 0.5338 - f1_39: 0.9426 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.9219 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.3260 - val_loss: 0.0665 - val_accuracy: 0.9826 - val_f1: 0.3967 - val_f1_2: 0.6709 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.8653 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6944 - val_f1_16: 0.6413 - val_f1_17: 0.8016 - val_f1_18: 1.0000 - val_f1_19: 0.7721 - val_f1_20: 0.9458 - val_f1_21: 0.9501 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.6226 - val_f1_26: 0.7722 - val_f1_27: 0.9975 - val_f1_28: 0.1294 - val_f1_30: 0.0000e+00 - val_f1_31: 0.8704 - val_f1_32: 0.7744 - val_f1_33: 0.7924 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8289 - val_f1_37: 0.0000e+00 - val_f1_38: 0.5074 - val_f1_39: 0.9235 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9186 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.3901\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.0584 - accuracy: 0.9851 - f1: 0.4132 - f1_2: 0.7662 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.9083 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.7659 - f1_16: 0.6408 - f1_17: 0.8673 - f1_18: 0.9977 - f1_19: 0.7807 - f1_20: 0.9473 - f1_21: 0.9466 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.5992 - f1_26: 0.8533 - f1_27: 0.9915 - f1_28: 0.2519 - f1_30: 0.0000e+00 - f1_31: 0.8846 - f1_32: 0.8204 - f1_33: 0.8431 - f1_35: 0.0000e+00 - f1_36: 0.8443 - f1_37: 0.0000e+00 - f1_38: 0.5782 - f1_39: 0.9432 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.9246 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.3730 - val_loss: 0.0639 - val_accuracy: 0.9831 - val_f1: 0.4035 - val_f1_2: 0.7014 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.8916 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7289 - val_f1_16: 0.6024 - val_f1_17: 0.8233 - val_f1_18: 1.0000 - val_f1_19: 0.7814 - val_f1_20: 0.9535 - val_f1_21: 0.9572 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.6633 - val_f1_26: 0.8081 - val_f1_27: 0.9975 - val_f1_28: 0.1966 - val_f1_30: 0.0000e+00 - val_f1_31: 0.8767 - val_f1_32: 0.7798 - val_f1_33: 0.8060 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8358 - val_f1_37: 0.0000e+00 - val_f1_38: 0.5503 - val_f1_39: 0.9300 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9198 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.3362\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.0561 - accuracy: 0.9857 - f1: 0.4222 - f1_2: 0.7872 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.9222 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.7799 - f1_16: 0.6661 - f1_17: 0.8943 - f1_18: 0.9986 - f1_19: 0.7915 - f1_20: 0.9478 - f1_21: 0.9606 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.6637 - f1_26: 0.8533 - f1_27: 0.9954 - f1_28: 0.2990 - f1_30: 0.0000e+00 - f1_31: 0.8926 - f1_32: 0.8319 - f1_33: 0.8485 - f1_35: 0.0000e+00 - f1_36: 0.8462 - f1_37: 0.0000e+00 - f1_38: 0.6061 - f1_39: 0.9487 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.9264 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.4268 - val_loss: 0.0623 - val_accuracy: 0.9834 - val_f1: 0.4109 - val_f1_2: 0.7060 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.9073 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7281 - val_f1_16: 0.6220 - val_f1_17: 0.8717 - val_f1_18: 1.0000 - val_f1_19: 0.7961 - val_f1_20: 0.9540 - val_f1_21: 0.9809 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.1001 - val_f1_25: 0.6326 - val_f1_26: 0.8562 - val_f1_27: 0.9975 - val_f1_28: 0.2456 - val_f1_30: 0.0000e+00 - val_f1_31: 0.8876 - val_f1_32: 0.7843 - val_f1_33: 0.8273 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8395 - val_f1_37: 0.0000e+00 - val_f1_38: 0.5121 - val_f1_39: 0.9602 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9187 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.3069\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.0542 - accuracy: 0.9861 - f1: 0.4318 - f1_2: 0.7946 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.9419 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.7962 - f1_16: 0.6705 - f1_17: 0.9082 - f1_18: 0.9986 - f1_19: 0.8014 - f1_20: 0.9501 - f1_21: 0.9706 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.1858 - f1_25: 0.6622 - f1_26: 0.8903 - f1_27: 0.9919 - f1_28: 0.3553 - f1_30: 0.0000e+00 - f1_31: 0.9038 - f1_32: 0.8297 - f1_33: 0.8529 - f1_35: 0.0000e+00 - f1_36: 0.8534 - f1_37: 0.0000e+00 - f1_38: 0.6218 - f1_39: 0.9583 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.9281 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.4084 - val_loss: 0.0605 - val_accuracy: 0.9838 - val_f1: 0.4197 - val_f1_2: 0.7076 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.9056 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6997 - val_f1_16: 0.6591 - val_f1_17: 0.8711 - val_f1_18: 1.0000 - val_f1_19: 0.7986 - val_f1_20: 0.9551 - val_f1_21: 0.9809 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.1273 - val_f1_25: 0.6493 - val_f1_26: 0.8513 - val_f1_27: 0.9975 - val_f1_28: 0.2508 - val_f1_30: 0.0000e+00 - val_f1_31: 0.8970 - val_f1_32: 0.7668 - val_f1_33: 0.8513 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8449 - val_f1_37: 0.0000e+00 - val_f1_38: 0.6048 - val_f1_39: 0.9588 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9205 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.4902\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - 68s 4s/step - loss: 0.0523 - accuracy: 0.9866 - f1: 0.4407 - f1_2: 0.7957 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.9527 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.7964 - f1_16: 0.6802 - f1_17: 0.9122 - f1_18: 0.9986 - f1_19: 0.8074 - f1_20: 0.9504 - f1_21: 0.9783 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.2899 - f1_25: 0.6903 - f1_26: 0.8813 - f1_27: 0.9943 - f1_28: 0.4014 - f1_30: 0.0000e+00 - f1_31: 0.9092 - f1_32: 0.8344 - f1_33: 0.8642 - f1_35: 0.0000e+00 - f1_36: 0.8634 - f1_37: 0.0000e+00 - f1_38: 0.6408 - f1_39: 0.9563 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.9326 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.4984 - val_loss: 0.0595 - val_accuracy: 0.9842 - val_f1: 0.4251 - val_f1_2: 0.7357 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.9285 - val_f1_10: 0.0348 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7726 - val_f1_16: 0.6644 - val_f1_17: 0.8999 - val_f1_18: 1.0000 - val_f1_19: 0.7777 - val_f1_20: 0.9544 - val_f1_21: 0.9809 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.1793 - val_f1_25: 0.7017 - val_f1_26: 0.8655 - val_f1_27: 0.9975 - val_f1_28: 0.2882 - val_f1_30: 0.0000e+00 - val_f1_31: 0.8963 - val_f1_32: 0.7771 - val_f1_33: 0.8319 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8378 - val_f1_37: 0.0000e+00 - val_f1_38: 0.5422 - val_f1_39: 0.9673 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9270 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.4425\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - 69s 4s/step - loss: 0.0508 - accuracy: 0.9870 - f1: 0.4528 - f1_2: 0.8143 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.9557 - f1_10: 0.0809 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8078 - f1_16: 0.6970 - f1_17: 0.9209 - f1_18: 0.9986 - f1_19: 0.8062 - f1_20: 0.9522 - f1_21: 0.9802 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.4631 - f1_25: 0.7022 - f1_26: 0.9054 - f1_27: 0.9939 - f1_28: 0.4587 - f1_30: 0.0000e+00 - f1_31: 0.9182 - f1_32: 0.8390 - f1_33: 0.8820 - f1_35: 0.0000e+00 - f1_36: 0.8652 - f1_37: 0.0000e+00 - f1_38: 0.6555 - f1_39: 0.9671 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.9320 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.5175 - val_loss: 0.0574 - val_accuracy: 0.9846 - val_f1: 0.4384 - val_f1_2: 0.7739 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.9240 - val_f1_10: 0.0915 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7628 - val_f1_16: 0.6619 - val_f1_17: 0.9176 - val_f1_18: 1.0000 - val_f1_19: 0.7753 - val_f1_20: 0.9559 - val_f1_21: 0.9809 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.3827 - val_f1_25: 0.7079 - val_f1_26: 0.8935 - val_f1_27: 0.9975 - val_f1_28: 0.3400 - val_f1_30: 0.0000e+00 - val_f1_31: 0.8992 - val_f1_32: 0.8027 - val_f1_33: 0.8554 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8452 - val_f1_37: 0.0000e+00 - val_f1_38: 0.5785 - val_f1_39: 0.9595 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9324 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.4977\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - 69s 4s/step - loss: 0.0490 - accuracy: 0.9875 - f1: 0.4629 - f1_2: 0.8143 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.9626 - f1_10: 0.1885 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8209 - f1_16: 0.7014 - f1_17: 0.9384 - f1_18: 0.9987 - f1_19: 0.8148 - f1_20: 0.9548 - f1_21: 0.9872 - f1_22: 0.0000e+00 - f1_23: 0.0594 - f1_24: 0.5378 - f1_25: 0.7152 - f1_26: 0.9231 - f1_27: 0.9921 - f1_28: 0.4724 - f1_30: 0.0000e+00 - f1_31: 0.9177 - f1_32: 0.8466 - f1_33: 0.8825 - f1_35: 0.0000e+00 - f1_36: 0.8755 - f1_37: 0.0000e+00 - f1_38: 0.6712 - f1_39: 0.9679 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.9380 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.5338 - val_loss: 0.0561 - val_accuracy: 0.9850 - val_f1: 0.4455 - val_f1_2: 0.7629 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.9316 - val_f1_10: 0.2107 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7491 - val_f1_16: 0.6811 - val_f1_17: 0.9115 - val_f1_18: 1.0000 - val_f1_19: 0.8039 - val_f1_20: 0.9565 - val_f1_21: 0.9819 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.3827 - val_f1_25: 0.7013 - val_f1_26: 0.9126 - val_f1_27: 0.9975 - val_f1_28: 0.3406 - val_f1_30: 0.0000e+00 - val_f1_31: 0.9067 - val_f1_32: 0.7882 - val_f1_33: 0.8546 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8500 - val_f1_37: 0.0000e+00 - val_f1_38: 0.6281 - val_f1_39: 0.9680 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9294 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.5691\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - 69s 4s/step - loss: 0.0478 - accuracy: 0.9877 - f1: 0.4724 - f1_2: 0.8245 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.9629 - f1_10: 0.3680 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8259 - f1_16: 0.7020 - f1_17: 0.9431 - f1_18: 0.9986 - f1_19: 0.8189 - f1_20: 0.9550 - f1_21: 0.9910 - f1_22: 0.0000e+00 - f1_23: 0.0510 - f1_24: 0.6514 - f1_25: 0.7367 - f1_26: 0.9090 - f1_27: 0.9931 - f1_28: 0.4786 - f1_30: 0.0000e+00 - f1_31: 0.9249 - f1_32: 0.8453 - f1_33: 0.8860 - f1_35: 0.0000e+00 - f1_36: 0.8851 - f1_37: 0.0000e+00 - f1_38: 0.6773 - f1_39: 0.9667 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.9400 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.5600 - val_loss: 0.0556 - val_accuracy: 0.9849 - val_f1: 0.4526 - val_f1_2: 0.7688 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.9466 - val_f1_10: 0.4271 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7458 - val_f1_16: 0.6810 - val_f1_17: 0.9092 - val_f1_18: 1.0000 - val_f1_19: 0.8092 - val_f1_20: 0.9570 - val_f1_21: 0.9838 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0455 - val_f1_24: 0.4489 - val_f1_25: 0.6931 - val_f1_26: 0.9262 - val_f1_27: 0.9975 - val_f1_28: 0.3415 - val_f1_30: 0.0000e+00 - val_f1_31: 0.9038 - val_f1_32: 0.8032 - val_f1_33: 0.8290 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8536 - val_f1_37: 0.0000e+00 - val_f1_38: 0.6202 - val_f1_39: 0.9752 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9298 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.5082\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - 71s 5s/step - loss: 0.0463 - accuracy: 0.9880 - f1: 0.4817 - f1_2: 0.8407 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.9651 - f1_10: 0.4267 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8254 - f1_16: 0.7134 - f1_17: 0.9536 - f1_18: 0.9984 - f1_19: 0.8270 - f1_20: 0.9560 - f1_21: 0.9918 - f1_22: 0.0000e+00 - f1_23: 0.1001 - f1_24: 0.7127 - f1_25: 0.7479 - f1_26: 0.9431 - f1_27: 0.9941 - f1_28: 0.5220 - f1_30: 0.0000e+00 - f1_31: 0.9273 - f1_32: 0.8504 - f1_33: 0.8831 - f1_35: 0.0000e+00 - f1_36: 0.8880 - f1_37: 0.0000e+00 - f1_38: 0.6964 - f1_39: 0.9725 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.9414 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.5911 - val_loss: 0.0539 - val_accuracy: 0.9853 - val_f1: 0.4637 - val_f1_2: 0.7860 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.9420 - val_f1_10: 0.4701 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7625 - val_f1_16: 0.6877 - val_f1_17: 0.9211 - val_f1_18: 1.0000 - val_f1_19: 0.8133 - val_f1_20: 0.9589 - val_f1_21: 0.9921 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0939 - val_f1_24: 0.5814 - val_f1_25: 0.6983 - val_f1_26: 0.9069 - val_f1_27: 0.9975 - val_f1_28: 0.4180 - val_f1_30: 0.0000e+00 - val_f1_31: 0.9034 - val_f1_32: 0.7944 - val_f1_33: 0.8676 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8580 - val_f1_37: 0.0000e+00 - val_f1_38: 0.6341 - val_f1_39: 0.9823 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9356 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.5440\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - 69s 4s/step - loss: 0.0450 - accuracy: 0.9883 - f1: 0.4903 - f1_2: 0.8427 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.9652 - f1_10: 0.5637 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8338 - f1_16: 0.7235 - f1_17: 0.9534 - f1_18: 0.9985 - f1_19: 0.8260 - f1_20: 0.9590 - f1_21: 0.9902 - f1_22: 0.0000e+00 - f1_23: 0.1599 - f1_24: 0.7506 - f1_25: 0.7571 - f1_26: 0.9472 - f1_27: 0.9937 - f1_28: 0.5439 - f1_30: 0.0000e+00 - f1_31: 0.9243 - f1_32: 0.8518 - f1_33: 0.8957 - f1_35: 0.0000e+00 - f1_36: 0.8901 - f1_37: 0.0000e+00 - f1_38: 0.7008 - f1_39: 0.9789 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.9441 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.6164 - val_loss: 0.0527 - val_accuracy: 0.9855 - val_f1: 0.4662 - val_f1_2: 0.8004 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.9392 - val_f1_10: 0.4722 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7712 - val_f1_16: 0.6833 - val_f1_17: 0.9421 - val_f1_18: 1.0000 - val_f1_19: 0.8131 - val_f1_20: 0.9613 - val_f1_21: 0.9921 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0939 - val_f1_24: 0.5784 - val_f1_25: 0.6815 - val_f1_26: 0.9126 - val_f1_27: 0.9975 - val_f1_28: 0.4330 - val_f1_30: 0.0000e+00 - val_f1_31: 0.9102 - val_f1_32: 0.7983 - val_f1_33: 0.8717 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8563 - val_f1_37: 0.0000e+00 - val_f1_38: 0.6328 - val_f1_39: 0.9807 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9394 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.5849\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.0437 - accuracy: 0.9886 - f1: 0.4977 - f1_2: 0.8481 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.9679 - f1_10: 0.6003 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8403 - f1_16: 0.7258 - f1_17: 0.9618 - f1_18: 0.9986 - f1_19: 0.8322 - f1_20: 0.9598 - f1_21: 0.9951 - f1_22: 0.0000e+00 - f1_23: 0.3273 - f1_24: 0.7662 - f1_25: 0.7509 - f1_26: 0.9475 - f1_27: 0.9838 - f1_28: 0.5618 - f1_30: 0.0000e+00 - f1_31: 0.9350 - f1_32: 0.8537 - f1_33: 0.9007 - f1_35: 0.0000e+00 - f1_36: 0.8916 - f1_37: 0.0000e+00 - f1_38: 0.7156 - f1_39: 0.9739 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.9474 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.6219 - val_loss: 0.0520 - val_accuracy: 0.9856 - val_f1: 0.4716 - val_f1_2: 0.7900 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.9509 - val_f1_10: 0.5944 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7805 - val_f1_16: 0.6914 - val_f1_17: 0.9440 - val_f1_18: 1.0000 - val_f1_19: 0.8171 - val_f1_20: 0.9639 - val_f1_21: 0.9921 - val_f1_22: 0.0000e+00 - val_f1_23: 0.1156 - val_f1_24: 0.6182 - val_f1_25: 0.7173 - val_f1_26: 0.9382 - val_f1_27: 0.9975 - val_f1_28: 0.3999 - val_f1_30: 0.0000e+00 - val_f1_31: 0.9072 - val_f1_32: 0.8118 - val_f1_33: 0.8659 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8651 - val_f1_37: 0.0000e+00 - val_f1_38: 0.6207 - val_f1_39: 0.9807 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9401 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.5629\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.0426 - accuracy: 0.9888 - f1: 0.5023 - f1_2: 0.8534 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.9712 - f1_10: 0.6851 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8424 - f1_16: 0.7352 - f1_17: 0.9593 - f1_18: 0.9986 - f1_19: 0.8299 - f1_20: 0.9621 - f1_21: 0.9964 - f1_22: 0.0000e+00 - f1_23: 0.3030 - f1_24: 0.7906 - f1_25: 0.7535 - f1_26: 0.9556 - f1_27: 0.9944 - f1_28: 0.5681 - f1_30: 0.0000e+00 - f1_31: 0.9316 - f1_32: 0.8566 - f1_33: 0.9027 - f1_35: 0.0000e+00 - f1_36: 0.9003 - f1_37: 0.0000e+00 - f1_38: 0.7239 - f1_39: 0.9742 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.9463 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.6589 - val_loss: 0.0517 - val_accuracy: 0.9856 - val_f1: 0.4728 - val_f1_2: 0.7910 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.9474 - val_f1_10: 0.5564 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7254 - val_f1_16: 0.6903 - val_f1_17: 0.9631 - val_f1_18: 1.0000 - val_f1_19: 0.8238 - val_f1_20: 0.9656 - val_f1_21: 0.9954 - val_f1_22: 0.0000e+00 - val_f1_23: 0.1156 - val_f1_24: 0.5946 - val_f1_25: 0.7359 - val_f1_26: 0.9437 - val_f1_27: 0.9975 - val_f1_28: 0.4500 - val_f1_30: 0.0000e+00 - val_f1_31: 0.9164 - val_f1_32: 0.8022 - val_f1_33: 0.8625 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8711 - val_f1_37: 0.0000e+00 - val_f1_38: 0.6196 - val_f1_39: 0.9775 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9410 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.6241\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - 69s 4s/step - loss: 0.0419 - accuracy: 0.9890 - f1: 0.5034 - f1_2: 0.8572 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.9747 - f1_10: 0.6116 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8446 - f1_16: 0.7469 - f1_17: 0.9691 - f1_18: 0.9979 - f1_19: 0.8357 - f1_20: 0.9639 - f1_21: 0.9960 - f1_22: 0.0000e+00 - f1_23: 0.3542 - f1_24: 0.7690 - f1_25: 0.7801 - f1_26: 0.9500 - f1_27: 0.9946 - f1_28: 0.5992 - f1_30: 0.0000e+00 - f1_31: 0.9371 - f1_32: 0.8536 - f1_33: 0.9026 - f1_35: 0.0000e+00 - f1_36: 0.8890 - f1_37: 0.0000e+00 - f1_38: 0.7125 - f1_39: 0.9795 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.9500 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.6663 - val_loss: 0.0504 - val_accuracy: 0.9860 - val_f1: 0.4816 - val_f1_2: 0.8244 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.9606 - val_f1_10: 0.7594 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7918 - val_f1_16: 0.6817 - val_f1_17: 0.9640 - val_f1_18: 1.0000 - val_f1_19: 0.8154 - val_f1_20: 0.9663 - val_f1_21: 0.9921 - val_f1_22: 0.0000e+00 - val_f1_23: 0.1358 - val_f1_24: 0.6386 - val_f1_25: 0.7381 - val_f1_26: 0.9635 - val_f1_27: 0.9975 - val_f1_28: 0.4206 - val_f1_30: 0.0000e+00 - val_f1_31: 0.9144 - val_f1_32: 0.8148 - val_f1_33: 0.8737 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8744 - val_f1_37: 0.0000e+00 - val_f1_38: 0.6374 - val_f1_39: 0.9759 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9410 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.5808\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - 69s 4s/step - loss: 0.0407 - accuracy: 0.9893 - f1: 0.5104 - f1_2: 0.8653 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.9762 - f1_10: 0.7411 - f1_11: 0.0069 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8490 - f1_16: 0.7464 - f1_17: 0.9688 - f1_18: 0.9986 - f1_19: 0.8406 - f1_20: 0.9670 - f1_21: 0.9965 - f1_22: 0.0000e+00 - f1_23: 0.4201 - f1_24: 0.7512 - f1_25: 0.7891 - f1_26: 0.9668 - f1_27: 0.9935 - f1_28: 0.5985 - f1_30: 0.0000e+00 - f1_31: 0.9363 - f1_32: 0.8644 - f1_33: 0.9088 - f1_35: 0.0000e+00 - f1_36: 0.9015 - f1_37: 0.0000e+00 - f1_38: 0.7160 - f1_39: 0.9764 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.9503 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.6882 - val_loss: 0.0494 - val_accuracy: 0.9863 - val_f1: 0.4833 - val_f1_2: 0.7890 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.9611 - val_f1_10: 0.6912 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7883 - val_f1_16: 0.7015 - val_f1_17: 0.9567 - val_f1_18: 1.0000 - val_f1_19: 0.8108 - val_f1_20: 0.9670 - val_f1_21: 0.9921 - val_f1_22: 0.0000e+00 - val_f1_23: 0.1573 - val_f1_24: 0.6316 - val_f1_25: 0.7596 - val_f1_26: 0.9369 - val_f1_27: 0.9975 - val_f1_28: 0.4770 - val_f1_30: 0.0000e+00 - val_f1_31: 0.9193 - val_f1_32: 0.8137 - val_f1_33: 0.8752 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8698 - val_f1_37: 0.0000e+00 - val_f1_38: 0.6750 - val_f1_39: 0.9915 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9458 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.6238\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - 68s 4s/step - loss: 0.0398 - accuracy: 0.9894 - f1: 0.5171 - f1_2: 0.8720 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.9767 - f1_10: 0.7660 - f1_11: 0.0192 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8563 - f1_16: 0.7563 - f1_17: 0.9699 - f1_18: 0.9987 - f1_19: 0.8385 - f1_20: 0.9662 - f1_21: 0.9968 - f1_22: 0.0000e+00 - f1_23: 0.5112 - f1_24: 0.8043 - f1_25: 0.8036 - f1_26: 0.9585 - f1_27: 0.9945 - f1_28: 0.6198 - f1_30: 0.0000e+00 - f1_31: 0.9391 - f1_32: 0.8622 - f1_33: 0.9105 - f1_35: 0.0000e+00 - f1_36: 0.9005 - f1_37: 0.0000e+00 - f1_38: 0.7385 - f1_39: 0.9817 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.9527 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.6890 - val_loss: 0.0492 - val_accuracy: 0.9862 - val_f1: 0.4878 - val_f1_2: 0.8136 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.9628 - val_f1_10: 0.7599 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7664 - val_f1_16: 0.7024 - val_f1_17: 0.9539 - val_f1_18: 1.0000 - val_f1_19: 0.8332 - val_f1_20: 0.9690 - val_f1_21: 0.9954 - val_f1_22: 0.0000e+00 - val_f1_23: 0.2396 - val_f1_24: 0.6420 - val_f1_25: 0.7095 - val_f1_26: 0.9591 - val_f1_27: 0.9975 - val_f1_28: 0.4751 - val_f1_30: 0.0000e+00 - val_f1_31: 0.9190 - val_f1_32: 0.8021 - val_f1_33: 0.8703 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8772 - val_f1_37: 0.0000e+00 - val_f1_38: 0.6405 - val_f1_39: 0.9895 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9428 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.6891\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - 69s 4s/step - loss: 0.0389 - accuracy: 0.9896 - f1: 0.5184 - f1_2: 0.8779 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.9794 - f1_10: 0.7368 - f1_11: 0.0460 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8613 - f1_16: 0.7612 - f1_17: 0.9684 - f1_18: 0.9987 - f1_19: 0.8486 - f1_20: 0.9679 - f1_21: 0.9960 - f1_22: 0.0000e+00 - f1_23: 0.5026 - f1_24: 0.8075 - f1_25: 0.7894 - f1_26: 0.9573 - f1_27: 0.9921 - f1_28: 0.6396 - f1_30: 0.0139 - f1_31: 0.9410 - f1_32: 0.8680 - f1_33: 0.9096 - f1_35: 0.0000e+00 - f1_36: 0.9035 - f1_37: 0.0000e+00 - f1_38: 0.7275 - f1_39: 0.9817 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.9526 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.7079 - val_loss: 0.0477 - val_accuracy: 0.9866 - val_f1: 0.4937 - val_f1_2: 0.8007 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.9671 - val_f1_10: 0.8196 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7912 - val_f1_16: 0.6976 - val_f1_17: 0.9676 - val_f1_18: 1.0000 - val_f1_19: 0.8228 - val_f1_20: 0.9703 - val_f1_21: 0.9921 - val_f1_22: 0.0000e+00 - val_f1_23: 0.3453 - val_f1_24: 0.6420 - val_f1_25: 0.7678 - val_f1_26: 0.9570 - val_f1_27: 0.9975 - val_f1_28: 0.4710 - val_f1_30: 0.0000e+00 - val_f1_31: 0.9234 - val_f1_32: 0.8261 - val_f1_33: 0.8826 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8845 - val_f1_37: 0.0000e+00 - val_f1_38: 0.6617 - val_f1_39: 0.9919 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9483 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.6203\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.0379 - accuracy: 0.9899 - f1: 0.5223 - f1_2: 0.8736 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.9818 - f1_10: 0.7542 - f1_11: 0.0524 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8648 - f1_16: 0.7754 - f1_17: 0.9703 - f1_18: 0.9985 - f1_19: 0.8508 - f1_20: 0.9694 - f1_21: 0.9961 - f1_22: 0.0000e+00 - f1_23: 0.5205 - f1_24: 0.7996 - f1_25: 0.8137 - f1_26: 0.9609 - f1_27: 0.9922 - f1_28: 0.6302 - f1_30: 0.0365 - f1_31: 0.9464 - f1_32: 0.8669 - f1_33: 0.9153 - f1_35: 0.0000e+00 - f1_36: 0.9121 - f1_37: 0.0000e+00 - f1_38: 0.7466 - f1_39: 0.9871 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.9574 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.7189 - val_loss: 0.0469 - val_accuracy: 0.9869 - val_f1: 0.4947 - val_f1_2: 0.8179 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.9806 - val_f1_10: 0.8169 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.8121 - val_f1_16: 0.7077 - val_f1_17: 0.9688 - val_f1_18: 1.0000 - val_f1_19: 0.8297 - val_f1_20: 0.9698 - val_f1_21: 0.9921 - val_f1_22: 0.0000e+00 - val_f1_23: 0.2906 - val_f1_24: 0.6420 - val_f1_25: 0.7511 - val_f1_26: 0.9685 - val_f1_27: 0.9975 - val_f1_28: 0.4491 - val_f1_30: 0.0000e+00 - val_f1_31: 0.9218 - val_f1_32: 0.8215 - val_f1_33: 0.8860 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8757 - val_f1_37: 0.0000e+00 - val_f1_38: 0.6647 - val_f1_39: 0.9919 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9486 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.6835\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - 69s 4s/step - loss: 0.0370 - accuracy: 0.9901 - f1: 0.5302 - f1_2: 0.8699 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.9822 - f1_10: 0.8514 - f1_11: 0.1125 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8715 - f1_16: 0.7749 - f1_17: 0.9726 - f1_18: 0.9986 - f1_19: 0.8513 - f1_20: 0.9695 - f1_21: 0.9968 - f1_22: 0.0000e+00 - f1_23: 0.5845 - f1_24: 0.8155 - f1_25: 0.8153 - f1_26: 0.9696 - f1_27: 0.9914 - f1_28: 0.6412 - f1_30: 0.0179 - f1_31: 0.9442 - f1_32: 0.8715 - f1_33: 0.9173 - f1_35: 0.0000e+00 - f1_36: 0.9082 - f1_37: 0.0156 - f1_38: 0.7414 - f1_39: 0.9857 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.9582 - f1_43: 0.0000e+00 - f1_44: 0.0562 - f1_45: 0.7227 - val_loss: 0.0472 - val_accuracy: 0.9867 - val_f1: 0.4923 - val_f1_2: 0.8329 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.9752 - val_f1_10: 0.7111 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7890 - val_f1_16: 0.7073 - val_f1_17: 0.9660 - val_f1_18: 1.0000 - val_f1_19: 0.8227 - val_f1_20: 0.9700 - val_f1_21: 0.9921 - val_f1_22: 0.0000e+00 - val_f1_23: 0.3028 - val_f1_24: 0.6386 - val_f1_25: 0.7737 - val_f1_26: 0.9542 - val_f1_27: 0.9975 - val_f1_28: 0.5038 - val_f1_30: 0.0000e+00 - val_f1_31: 0.9208 - val_f1_32: 0.8207 - val_f1_33: 0.8826 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8693 - val_f1_37: 0.0000e+00 - val_f1_38: 0.6697 - val_f1_39: 0.9953 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9512 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.6442\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - 68s 4s/step - loss: 0.0364 - accuracy: 0.9902 - f1: 0.5318 - f1_2: 0.8770 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.9843 - f1_10: 0.8324 - f1_11: 0.1126 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8712 - f1_16: 0.7769 - f1_17: 0.9747 - f1_18: 0.9973 - f1_19: 0.8535 - f1_20: 0.9710 - f1_21: 0.9965 - f1_22: 0.0000e+00 - f1_23: 0.5582 - f1_24: 0.8230 - f1_25: 0.8224 - f1_26: 0.9656 - f1_27: 0.9930 - f1_28: 0.6466 - f1_30: 0.0961 - f1_31: 0.9444 - f1_32: 0.8736 - f1_33: 0.9169 - f1_35: 0.0000e+00 - f1_36: 0.9066 - f1_37: 0.0000e+00 - f1_38: 0.7589 - f1_39: 0.9886 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.9566 - f1_43: 0.0000e+00 - f1_44: 0.0429 - f1_45: 0.7299 - val_loss: 0.0468 - val_accuracy: 0.9870 - val_f1: 0.5029 - val_f1_2: 0.8393 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.9834 - val_f1_10: 0.8643 - val_f1_11: 0.0096 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.8193 - val_f1_16: 0.7048 - val_f1_17: 0.9681 - val_f1_18: 1.0000 - val_f1_19: 0.8213 - val_f1_20: 0.9689 - val_f1_21: 0.9954 - val_f1_22: 0.0000e+00 - val_f1_23: 0.4411 - val_f1_24: 0.6420 - val_f1_25: 0.7705 - val_f1_26: 0.9698 - val_f1_27: 0.9975 - val_f1_28: 0.3931 - val_f1_30: 0.1124 - val_f1_31: 0.9222 - val_f1_32: 0.8179 - val_f1_33: 0.8852 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8846 - val_f1_37: 0.0000e+00 - val_f1_38: 0.6727 - val_f1_39: 0.9926 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9458 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.6952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrM_XiQ1Bdpp"
      },
      "source": [
        "loss: 0.1255 - accuracy: 0.9631\n",
        "\n",
        "\n",
        "loss: 0.0583 - accuracy: 0.9840 - val_loss: 0.0741 - val_accuracy: 0.9790\n",
        "\n",
        "loss: 0.0589 - accuracy: 0.9836 - val_loss: 0.0711 - val_accuracy: 0.9798"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#History f1 for class"
      ],
      "metadata": {
        "id": "87_kBxnCwm0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_val_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_val_list[i] = history.history['val_f1_{}'.format(i)][-1]\n",
        "f1_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_list[i] = history.history['f1_{}'.format(i)][-1]"
      ],
      "metadata": {
        "id": "nbNUZ6RoZ_qp"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index2tag = {value : key for (key, value) in tag2index.items()}\n",
        "index2tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHprJp5QaCIj",
        "outputId": "1b4fd3be-c69b-4486-d8cd-3c56b2acb0b3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '-PAD-',\n",
              " 1: '.',\n",
              " 2: 'VBD',\n",
              " 3: '``',\n",
              " 4: '#',\n",
              " 5: 'JJS',\n",
              " 6: 'SYM',\n",
              " 7: 'LS',\n",
              " 8: ',',\n",
              " 9: 'CC',\n",
              " 10: 'WDT',\n",
              " 11: 'JJR',\n",
              " 12: 'FW',\n",
              " 13: 'UH',\n",
              " 14: 'RBR',\n",
              " 15: 'NNS',\n",
              " 16: 'JJ',\n",
              " 17: 'PRP',\n",
              " 18: 'TO',\n",
              " 19: 'NN',\n",
              " 20: 'DT',\n",
              " 21: 'POS',\n",
              " 22: 'RBS',\n",
              " 23: 'RP',\n",
              " 24: 'WP',\n",
              " 25: 'VBP',\n",
              " 26: 'MD',\n",
              " 27: '$',\n",
              " 28: 'VBG',\n",
              " 29: \"''\",\n",
              " 30: 'WRB',\n",
              " 31: 'VBZ',\n",
              " 32: 'NNP',\n",
              " 33: 'CD',\n",
              " 34: ':',\n",
              " 35: 'EX',\n",
              " 36: 'VB',\n",
              " 37: '-LRB-',\n",
              " 38: 'VBN',\n",
              " 39: 'PRP$',\n",
              " 40: 'NNPS',\n",
              " 41: 'PDT',\n",
              " 42: 'IN',\n",
              " 43: 'WP$',\n",
              " 44: '-RRB-',\n",
              " 45: 'RB'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- F1: {}'.format(index2tag[i], f1_list[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoPk9XRiaEgx",
        "outputId": "e1b9530c-d5d6-4329-ac41-0dd4ef3fc876"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: VBD --- F1: 0.8770222067832947\n",
            "Tag: # --- F1: 0.0\n",
            "Tag: JJS --- F1: 0.0\n",
            "Tag: SYM --- F1: 0.0\n",
            "Tag: LS --- F1: 0.0\n",
            "Tag: CC --- F1: 0.9842978119850159\n",
            "Tag: WDT --- F1: 0.8323521614074707\n",
            "Tag: JJR --- F1: 0.11260323226451874\n",
            "Tag: FW --- F1: 0.0\n",
            "Tag: UH --- F1: 0.0\n",
            "Tag: RBR --- F1: 0.0\n",
            "Tag: NNS --- F1: 0.8711555004119873\n",
            "Tag: JJ --- F1: 0.7768896818161011\n",
            "Tag: PRP --- F1: 0.9746993780136108\n",
            "Tag: TO --- F1: 0.9973098039627075\n",
            "Tag: NN --- F1: 0.8535341620445251\n",
            "Tag: DT --- F1: 0.9710032343864441\n",
            "Tag: POS --- F1: 0.9965294599533081\n",
            "Tag: RBS --- F1: 0.0\n",
            "Tag: RP --- F1: 0.5581799149513245\n",
            "Tag: WP --- F1: 0.8229836225509644\n",
            "Tag: VBP --- F1: 0.8223810195922852\n",
            "Tag: MD --- F1: 0.9655870795249939\n",
            "Tag: $ --- F1: 0.9929778575897217\n",
            "Tag: VBG --- F1: 0.646584689617157\n",
            "Tag: WRB --- F1: 0.09608584642410278\n",
            "Tag: VBZ --- F1: 0.9444485902786255\n",
            "Tag: NNP --- F1: 0.8736172914505005\n",
            "Tag: CD --- F1: 0.9168846011161804\n",
            "Tag: EX --- F1: 0.0\n",
            "Tag: VB --- F1: 0.9065517783164978\n",
            "Tag: -LRB- --- F1: 0.0\n",
            "Tag: VBN --- F1: 0.7588644027709961\n",
            "Tag: PRP$ --- F1: 0.98860102891922\n",
            "Tag: NNPS --- F1: 0.0\n",
            "Tag: PDT --- F1: 0.0\n",
            "Tag: IN --- F1: 0.9565752148628235\n",
            "Tag: WP$ --- F1: 0.0\n",
            "Tag: -RRB- --- F1: 0.04285714030265808\n",
            "Tag: RB --- F1: 0.7298955321311951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- Val_F1: {}'.format(index2tag[i], f1_val_list[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AIxv0xZaG9M",
        "outputId": "6335a3e0-ec64-4a91-f597-67c179960637"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: VBD --- Val_F1: 0.8392686247825623\n",
            "Tag: # --- Val_F1: 0.0\n",
            "Tag: JJS --- Val_F1: 0.0\n",
            "Tag: SYM --- Val_F1: 0.0\n",
            "Tag: LS --- Val_F1: 0.0\n",
            "Tag: CC --- Val_F1: 0.9833873510360718\n",
            "Tag: WDT --- Val_F1: 0.864305853843689\n",
            "Tag: JJR --- Val_F1: 0.009569376707077026\n",
            "Tag: FW --- Val_F1: 0.0\n",
            "Tag: UH --- Val_F1: 0.0\n",
            "Tag: RBR --- Val_F1: 0.0\n",
            "Tag: NNS --- Val_F1: 0.8193391561508179\n",
            "Tag: JJ --- Val_F1: 0.7048242092132568\n",
            "Tag: PRP --- Val_F1: 0.9680726528167725\n",
            "Tag: TO --- Val_F1: 1.0\n",
            "Tag: NN --- Val_F1: 0.8212551474571228\n",
            "Tag: DT --- Val_F1: 0.9688577651977539\n",
            "Tag: POS --- Val_F1: 0.9954264163970947\n",
            "Tag: RBS --- Val_F1: 0.0\n",
            "Tag: RP --- Val_F1: 0.4411255121231079\n",
            "Tag: WP --- Val_F1: 0.641991376876831\n",
            "Tag: VBP --- Val_F1: 0.7705489993095398\n",
            "Tag: MD --- Val_F1: 0.9698238372802734\n",
            "Tag: $ --- Val_F1: 0.9975429773330688\n",
            "Tag: VBG --- Val_F1: 0.3931117057800293\n",
            "Tag: WRB --- Val_F1: 0.11235076934099197\n",
            "Tag: VBZ --- Val_F1: 0.922249972820282\n",
            "Tag: NNP --- Val_F1: 0.8178548812866211\n",
            "Tag: CD --- Val_F1: 0.8852079510688782\n",
            "Tag: EX --- Val_F1: 0.0\n",
            "Tag: VB --- Val_F1: 0.8845916986465454\n",
            "Tag: -LRB- --- Val_F1: 0.0\n",
            "Tag: VBN --- Val_F1: 0.6726688742637634\n",
            "Tag: PRP$ --- Val_F1: 0.9925627112388611\n",
            "Tag: NNPS --- Val_F1: 0.0\n",
            "Tag: PDT --- Val_F1: 0.0\n",
            "Tag: IN --- Val_F1: 0.9458006024360657\n",
            "Tag: WP$ --- Val_F1: 0.0\n",
            "Tag: -RRB- --- Val_F1: 0.0\n",
            "Tag: RB --- Val_F1: 0.6951992511749268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Infos"
      ],
      "metadata": {
        "id": "tdQOOMeBaK6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 40 epochs, batch 128\n",
        "loss: 0.0370 - accuracy: 0.9900 - f1: 0.5324"
      ],
      "metadata": {
        "id": "sjumesASaNsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_without_point = []\n",
        "\n",
        "for i in test:\n",
        "  if i[0] != '':\n",
        "    if i[1] != '.' and i[1] != ',' and i[1] != '``' and i[1] != \"''\":\n",
        "      test_without_point += [i]\n",
        "  else: test_without_point += ''\n",
        "\n",
        "#print(test_without_point)\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in test_without_point:\n",
        "  if i != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    test_sentences.append(s)\n",
        "    test_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "test_sentences_X,  test_tags_y = [], []\n",
        "\n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        "\n",
        "for s in test_tags:\n",
        "    test_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ],
      "metadata": {
        "id": "sUZ9F2amtq-V"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_score_single(y_true, y_pred):\n",
        "    y_true = set(y_true)\n",
        "    y_pred = set(y_pred)\n",
        "    cross_size = len(y_true & y_pred)\n",
        "    if cross_size == 0: return 0.\n",
        "    p = 1. * cross_size / len(y_pred)\n",
        "    r = 1. * cross_size / len(y_true)\n",
        "    return 2 * p * r / (p + r)\n",
        "    \n",
        "def f1_test(y_true, y_pred):\n",
        "    return np.mean([f1_score_single(x, y) for x, y in zip(y_true, y_pred)])"
      ],
      "metadata": {
        "id": "eDtMgfzA95AG"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_pred = model.predict(test_sentences_X)\n",
        "\n",
        "y_pred = np.zeros((y_val_pred.shape[0], y_val_pred.shape[1]), dtype=int)\n",
        "for i in range(len(y_val_pred)):\n",
        "  for j in range(len(y_val_pred[i])):\n",
        "    y_pred[i][j] = np.argmax(y_val_pred[i][j])\n",
        "\n",
        "\n",
        "f1_val = f1_test(test_tags_y, y_pred)"
      ],
      "metadata": {
        "id": "b7uGU-eftvUI"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF73WVsD9-W9",
        "outputId": "7cf381f4-3ba1-41b9-e18a-5649167a32fc"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9391729910364746"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "tags_flat = test_tags_y.flatten()\n",
        "pred_flat = y_pred.flatten()\n",
        "\n",
        "print(f1_score(tags_flat, pred_flat, average='weighted'))\n",
        "print(f1_score(tags_flat, pred_flat, average='macro'))\n",
        "print(f1_score(tags_flat, pred_flat, average='micro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWRGwHcaTeAh",
        "outputId": "313cf93f-c2e0-4296-cb91-1dc78005716c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9877444114236936\n",
            "0.710953462908515\n",
            "0.9879949244832089\n"
          ]
        }
      ]
    }
  ]
}