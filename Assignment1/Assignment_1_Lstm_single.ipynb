{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1_Lstm_single.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Due to**: 23/12/2021 (dd/mm/yyyy)\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Part-of Speech (POS) tagging as Sequence Labelling using Recurrent Neural Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4_wqPdlBcKS"
      },
      "source": [
        "# Intro\n",
        "\n",
        "In this assignment  we will ask you to perform POS tagging using neural architectures\n",
        "\n",
        "You are asked to follow these steps:\n",
        "*   Download the corpora and split it in training and test sets, structuring a dataframe.\n",
        "*   Embed the words using GloVe embeddings\n",
        "*   Create a baseline model, using a simple neural architecture\n",
        "*   Experiment doing small modifications to the baseline model, choose hyperparameters using the validation set\n",
        "*   Evaluate your two best model\n",
        "*   Analyze the errors of your model\n",
        "\n",
        "\n",
        "**Task**: given a corpus of documents, predict the POS tag for each word\n",
        "\n",
        "**Corpus**:\n",
        "Ignore the numeric value in the third column, use only the words/symbols and its label. \n",
        "The corpus is available at:\n",
        "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
        "\n",
        "**Splits**: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
        "\n",
        "\n",
        "**Features**: you MUST use GloVe embeddings as the only input features to the model.\n",
        "\n",
        "**Splitting**: you can decide to split documents into sentences or not, the choice is yours.\n",
        "\n",
        "**I/O structure**: The input data will have three dimensions: 1-documents/sentences, 2-token, 3-features; for the output there are 2 possibilities: if you use one-hot encoding it will be 1-documents/sentences, 2-token labels, 3-classes, if you use a single integer that indicates the number of the class it will be 1-documents/sentences, 2-token labels.\n",
        "\n",
        "**Baseline**: two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top; the choice of hyper-parameters is yours.\n",
        "\n",
        "**Architectures**: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and adding an additional dense layer; do not mix these variantions.\n",
        "\n",
        "\n",
        "**Training and Experiments**: all the experiments must involve only the training and validation sets.\n",
        "\n",
        "**Evaluation**: in the end, only the two best models of your choice (according to the validation set) must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech. DO NOT CONSIDER THE PUNCTUATION CLASSES.\n",
        "\n",
        "**Metrics**: the metric you must use to evaluate your final model is the F1-macro, WITHOUT considering punctuation/symbols classes; during the training process you can use accuracy because you can't use the F1 metric unless you use a single (gigantic) batch because there is no way to aggregate \"partial\" F1 scores computed on mini-batches.\n",
        "\n",
        "**Discussion and Error Analysis** : verify and discuss if the results on the test sets are coherent with those on the validation set; analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
        "\n",
        "**Report**: you are asked to deliver the code of your experiments and a small pdf report of about 2 pages; the pdf must begin with the names of the people of your team and a small abstract (4-5 lines) that sums up your findings.\n",
        "\n",
        "# Out Of Vocabulary (OOV) terms\n",
        "\n",
        "How to handle words that are not in GloVe vocabulary?\n",
        "You can handle them as you want (random embedding, placeholder, whatever!), but they must be STATIC embeddings (you cannot train them).\n",
        "\n",
        "But there is a very important caveat! As usual, the element of the test set must not influence the elements of the other splits!\n",
        "\n",
        "So, when you compute new embeddings for train+validation, you must forget about test documents.\n",
        "The motivation is to emulate a real-world scenario, where you select and train a model in the first stage, without knowing nothing about the testing environment.\n",
        "\n",
        "For implementation convenience, you CAN use a single vocabulary file/matrix/whatever. The principle of the previous point is that the embeddings inside that file/matrix must be generated independently for train and test splits.\n",
        "\n",
        "Basically in a real-world scenario, this is what would happen:\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Training of the model(s)\n",
        "5. Compute embeddings for terms OOV2 of the validation split \n",
        "6. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "7. Validation of the model(s)\n",
        "8. Compute embeddings for terms OOV3 of the test split \n",
        "9. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2+OOV3\n",
        "10. Testing of the final model\n",
        "\n",
        "In this case, where we already have all the documents, we can simplify the process a bit, but the procedure must remain rigorous.\n",
        "\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Compute embeddings for terms OOV2 of the validation split \n",
        "5. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "6. Compute embeddings for terms OOV3 of the test split \n",
        "7. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2\n",
        "8. Training of the model(s)\n",
        "9. Validation of the model(s)\n",
        "10. Testing of the final model\n",
        "\n",
        "Step 2 and step 6 must be completely independent of each other, for what concerns the method and the documents. But they can rely on the previous vocabulary (V1 for step 2 and V3 for step 6)\n",
        "THEREFORE if a word is present both in the training set and the test split and not in the starting vocabulary, its embedding is computed in step 2) and it is not considered OOV anymore in step 6).\n",
        "\n",
        "# Report\n",
        "The report must not be just a copy and paste of graphs and tables!\n",
        "\n",
        "The report must not be longer than 2 pages and must contain:\n",
        "* The names of the member of your team\n",
        "* A short abstract (4-5 lines) that sum ups everything\n",
        "* A general description of the task you have addressed and how you have addressed it\n",
        "* A short description of the models you have used\n",
        "* Some tables that sum up your findings in validation and test and a discussion of those results\n",
        "* The most relevant findings of your error analysis\n",
        "\n",
        "# Evaluation Criterion\n",
        "\n",
        "The goal of this assignment is not to prove you can find best model ever, but to face a common task, structure it correctly, and follow a correct and rigorous experimental procedure.\n",
        "In other words, we don't care if you final models are awful as long as you have followed the correct procedure and wrote a decent report.\n",
        "\n",
        "The score of the assignment will be computed roughly as follows\n",
        "* 1 point for the general setting of the problem\n",
        "* 1 point for the handling of OOV terms\n",
        "* 1 point for the models\n",
        "* 1 point for train-validation-test procedure\n",
        "* 2 point for the discussion of the results, error analysis, and report\n",
        "\n",
        "This distribution of scores is tentative and we may decide to alter it at any moment.\n",
        "We also reserve the right to assign a small bonus (0.5 points) to any assignment that is particularly worthy. Similarly, in case of grave errors, we may decide to assign an equivalent malus (-0.5 points).\n",
        "\n",
        "# Contacts\n",
        "\n",
        "In case of any doubt, question, issue, or help we highly recommend you to check the [course useful material](https://virtuale.unibo.it/pluginfile.php/1036039/mod_resource/content/2/NLP_Course_Useful_Material.pdf) for additional information, and to use the Virtuale forums to discuss with other students.\n",
        "\n",
        "You can always contact us at the following email addresses. To increase the probability of a prompt response, we reccomend you to write to both the teaching assistants.\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Andrea Galassi -> a.galassi@unibo.it\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it\n",
        "\n",
        "\n",
        "# FAQ\n",
        "* You can use a non-trainable Embedding layer to load the glove embeddings\n",
        "* You can use any library of your choice to implement the networks. Two options are tensorflow/keras or pythorch. Both these libraries have all the classes you need to implement these simple architectures and there are plenty of tutorials around, where you can learn how to use them."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Dataset"
      ],
      "metadata": {
        "id": "COWq7PIOWtYr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emdubW66fnlD"
      },
      "source": [
        "import os, shutil  #  file management\n",
        "import sys \n",
        "import pandas as pd  #  dataframe management\n",
        "import numpy as np  #  data manipulation\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request  #  download files\n",
        "import zipfile  #  unzip files\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B-w2pqyfpF1"
      },
      "source": [
        "from keras import metrics \n",
        "from tensorflow.python.keras.metrics import Metric\n",
        "from sklearn.metrics import f1_score\n",
        "import keras.backend as K"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8prqSTRX-TK",
        "outputId": "099a5b1f-05fa-4160-ca0c-fbddd4f83302"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3xn1MZPgA_v",
        "outputId": "55e3f9ca-ddad-4b04-ba34-3bccf30119c0"
      },
      "source": [
        "dataset_folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\")\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "    os.makedirs(dataset_folder)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
        "\n",
        "dataset_path = os.path.join(dataset_folder, \"treebank.zip\")\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    urllib.request.urlretrieve(url, dataset_path)\n",
        "    print(\"Successful download\")\n",
        "\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_folder)\n",
        "print(\"Successful extraction\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful download\n",
            "Successful extraction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre Processing\n",
        "- splitting"
      ],
      "metadata": {
        "id": "iXl3LpVxW2h5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKbUsrVHhGAG"
      },
      "source": [
        "dataset_name = \"dependency_treebank\"\n",
        "\n",
        "\n",
        "folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\", dataset_name)\n",
        "\n",
        "pre_train = []\n",
        "pre_valid = []\n",
        "pre_test = []\n",
        "i = 1\n",
        "\n",
        "file_list = sorted(os.listdir(folder))\n",
        "\n",
        "for filename in file_list:\n",
        "  file_path = os.path.join(folder, filename)\n",
        "  if os.path.isfile(file_path):\n",
        "    # open the file\n",
        "      text = []\n",
        "      with open(file_path, mode='r', encoding='utf-8') as text_file:\n",
        "        text = text_file.read()\n",
        "        if i <= 100:\n",
        "          pre_train.append(text)\n",
        "        elif i <= 150:\n",
        "          pre_valid.append(text)\n",
        "        else:\n",
        "          pre_test.append(text)  \n",
        "  i+=1\n",
        "\n",
        "tr = []\n",
        "val = []\n",
        "tes = []\n",
        "\n",
        "for paragraph in pre_train:\n",
        "   tr.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_valid:\n",
        "   val.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_test:\n",
        "   tes.append(paragraph.split('\\n'))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUgbTXi-F7Sa"
      },
      "source": [
        "train = []\n",
        "valid = []\n",
        "test = []\n",
        "\n",
        "for i in tr:\n",
        "  for j in i:\n",
        "    train.append(j.split('\\t'))\n",
        "\n",
        "for i in val:\n",
        "  for j in i:\n",
        "    valid.append(j.split('\\t'))\n",
        "\n",
        "for i in tes:\n",
        "  for j in i:\n",
        "    test.append(j.split('\\t'))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmzVaRuQFp0M"
      },
      "source": [
        "train_sentences = []\n",
        "valid_sentences = []\n",
        "test_sentences = []\n",
        "train_tags = []\n",
        "valid_tags = []\n",
        "test_tags = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in train:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    train_sentences.append(s)\n",
        "    train_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in valid:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    valid_sentences.append(s)\n",
        "    valid_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in test:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    test_sentences.append(s)\n",
        "    test_tags.append(t)\n",
        "    s = []\n",
        "    t = []"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB65cUFLIeEr"
      },
      "source": [
        "flat_train = [item for sublist in train_sentences for item in sublist]\n",
        "flat_valid = [item for sublist in valid_sentences for item in sublist]\n",
        "flat_test = [item for sublist in test_sentences for item in sublist]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "- vocabularies\n",
        "- GloVe download"
      ],
      "metadata": {
        "id": "VQsxPZcXW9EF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZO6ga1EGHdh"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "train_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "train_tokenizer.fit_on_texts(train_sentences)                    # fit tokeniser on data\n",
        "train_encoded = train_tokenizer.texts_to_sequences(train_sentences)\n",
        "\n",
        "valid_tokenizer = Tokenizer()       \n",
        "valid_tokenizer.fit_on_texts(valid_sentences)                  # instantiate tokeniser\n",
        "valid_encoded = valid_tokenizer.texts_to_sequences(valid_sentences)\n",
        "\n",
        "test_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "test_tokenizer.fit_on_texts(test_sentences)                    # fit tokeniser on data\n",
        "test_encoded = test_tokenizer.texts_to_sequences(test_sentences)  # use the tokeniser to encode input sequence"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(train_tags)\n",
        "tags_encoded = tag_tokenizer.texts_to_sequences(train_tags)"
      ],
      "metadata": {
        "id": "1nC8ma_jXYtc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMsG0Yw1l50L",
        "outputId": "357e71b3-b833-4946-e938-b9a1f1997a07"
      },
      "source": [
        "voc = list(set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(valid_tokenizer.word_index.keys()) - set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(test_tokenizer.word_index.keys()) - set(voc))\n",
        "print(len(voc))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7404\n",
            "9901\n",
            "10947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS9RwnOtJuas"
      },
      "source": [
        "word_index = dict(zip(voc, range(2, len(voc)+3)))\n",
        "word_index['-PAD-'] = 0\n",
        "word_index['-OOV-'] = 1\n",
        "\n",
        "tags = set([item for sublist in train_tags for item in sublist])\n",
        "\n",
        "tag2index = {t: i + 1 for i, t in enumerate(tags)}\n",
        "tag2index['-PAD-'] = 0 "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoj4Zx51J2MU",
        "outputId": "8bede0ec-5cc4-4d6e-b89d-f64d371ed017"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-14 16:12:44--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-12-14 16:12:44--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-12-14 16:12:45--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.15MB/s    in 2m 40s  \n",
            "\n",
            "2021-12-14 16:15:25 (5.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc-T6FcLKtS9",
        "outputId": "569e88d3-9285-411b-ff5d-5b70d6c6f5c4"
      },
      "source": [
        "path_to_glove_file = os.path.join(os.getcwd(), \"glove.6B.100d.txt\")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding\n",
        "- matrix creation\n",
        "- padding\n",
        "- one hot encoding of tags\n",
        "- punctuation identification"
      ],
      "metadata": {
        "id": "_x9MpHE6XB2l"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0mOStEPMeXt",
        "outputId": "8e124443-ec86-412d-a94c-56d994498a9c"
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        embedding_matrix += np.random.uniform(low=-0.05, high=0.05, size=embedding_dim)\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 10271 words (678 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwoJ4hbYXFPq"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpYrHs0RMtQJ"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8EZM31Js06-",
        "outputId": "83168e4a-fcce-4889-954e-3d52192438c6"
      },
      "source": [
        "MAX_LENGTH = len(max(train_sentences, key=len))\n",
        "print(MAX_LENGTH)\n",
        "MAX_LENGTH2 = len(max(valid_sentences, key=len))\n",
        "print(MAX_LENGTH2)\n",
        "MAX_LENGTH3 = len(max(test_sentences, key=len))\n",
        "print(MAX_LENGTH3)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "249\n",
            "81\n",
            "58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSRa9SwTuNrX"
      },
      "source": [
        "train_sentences_X, valid_sentences_X, train_tags_y, valid_tags_y = [], [], [], []\n",
        " \n",
        "for s in train_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        "\n",
        "for s in valid_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    valid_sentences_X.append(s_int)\n",
        " \n",
        "for s in train_tags:\n",
        "    train_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "for s in valid_tags:\n",
        "    valid_tags_y.append([tag2index[t] for t in s])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf-XlSrStQx3"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_sentences_X = pad_sequences(valid_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_tags_y = pad_sequences(valid_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)"
      ],
      "metadata": {
        "id": "yc_u-8itYXQB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "point = [tag2index['.']]\n",
        "virg = [tag2index[',']]\n",
        "weird_apex = [tag2index['``']]\n",
        "single_apex = [tag2index[\"''\"]]\n",
        "two_dots = [tag2index[':']]\n",
        "pad = [tag2index['-PAD-']]\n",
        "\n",
        "punct_cat_classes = to_categorical([point, virg, weird_apex, single_apex, two_dots, pad], len(tag2index))\n",
        "punct_cat_classes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tvb1ZbVIYZ4Y",
        "outputId": "8cb3a535-217e-4112-ea74-ee7946c99915"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 1, 46)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cum_tags = np.zeros(46)\n",
        "for i in punct_cat_classes:\n",
        "  print(i[0])\n",
        "  cum_tags += i[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7oHM_vUYf4c",
        "outputId": "052eb250-5d6c-4952-cf69-c8f308afb0f7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "where_tags = np.where(np.logical_not(cum_tags))\n",
        "where_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5uT2QJrYi15",
        "outputId": "4e3581ee-7838-4b09-cab3-fc21c69c2d58"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 1,  2,  3,  4,  5,  6,  7,  9, 11, 12, 13, 14, 16, 17, 18, 19, 20,\n",
              "        21, 22, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39,\n",
              "        40, 41, 42, 43, 44, 45]),)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_punct_indexes = where_tags[0]\n",
        "n_classes = len(no_punct_indexes)"
      ],
      "metadata": {
        "id": "nPBHVI2HYlxs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "cat_val_tags_y = to_categorical(valid_tags_y, len(tag2index))\n",
        "print(len(cat_train_tags_y), len(cat_val_tags_y))\n",
        "print(len(train_sentences_X), len(valid_sentences_X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SPgQ9YmYnbG",
        "outputId": "8bdae368-6d67-450b-86af-fd411431fa8c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1963 1299\n",
            "1963 1299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mini Data Vizualization Inset"
      ],
      "metadata": {
        "id": "426ajTPoYtaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_tags = np.zeros(46)\n",
        "for i in cat_train_tags_y:\n",
        "  for t in i:\n",
        "    distribution_tags += t"
      ],
      "metadata": {
        "id": "Jf_djyP3YvhV"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = sum(distribution_tags[1:])\n",
        "norm = [float(i)/s for i in distribution_tags[1:]]"
      ],
      "metadata": {
        "id": "8jqHRmS_Yyde"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt "
      ],
      "metadata": {
        "id": "-ksL6xl1Y2Sw"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(np.arange(len(norm)),norm)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "o4EY5adaY3Z6",
        "outputId": "9e06ded0-c663-4a34-dd91-457cfecde03b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#F1 Metric"
      ],
      "metadata": {
        "id": "yUvMJPihY4nK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric.\n",
        "\n",
        "    Only computes a batch-wise average of precision.\n",
        "\n",
        "    Computes the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric.\n",
        "\n",
        "    Only computes a batch-wise average of recall.\n",
        "\n",
        "    Computes the recall, a metric for multi-label classification of\n",
        "    how many relevant items are selected.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def f1_binary(y_true, y_pred):\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
        "\n",
        "def metric_wrapper(index):\n",
        "  def f1_for_class(y_true, y_pred):\n",
        "    #get only the desired class\n",
        "    true = y_true[:,:,index]\n",
        "    pred = y_pred[:,:,index]\n",
        "    #return dice per class\n",
        "    tmp = f1_binary(true,pred)\n",
        "    return tmp\n",
        "  f1_for_class.__name__ = 'f1_' + str(index)\n",
        "  return f1_for_class\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    result = 0.0\n",
        "    for class_id in no_punct_indexes:\n",
        "        y_true_single_class = y_true[:,:,class_id]\n",
        "        y_pred_single_class = y_pred[:,:,class_id]\n",
        "        f1_single = f1_binary(y_true_single_class, y_pred_single_class)\n",
        "        result += f1_single / float(n_classes)\n",
        "    return result"
      ],
      "metadata": {
        "id": "_4dwoTOuZCmi"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsVGByW_sdec",
        "outputId": "9cf933c5-5cae-4abf-e47a-577e6bea6ab7"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(embedding_layer)\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(Dense(len(tag2index)))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy', f1, [metric_wrapper(i) for i in no_punct_indexes]])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 249, 100)          1094900   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 249, 512)         731136    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 249, 46)           23598     \n",
            "                                                                 \n",
            " activation (Activation)     (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,849,634\n",
            "Trainable params: 754,734\n",
            "Non-trainable params: 1,094,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "history = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=40, validation_data=(valid_sentences_X, cat_val_tags_y), callbacks=[callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n45419-PZSR7",
        "outputId": "2f8e3a0e-0cb0-41f6-8b42-3def09df25f4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "16/16 [==============================] - 92s 5s/step - loss: 0.7588 - accuracy: 0.8471 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.3376 - val_accuracy: 0.9173 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.3056 - accuracy: 0.9193 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2864 - val_accuracy: 0.9229 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - 74s 5s/step - loss: 0.2762 - accuracy: 0.9281 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2660 - val_accuracy: 0.9343 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - 74s 5s/step - loss: 0.2562 - accuracy: 0.9378 - f1: 1.0813e-05 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 4.3253e-04 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2467 - val_accuracy: 0.9421 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.2355 - accuracy: 0.9434 - f1: 0.0015 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0617 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2249 - val_accuracy: 0.9445 - val_f1: 0.0022 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0858 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 5.3163e-04 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - 74s 5s/step - loss: 0.2126 - accuracy: 0.9462 - f1: 0.0107 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.2842 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0846 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0581 - f1_35: 3.0864e-04 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2017 - val_accuracy: 0.9480 - val_f1: 0.0215 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.2542 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.3865 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.2141 - val_f1_35: 0.0069 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.1889 - accuracy: 0.9504 - f1: 0.0401 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.4568 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.5201 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.6011 - f1_35: 0.0273 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1793 - val_accuracy: 0.9526 - val_f1: 0.0487 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.4344 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.5593 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8525 - val_f1_35: 0.1037 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - 74s 5s/step - loss: 0.1673 - accuracy: 0.9550 - f1: 0.0574 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0409 - f1_4: 0.5716 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0121 - f1_20: 0.0000e+00 - f1_21: 0.6412 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.8664 - f1_35: 0.1650 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1598 - val_accuracy: 0.9579 - val_f1: 0.0641 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0239 - val_f1_4: 0.5213 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.1870 - val_f1_20: 0.0000e+00 - val_f1_21: 0.6775 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8936 - val_f1_35: 0.2535 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0058 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.1483 - accuracy: 0.9599 - f1: 0.0866 - f1_1: 0.0028 - f1_2: 0.0000e+00 - f1_3: 0.2192 - f1_4: 0.6355 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.6125 - f1_20: 0.0000e+00 - f1_21: 0.7461 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.8982 - f1_35: 0.3395 - f1_36: 0.0000e+00 - f1_37: 0.0049 - f1_38: 0.0070 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1433 - val_accuracy: 0.9625 - val_f1: 0.1013 - val_f1_1: 0.0650 - val_f1_2: 0.0000e+00 - val_f1_3: 0.3618 - val_f1_4: 0.5833 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.9016 - val_f1_20: 0.0000e+00 - val_f1_21: 0.7666 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8989 - val_f1_35: 0.4095 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0120 - val_f1_38: 0.0551 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.1327 - accuracy: 0.9652 - f1: 0.1130 - f1_1: 0.0977 - f1_2: 0.0000e+00 - f1_3: 0.4146 - f1_4: 0.6650 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.9705 - f1_20: 0.0000e+00 - f1_21: 0.8086 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0298 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9089 - f1_35: 0.4755 - f1_36: 0.0000e+00 - f1_37: 0.0643 - f1_38: 0.0808 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0036 - val_loss: 0.1298 - val_accuracy: 0.9664 - val_f1: 0.1252 - val_f1_1: 0.2735 - val_f1_2: 0.0000e+00 - val_f1_3: 0.5270 - val_f1_4: 0.5843 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.9873 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8257 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.1365 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9131 - val_f1_35: 0.4879 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0970 - val_f1_38: 0.1706 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0069\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.1200 - accuracy: 0.9688 - f1: 0.1473 - f1_1: 0.3349 - f1_2: 0.0000e+00 - f1_3: 0.5500 - f1_4: 0.6946 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.9931 - f1_20: 0.0000e+00 - f1_21: 0.8473 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.4682 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9181 - f1_35: 0.5434 - f1_36: 0.0000e+00 - f1_37: 0.1802 - f1_38: 0.2005 - f1_39: 0.1058 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0544 - val_loss: 0.1191 - val_accuracy: 0.9693 - val_f1: 0.1637 - val_f1_1: 0.5204 - val_f1_2: 0.0000e+00 - val_f1_3: 0.6002 - val_f1_4: 0.6169 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_11: 0.0074 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.9970 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8404 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.6681 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9232 - val_f1_35: 0.4953 - val_f1_36: 0.0000e+00 - val_f1_37: 0.2064 - val_f1_38: 0.3307 - val_f1_39: 0.2812 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0591\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - 74s 5s/step - loss: 0.1095 - accuracy: 0.9718 - f1: 0.1870 - f1_1: 0.5101 - f1_2: 0.0000e+00 - f1_3: 0.6093 - f1_4: 0.7170 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0450 - f1_9: 0.0000e+00 - f1_11: 0.0169 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.9974 - f1_20: 0.0000e+00 - f1_21: 0.8675 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.8515 - f1_26: 0.0121 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9254 - f1_35: 0.5812 - f1_36: 0.0000e+00 - f1_37: 0.2781 - f1_38: 0.3483 - f1_39: 0.5282 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.1916 - val_loss: 0.1099 - val_accuracy: 0.9713 - val_f1: 0.2023 - val_f1_1: 0.5746 - val_f1_2: 0.0000e+00 - val_f1_3: 0.6176 - val_f1_4: 0.6573 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.1608 - val_f1_9: 0.0000e+00 - val_f1_11: 0.0985 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8602 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9197 - val_f1_26: 0.0453 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9274 - val_f1_35: 0.5716 - val_f1_36: 0.0000e+00 - val_f1_37: 0.2564 - val_f1_38: 0.4628 - val_f1_39: 0.7045 - val_f1_40: 0.0130 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.2210\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - 74s 5s/step - loss: 0.1007 - accuracy: 0.9742 - f1: 0.2275 - f1_1: 0.6332 - f1_2: 0.0000e+00 - f1_3: 0.6459 - f1_4: 0.7391 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.5511 - f1_9: 0.0000e+00 - f1_11: 0.1741 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.9986 - f1_20: 0.0000e+00 - f1_21: 0.8782 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9467 - f1_26: 0.0603 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9287 - f1_35: 0.6204 - f1_36: 0.0000e+00 - f1_37: 0.3606 - f1_38: 0.4518 - f1_39: 0.7657 - f1_40: 0.0042 - f1_41: 0.0024 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.3394 - val_loss: 0.1024 - val_accuracy: 0.9729 - val_f1: 0.2406 - val_f1_1: 0.6997 - val_f1_2: 0.0000e+00 - val_f1_3: 0.6707 - val_f1_4: 0.6358 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.7118 - val_f1_9: 0.0000e+00 - val_f1_11: 0.3114 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8731 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9928 - val_f1_26: 0.1642 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9300 - val_f1_35: 0.6338 - val_f1_36: 0.0000e+00 - val_f1_37: 0.3357 - val_f1_38: 0.4615 - val_f1_39: 0.7309 - val_f1_40: 0.0182 - val_f1_41: 0.0086 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.4466\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0933 - accuracy: 0.9760 - f1: 0.2587 - f1_1: 0.7104 - f1_2: 0.0000e+00 - f1_3: 0.6854 - f1_4: 0.7469 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8083 - f1_9: 0.0000e+00 - f1_11: 0.4295 - f1_12: 0.0130 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.9977 - f1_20: 0.0000e+00 - f1_21: 0.8844 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9812 - f1_26: 0.2088 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9324 - f1_35: 0.6630 - f1_36: 0.0000e+00 - f1_37: 0.4148 - f1_38: 0.5203 - f1_39: 0.8028 - f1_40: 0.0300 - f1_41: 0.0082 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.5098 - val_loss: 0.0957 - val_accuracy: 0.9749 - val_f1: 0.2639 - val_f1_1: 0.7568 - val_f1_2: 0.0000e+00 - val_f1_3: 0.6991 - val_f1_4: 0.6943 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.8515 - val_f1_9: 0.0000e+00 - val_f1_11: 0.4709 - val_f1_12: 0.0185 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0200 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8759 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9975 - val_f1_26: 0.3722 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9330 - val_f1_35: 0.6072 - val_f1_36: 0.0000e+00 - val_f1_37: 0.3862 - val_f1_38: 0.5459 - val_f1_39: 0.7589 - val_f1_40: 0.0572 - val_f1_41: 0.0102 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.5021\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - 74s 5s/step - loss: 0.0870 - accuracy: 0.9777 - f1: 0.2823 - f1_1: 0.7490 - f1_2: 0.0000e+00 - f1_3: 0.6999 - f1_4: 0.7598 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8870 - f1_9: 0.0000e+00 - f1_11: 0.5473 - f1_12: 0.0871 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0760 - f1_18: 0.0000e+00 - f1_19: 0.9984 - f1_20: 0.0000e+00 - f1_21: 0.8921 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9925 - f1_26: 0.4039 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0408 - f1_33: 0.0000e+00 - f1_34: 0.9374 - f1_35: 0.6819 - f1_36: 0.0000e+00 - f1_37: 0.4636 - f1_38: 0.5809 - f1_39: 0.8178 - f1_40: 0.0882 - f1_41: 0.0262 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.5637 - val_loss: 0.0900 - val_accuracy: 0.9760 - val_f1: 0.2917 - val_f1_1: 0.7576 - val_f1_2: 0.0000e+00 - val_f1_3: 0.7052 - val_f1_4: 0.7067 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9210 - val_f1_9: 0.0000e+00 - val_f1_11: 0.5439 - val_f1_12: 0.2687 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.1106 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8802 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9975 - val_f1_26: 0.5487 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0780 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9376 - val_f1_35: 0.6485 - val_f1_36: 0.0000e+00 - val_f1_37: 0.4458 - val_f1_38: 0.6225 - val_f1_39: 0.7737 - val_f1_40: 0.0971 - val_f1_41: 0.0102 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.6157\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0814 - accuracy: 0.9790 - f1: 0.3106 - f1_1: 0.7845 - f1_2: 0.0000e+00 - f1_3: 0.7267 - f1_4: 0.7741 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.9175 - f1_9: 0.0000e+00 - f1_11: 0.6211 - f1_12: 0.2139 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.2541 - f1_18: 0.0000e+00 - f1_19: 0.9986 - f1_20: 0.0000e+00 - f1_21: 0.8963 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9921 - f1_26: 0.5374 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0053 - f1_32: 0.2654 - f1_33: 0.0000e+00 - f1_34: 0.9389 - f1_35: 0.7026 - f1_36: 0.0000e+00 - f1_37: 0.4995 - f1_38: 0.6278 - f1_39: 0.8266 - f1_40: 0.1584 - f1_41: 0.0541 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.6274 - val_loss: 0.0855 - val_accuracy: 0.9774 - val_f1: 0.3201 - val_f1_1: 0.7976 - val_f1_2: 0.0000e+00 - val_f1_3: 0.7247 - val_f1_4: 0.7326 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9382 - val_f1_9: 0.0000e+00 - val_f1_11: 0.6598 - val_f1_12: 0.3686 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.2217 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8874 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9975 - val_f1_26: 0.6142 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.4874 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9434 - val_f1_35: 0.6207 - val_f1_36: 0.0000e+00 - val_f1_37: 0.4703 - val_f1_38: 0.6512 - val_f1_39: 0.7737 - val_f1_40: 0.2283 - val_f1_41: 0.0843 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.6030\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0768 - accuracy: 0.9802 - f1: 0.3383 - f1_1: 0.8067 - f1_2: 0.0000e+00 - f1_3: 0.7430 - f1_4: 0.7847 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.9330 - f1_9: 0.0000e+00 - f1_11: 0.6990 - f1_12: 0.3413 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.3883 - f1_18: 0.0000e+00 - f1_19: 0.9987 - f1_20: 0.0000e+00 - f1_21: 0.9010 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9934 - f1_26: 0.5863 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0190 - f1_32: 0.6676 - f1_33: 0.0000e+00 - f1_34: 0.9423 - f1_35: 0.7074 - f1_36: 0.0000e+00 - f1_37: 0.5202 - f1_38: 0.6528 - f1_39: 0.8209 - f1_40: 0.2610 - f1_41: 0.0979 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.6689 - val_loss: 0.0806 - val_accuracy: 0.9785 - val_f1: 0.3420 - val_f1_1: 0.8105 - val_f1_2: 0.0000e+00 - val_f1_3: 0.7306 - val_f1_4: 0.7208 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9407 - val_f1_9: 0.0000e+00 - val_f1_11: 0.7475 - val_f1_12: 0.4809 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.4294 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8906 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9975 - val_f1_26: 0.7065 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0161 - val_f1_32: 0.7165 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9438 - val_f1_35: 0.7055 - val_f1_36: 0.0000e+00 - val_f1_37: 0.4757 - val_f1_38: 0.6675 - val_f1_39: 0.7737 - val_f1_40: 0.2529 - val_f1_41: 0.0493 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.6229\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0726 - accuracy: 0.9813 - f1: 0.3609 - f1_1: 0.8223 - f1_2: 0.0000e+00 - f1_3: 0.7622 - f1_4: 0.7898 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.9376 - f1_9: 0.0000e+00 - f1_11: 0.7687 - f1_12: 0.4251 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.5429 - f1_18: 0.0000e+00 - f1_19: 0.9986 - f1_20: 0.0000e+00 - f1_21: 0.9046 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9946 - f1_26: 0.6730 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0491 - f1_32: 0.8561 - f1_33: 0.0000e+00 - f1_34: 0.9400 - f1_35: 0.7372 - f1_36: 0.0000e+00 - f1_37: 0.5347 - f1_38: 0.6915 - f1_39: 0.8283 - f1_40: 0.3296 - f1_41: 0.1390 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.7096 - val_loss: 0.0771 - val_accuracy: 0.9796 - val_f1: 0.3569 - val_f1_1: 0.8144 - val_f1_2: 0.0000e+00 - val_f1_3: 0.7519 - val_f1_4: 0.7540 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9415 - val_f1_9: 0.0000e+00 - val_f1_11: 0.7173 - val_f1_12: 0.5348 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.5227 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8959 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9975 - val_f1_26: 0.7493 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0537 - val_f1_32: 0.8187 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9459 - val_f1_35: 0.6819 - val_f1_36: 0.0000e+00 - val_f1_37: 0.5273 - val_f1_38: 0.6778 - val_f1_39: 0.7737 - val_f1_40: 0.2816 - val_f1_41: 0.1461 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.6887\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0690 - accuracy: 0.9824 - f1: 0.3764 - f1_1: 0.8306 - f1_2: 0.0000e+00 - f1_3: 0.7832 - f1_4: 0.7970 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.9380 - f1_9: 0.0000e+00 - f1_11: 0.8129 - f1_12: 0.5151 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.6396 - f1_18: 0.0000e+00 - f1_19: 0.9986 - f1_20: 0.0000e+00 - f1_21: 0.9060 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9954 - f1_26: 0.7340 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.1271 - f1_32: 0.8903 - f1_33: 0.0000e+00 - f1_34: 0.9448 - f1_35: 0.7472 - f1_36: 0.0000e+00 - f1_37: 0.5579 - f1_38: 0.7149 - f1_39: 0.8248 - f1_40: 0.3812 - f1_41: 0.1851 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.7338 - val_loss: 0.0745 - val_accuracy: 0.9804 - val_f1: 0.3696 - val_f1_1: 0.8230 - val_f1_2: 0.0000e+00 - val_f1_3: 0.7817 - val_f1_4: 0.6610 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9385 - val_f1_9: 0.0000e+00 - val_f1_11: 0.7813 - val_f1_12: 0.5517 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.5821 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8980 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9975 - val_f1_26: 0.7750 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0840 - val_f1_32: 0.8641 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9461 - val_f1_35: 0.7322 - val_f1_36: 0.0000e+00 - val_f1_37: 0.5605 - val_f1_38: 0.6917 - val_f1_39: 0.7737 - val_f1_40: 0.3532 - val_f1_41: 0.2644 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.7247\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0659 - accuracy: 0.9831 - f1: 0.3897 - f1_1: 0.8383 - f1_2: 0.0000e+00 - f1_3: 0.7915 - f1_4: 0.8002 - f1_5: 0.0252 - f1_6: 0.0000e+00 - f1_7: 0.9421 - f1_9: 0.0000e+00 - f1_11: 0.8395 - f1_12: 0.5611 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.6884 - f1_18: 0.0000e+00 - f1_19: 0.9986 - f1_20: 0.0000e+00 - f1_21: 0.9131 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9951 - f1_26: 0.8055 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.1775 - f1_32: 0.9130 - f1_33: 0.0000e+00 - f1_34: 0.9446 - f1_35: 0.7577 - f1_36: 0.0000e+00 - f1_37: 0.5795 - f1_38: 0.7285 - f1_39: 0.8337 - f1_40: 0.4538 - f1_41: 0.2424 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.7594 - val_loss: 0.0707 - val_accuracy: 0.9813 - val_f1: 0.3819 - val_f1_1: 0.8306 - val_f1_2: 0.0000e+00 - val_f1_3: 0.7835 - val_f1_4: 0.7416 - val_f1_5: 0.0429 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9388 - val_f1_9: 0.0000e+00 - val_f1_11: 0.8273 - val_f1_12: 0.5837 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.6536 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.9091 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9975 - val_f1_26: 0.8048 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.1757 - val_f1_32: 0.8767 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9475 - val_f1_35: 0.7406 - val_f1_36: 0.0000e+00 - val_f1_37: 0.5701 - val_f1_38: 0.7052 - val_f1_39: 0.7747 - val_f1_40: 0.3979 - val_f1_41: 0.2514 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.7244\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0629 - accuracy: 0.9837 - f1: 0.4074 - f1_1: 0.8493 - f1_2: 0.0000e+00 - f1_3: 0.7941 - f1_4: 0.8100 - f1_5: 0.3513 - f1_6: 0.0000e+00 - f1_7: 0.9371 - f1_9: 0.0000e+00 - f1_11: 0.8543 - f1_12: 0.6178 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.7693 - f1_18: 0.0000e+00 - f1_19: 0.9986 - f1_20: 0.0000e+00 - f1_21: 0.9172 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9940 - f1_26: 0.8315 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.2291 - f1_32: 0.9285 - f1_33: 0.0000e+00 - f1_34: 0.9458 - f1_35: 0.7681 - f1_36: 0.0000e+00 - f1_37: 0.5932 - f1_38: 0.7457 - f1_39: 0.8309 - f1_40: 0.5016 - f1_41: 0.2714 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.7554 - val_loss: 0.0681 - val_accuracy: 0.9820 - val_f1: 0.3968 - val_f1_1: 0.8344 - val_f1_2: 0.0000e+00 - val_f1_3: 0.7990 - val_f1_4: 0.7630 - val_f1_5: 0.2983 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9427 - val_f1_9: 0.0000e+00 - val_f1_11: 0.8491 - val_f1_12: 0.5614 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_16: 0.0101 - val_f1_17: 0.7425 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.9075 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9975 - val_f1_26: 0.8329 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.2274 - val_f1_32: 0.9030 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9494 - val_f1_35: 0.7371 - val_f1_36: 0.0000e+00 - val_f1_37: 0.5721 - val_f1_38: 0.7148 - val_f1_39: 0.7852 - val_f1_40: 0.4127 - val_f1_41: 0.2973 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.7355\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0603 - accuracy: 0.9844 - f1: 0.4189 - f1_1: 0.8525 - f1_2: 0.0000e+00 - f1_3: 0.8109 - f1_4: 0.8175 - f1_5: 0.5312 - f1_6: 0.0000e+00 - f1_7: 0.9394 - f1_9: 0.0000e+00 - f1_11: 0.8685 - f1_12: 0.5948 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.8004 - f1_18: 0.0000e+00 - f1_19: 0.9986 - f1_20: 0.0000e+00 - f1_21: 0.9212 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9927 - f1_26: 0.8580 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.2623 - f1_32: 0.9274 - f1_33: 0.0000e+00 - f1_34: 0.9493 - f1_35: 0.7785 - f1_36: 0.0000e+00 - f1_37: 0.6150 - f1_38: 0.7697 - f1_39: 0.8467 - f1_40: 0.5241 - f1_41: 0.3156 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.7823 - val_loss: 0.0660 - val_accuracy: 0.9825 - val_f1: 0.4114 - val_f1_1: 0.8368 - val_f1_2: 0.0000e+00 - val_f1_3: 0.8086 - val_f1_4: 0.7658 - val_f1_5: 0.4611 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9427 - val_f1_9: 0.0000e+00 - val_f1_11: 0.8609 - val_f1_12: 0.6338 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_16: 0.0101 - val_f1_17: 0.8366 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.9114 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9975 - val_f1_26: 0.8595 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.3311 - val_f1_32: 0.9076 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9523 - val_f1_35: 0.7303 - val_f1_36: 0.0000e+00 - val_f1_37: 0.5521 - val_f1_38: 0.7392 - val_f1_39: 0.7918 - val_f1_40: 0.4401 - val_f1_41: 0.3383 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.7478\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0581 - accuracy: 0.9849 - f1: 0.4282 - f1_1: 0.8563 - f1_2: 0.0000e+00 - f1_3: 0.8143 - f1_4: 0.8182 - f1_5: 0.5921 - f1_6: 0.0000e+00 - f1_7: 0.9382 - f1_9: 0.0000e+00 - f1_11: 0.8795 - f1_12: 0.6540 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.8486 - f1_18: 0.0000e+00 - f1_19: 0.9986 - f1_20: 0.0000e+00 - f1_21: 0.9242 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9937 - f1_26: 0.8742 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.3190 - f1_32: 0.9283 - f1_33: 0.0000e+00 - f1_34: 0.9509 - f1_35: 0.7812 - f1_36: 0.0000e+00 - f1_37: 0.6177 - f1_38: 0.7805 - f1_39: 0.8571 - f1_40: 0.5441 - f1_41: 0.3687 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.7900 - val_loss: 0.0641 - val_accuracy: 0.9831 - val_f1: 0.4231 - val_f1_1: 0.8398 - val_f1_2: 0.0000e+00 - val_f1_3: 0.8065 - val_f1_4: 0.7730 - val_f1_5: 0.5969 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9470 - val_f1_9: 0.0000e+00 - val_f1_11: 0.8737 - val_f1_12: 0.7011 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_16: 0.0101 - val_f1_17: 0.8370 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.9101 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9975 - val_f1_26: 0.8647 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.3697 - val_f1_32: 0.9150 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9527 - val_f1_35: 0.7279 - val_f1_36: 0.0000e+00 - val_f1_37: 0.6240 - val_f1_38: 0.7507 - val_f1_39: 0.8353 - val_f1_40: 0.4695 - val_f1_41: 0.3729 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.7499\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0561 - accuracy: 0.9854 - f1: 0.4412 - f1_1: 0.8631 - f1_2: 0.0000e+00 - f1_3: 0.8257 - f1_4: 0.8209 - f1_5: 0.7193 - f1_6: 0.0000e+00 - f1_7: 0.9437 - f1_9: 0.0000e+00 - f1_11: 0.8842 - f1_12: 0.6809 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.0923 - f1_17: 0.8590 - f1_18: 0.0000e+00 - f1_19: 0.9987 - f1_20: 0.0000e+00 - f1_21: 0.9282 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9938 - f1_26: 0.8892 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.3830 - f1_32: 0.9391 - f1_33: 0.0000e+00 - f1_34: 0.9538 - f1_35: 0.7849 - f1_36: 0.0000e+00 - f1_37: 0.6353 - f1_38: 0.7915 - f1_39: 0.8797 - f1_40: 0.5909 - f1_41: 0.3999 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.7895 - val_loss: 0.0631 - val_accuracy: 0.9830 - val_f1: 0.4314 - val_f1_1: 0.8464 - val_f1_2: 0.0000e+00 - val_f1_3: 0.8278 - val_f1_4: 0.6733 - val_f1_5: 0.6428 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9470 - val_f1_9: 0.0000e+00 - val_f1_11: 0.8867 - val_f1_12: 0.6959 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_16: 0.1159 - val_f1_17: 0.8471 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.9211 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9975 - val_f1_26: 0.8582 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.4134 - val_f1_32: 0.9161 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9542 - val_f1_35: 0.7622 - val_f1_36: 0.0000e+00 - val_f1_37: 0.6486 - val_f1_38: 0.7584 - val_f1_39: 0.8261 - val_f1_40: 0.5679 - val_f1_41: 0.4163 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.7319\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0543 - accuracy: 0.9858 - f1: 0.4505 - f1_1: 0.8653 - f1_2: 0.0000e+00 - f1_3: 0.8423 - f1_4: 0.8193 - f1_5: 0.7327 - f1_6: 0.0000e+00 - f1_7: 0.9546 - f1_9: 0.0000e+00 - f1_11: 0.8965 - f1_12: 0.6876 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_16: 0.2246 - f1_17: 0.8732 - f1_18: 0.0000e+00 - f1_19: 0.9986 - f1_20: 0.0000e+00 - f1_21: 0.9304 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9934 - f1_26: 0.9055 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.4274 - f1_32: 0.9408 - f1_33: 0.0000e+00 - f1_34: 0.9541 - f1_35: 0.7952 - f1_36: 0.0000e+00 - f1_37: 0.6486 - f1_38: 0.7989 - f1_39: 0.8974 - f1_40: 0.6050 - f1_41: 0.4272 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.8005 - val_loss: 0.0602 - val_accuracy: 0.9840 - val_f1: 0.4486 - val_f1_1: 0.8436 - val_f1_2: 0.0000e+00 - val_f1_3: 0.8239 - val_f1_4: 0.7551 - val_f1_5: 0.6428 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9692 - val_f1_9: 0.0000e+00 - val_f1_11: 0.8801 - val_f1_12: 0.7070 - val_f1_13: 0.0455 - val_f1_14: 0.0000e+00 - val_f1_16: 0.4576 - val_f1_17: 0.8985 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.9215 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9975 - val_f1_26: 0.8971 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.4303 - val_f1_32: 0.9320 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9544 - val_f1_35: 0.7683 - val_f1_36: 0.0000e+00 - val_f1_37: 0.6447 - val_f1_38: 0.7743 - val_f1_39: 0.8758 - val_f1_40: 0.5650 - val_f1_41: 0.3997 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.7584\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0523 - accuracy: 0.9864 - f1: 0.4633 - f1_1: 0.8696 - f1_2: 0.0000e+00 - f1_3: 0.8445 - f1_4: 0.8329 - f1_5: 0.7864 - f1_6: 0.0000e+00 - f1_7: 0.9538 - f1_9: 0.0000e+00 - f1_11: 0.9003 - f1_12: 0.7018 - f1_13: 0.0357 - f1_14: 0.0000e+00 - f1_16: 0.4257 - f1_17: 0.8825 - f1_18: 0.0000e+00 - f1_19: 0.9987 - f1_20: 0.0000e+00 - f1_21: 0.9331 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9938 - f1_26: 0.9187 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.4622 - f1_32: 0.9478 - f1_33: 0.0000e+00 - f1_34: 0.9533 - f1_35: 0.8055 - f1_36: 0.0000e+00 - f1_37: 0.6630 - f1_38: 0.8081 - f1_39: 0.9214 - f1_40: 0.6134 - f1_41: 0.4654 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.8151 - val_loss: 0.0591 - val_accuracy: 0.9840 - val_f1: 0.4484 - val_f1_1: 0.8531 - val_f1_2: 0.0000e+00 - val_f1_3: 0.8290 - val_f1_4: 0.7661 - val_f1_5: 0.6632 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9692 - val_f1_9: 0.0000e+00 - val_f1_11: 0.8973 - val_f1_12: 0.6789 - val_f1_13: 0.0455 - val_f1_14: 0.0000e+00 - val_f1_16: 0.3935 - val_f1_17: 0.8877 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.9294 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9975 - val_f1_26: 0.9100 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.4670 - val_f1_32: 0.9271 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9563 - val_f1_35: 0.7693 - val_f1_36: 0.0000e+00 - val_f1_37: 0.5741 - val_f1_38: 0.7852 - val_f1_39: 0.8501 - val_f1_40: 0.5081 - val_f1_41: 0.5012 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.7753\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0507 - accuracy: 0.9868 - f1: 0.4701 - f1_1: 0.8775 - f1_2: 0.0000e+00 - f1_3: 0.8505 - f1_4: 0.8353 - f1_5: 0.8013 - f1_6: 0.0000e+00 - f1_7: 0.9637 - f1_9: 0.0000e+00 - f1_11: 0.9075 - f1_12: 0.7017 - f1_13: 0.0975 - f1_14: 0.0000e+00 - f1_16: 0.4704 - f1_17: 0.9010 - f1_18: 0.0000e+00 - f1_19: 0.9986 - f1_20: 0.0000e+00 - f1_21: 0.9378 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9940 - f1_26: 0.9233 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.4915 - f1_32: 0.9507 - f1_33: 0.0000e+00 - f1_34: 0.9568 - f1_35: 0.8118 - f1_36: 0.0000e+00 - f1_37: 0.6661 - f1_38: 0.8144 - f1_39: 0.9323 - f1_40: 0.6209 - f1_41: 0.4841 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.8171 - val_loss: 0.0574 - val_accuracy: 0.9845 - val_f1: 0.4595 - val_f1_1: 0.8476 - val_f1_2: 0.0000e+00 - val_f1_3: 0.8358 - val_f1_4: 0.7672 - val_f1_5: 0.6632 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9692 - val_f1_9: 0.0000e+00 - val_f1_11: 0.9008 - val_f1_12: 0.6741 - val_f1_13: 0.0455 - val_f1_14: 0.0000e+00 - val_f1_16: 0.5280 - val_f1_17: 0.9112 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.9321 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9975 - val_f1_26: 0.9085 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.5370 - val_f1_32: 0.9463 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9587 - val_f1_35: 0.7604 - val_f1_36: 0.0000e+00 - val_f1_37: 0.6599 - val_f1_38: 0.7927 - val_f1_39: 0.9053 - val_f1_40: 0.6031 - val_f1_41: 0.4688 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.7676\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0492 - accuracy: 0.9870 - f1: 0.4787 - f1_1: 0.8798 - f1_2: 0.0000e+00 - f1_3: 0.8608 - f1_4: 0.8406 - f1_5: 0.7911 - f1_6: 0.0000e+00 - f1_7: 0.9686 - f1_9: 0.0000e+00 - f1_11: 0.9092 - f1_12: 0.7184 - f1_13: 0.1164 - f1_14: 0.0000e+00 - f1_16: 0.5825 - f1_17: 0.9199 - f1_18: 0.0000e+00 - f1_19: 0.9987 - f1_20: 0.0000e+00 - f1_21: 0.9398 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9951 - f1_26: 0.9365 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.5186 - f1_32: 0.9485 - f1_33: 0.0000e+00 - f1_34: 0.9590 - f1_35: 0.8144 - f1_36: 0.0000e+00 - f1_37: 0.6779 - f1_38: 0.8211 - f1_39: 0.9376 - f1_40: 0.6633 - f1_41: 0.5238 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.8249 - val_loss: 0.0563 - val_accuracy: 0.9847 - val_f1: 0.4626 - val_f1_1: 0.8490 - val_f1_2: 0.0000e+00 - val_f1_3: 0.8383 - val_f1_4: 0.7586 - val_f1_5: 0.6632 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9721 - val_f1_9: 0.0000e+00 - val_f1_11: 0.9022 - val_f1_12: 0.7228 - val_f1_13: 0.0455 - val_f1_14: 0.0000e+00 - val_f1_16: 0.5704 - val_f1_17: 0.9080 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.9325 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9975 - val_f1_26: 0.9148 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.5110 - val_f1_32: 0.9510 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9602 - val_f1_35: 0.7685 - val_f1_36: 0.0000e+00 - val_f1_37: 0.6905 - val_f1_38: 0.7791 - val_f1_39: 0.9074 - val_f1_40: 0.6140 - val_f1_41: 0.4736 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.7729\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0482 - accuracy: 0.9871 - f1: 0.4830 - f1_1: 0.8772 - f1_2: 0.0000e+00 - f1_3: 0.8592 - f1_4: 0.8354 - f1_5: 0.7665 - f1_6: 0.0000e+00 - f1_7: 0.9728 - f1_9: 0.0000e+00 - f1_11: 0.9061 - f1_12: 0.7347 - f1_13: 0.2426 - f1_14: 0.0000e+00 - f1_16: 0.6212 - f1_17: 0.9095 - f1_18: 0.0000e+00 - f1_19: 0.9987 - f1_20: 0.0000e+00 - f1_21: 0.9426 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9943 - f1_26: 0.9362 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.5296 - f1_32: 0.9538 - f1_33: 0.0000e+00 - f1_34: 0.9613 - f1_35: 0.8085 - f1_36: 0.0000e+00 - f1_37: 0.6842 - f1_38: 0.8125 - f1_39: 0.9500 - f1_40: 0.6535 - f1_41: 0.5440 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.8266 - val_loss: 0.0549 - val_accuracy: 0.9849 - val_f1: 0.4668 - val_f1_1: 0.8573 - val_f1_2: 0.0000e+00 - val_f1_3: 0.8496 - val_f1_4: 0.7912 - val_f1_5: 0.6632 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9857 - val_f1_9: 0.0000e+00 - val_f1_11: 0.9043 - val_f1_12: 0.7106 - val_f1_13: 0.1303 - val_f1_14: 0.0000e+00 - val_f1_16: 0.6674 - val_f1_17: 0.9311 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.9294 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9975 - val_f1_26: 0.9279 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.4852 - val_f1_32: 0.9493 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9632 - val_f1_35: 0.7961 - val_f1_36: 0.0000e+00 - val_f1_37: 0.5878 - val_f1_38: 0.8030 - val_f1_39: 0.9186 - val_f1_40: 0.5634 - val_f1_41: 0.4737 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.7861\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0466 - accuracy: 0.9876 - f1: 0.4907 - f1_1: 0.8891 - f1_2: 0.0000e+00 - f1_3: 0.8586 - f1_4: 0.8432 - f1_5: 0.7763 - f1_6: 0.0000e+00 - f1_7: 0.9764 - f1_9: 0.0000e+00 - f1_11: 0.9147 - f1_12: 0.7458 - f1_13: 0.3349 - f1_14: 0.0000e+00 - f1_16: 0.6491 - f1_17: 0.9273 - f1_18: 0.0000e+00 - f1_19: 0.9986 - f1_20: 0.0000e+00 - f1_21: 0.9417 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9949 - f1_26: 0.9403 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.5560 - f1_32: 0.9582 - f1_33: 0.0000e+00 - f1_34: 0.9632 - f1_35: 0.8215 - f1_36: 0.0000e+00 - f1_37: 0.6935 - f1_38: 0.8320 - f1_39: 0.9606 - f1_40: 0.6589 - f1_41: 0.5555 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.8381 - val_loss: 0.0536 - val_accuracy: 0.9854 - val_f1: 0.4752 - val_f1_1: 0.8589 - val_f1_2: 0.0000e+00 - val_f1_3: 0.8509 - val_f1_4: 0.7707 - val_f1_5: 0.6632 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9875 - val_f1_9: 0.0000e+00 - val_f1_11: 0.9071 - val_f1_12: 0.7091 - val_f1_13: 0.1459 - val_f1_14: 0.0000e+00 - val_f1_16: 0.6855 - val_f1_17: 0.9317 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.9335 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9975 - val_f1_26: 0.9301 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.5721 - val_f1_32: 0.9541 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9645 - val_f1_35: 0.7989 - val_f1_36: 0.0000e+00 - val_f1_37: 0.6429 - val_f1_38: 0.8025 - val_f1_39: 0.9242 - val_f1_40: 0.6365 - val_f1_41: 0.5491 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.7900\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0453 - accuracy: 0.9879 - f1: 0.4946 - f1_1: 0.8881 - f1_2: 0.0000e+00 - f1_3: 0.8704 - f1_4: 0.8446 - f1_5: 0.8202 - f1_6: 0.0000e+00 - f1_7: 0.9873 - f1_9: 0.0000e+00 - f1_11: 0.9250 - f1_12: 0.7457 - f1_13: 0.2701 - f1_14: 0.0000e+00 - f1_16: 0.6856 - f1_17: 0.9442 - f1_18: 0.0000e+00 - f1_19: 0.9985 - f1_20: 0.0000e+00 - f1_21: 0.9469 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9931 - f1_26: 0.9427 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.5697 - f1_32: 0.9672 - f1_33: 0.0000e+00 - f1_34: 0.9647 - f1_35: 0.8208 - f1_36: 0.0000e+00 - f1_37: 0.7015 - f1_38: 0.8354 - f1_39: 0.9588 - f1_40: 0.6827 - f1_41: 0.5833 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.8357 - val_loss: 0.0533 - val_accuracy: 0.9852 - val_f1: 0.4790 - val_f1_1: 0.8653 - val_f1_2: 0.0000e+00 - val_f1_3: 0.8498 - val_f1_4: 0.7954 - val_f1_5: 0.6632 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9836 - val_f1_9: 0.0000e+00 - val_f1_11: 0.8980 - val_f1_12: 0.7486 - val_f1_13: 0.1459 - val_f1_14: 0.0000e+00 - val_f1_16: 0.7772 - val_f1_17: 0.9264 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.9378 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9975 - val_f1_26: 0.9276 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.5882 - val_f1_32: 0.9617 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9660 - val_f1_35: 0.7481 - val_f1_36: 0.0000e+00 - val_f1_37: 0.6654 - val_f1_38: 0.8060 - val_f1_39: 0.9550 - val_f1_40: 0.5887 - val_f1_41: 0.5682 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.7962\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - 74s 5s/step - loss: 0.0444 - accuracy: 0.9881 - f1: 0.5027 - f1_1: 0.8956 - f1_2: 0.0000e+00 - f1_3: 0.8780 - f1_4: 0.8427 - f1_5: 0.8180 - f1_6: 0.0000e+00 - f1_7: 0.9884 - f1_9: 0.0000e+00 - f1_11: 0.9284 - f1_12: 0.7619 - f1_13: 0.4344 - f1_14: 0.0000e+00 - f1_16: 0.7243 - f1_17: 0.9460 - f1_18: 0.0000e+00 - f1_19: 0.9987 - f1_20: 0.0000e+00 - f1_21: 0.9465 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9922 - f1_26: 0.9460 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.6024 - f1_32: 0.9629 - f1_33: 0.0000e+00 - f1_34: 0.9647 - f1_35: 0.8198 - f1_36: 0.0000e+00 - f1_37: 0.7190 - f1_38: 0.8360 - f1_39: 0.9694 - f1_40: 0.6857 - f1_41: 0.6011 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.8456 - val_loss: 0.0518 - val_accuracy: 0.9856 - val_f1: 0.4839 - val_f1_1: 0.8668 - val_f1_2: 0.0000e+00 - val_f1_3: 0.8655 - val_f1_4: 0.7538 - val_f1_5: 0.6632 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9863 - val_f1_9: 0.0000e+00 - val_f1_11: 0.9099 - val_f1_12: 0.7414 - val_f1_13: 0.2069 - val_f1_14: 0.0217 - val_f1_16: 0.8076 - val_f1_17: 0.9331 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.9381 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9975 - val_f1_26: 0.9361 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.5650 - val_f1_32: 0.9600 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9671 - val_f1_35: 0.8092 - val_f1_36: 0.0000e+00 - val_f1_37: 0.6538 - val_f1_38: 0.7958 - val_f1_39: 0.9483 - val_f1_40: 0.6172 - val_f1_41: 0.6160 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.7966\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - 74s 5s/step - loss: 0.0432 - accuracy: 0.9884 - f1: 0.5071 - f1_1: 0.8932 - f1_2: 0.0000e+00 - f1_3: 0.8855 - f1_4: 0.8478 - f1_5: 0.8187 - f1_6: 0.0000e+00 - f1_7: 0.9872 - f1_9: 0.0000e+00 - f1_11: 0.9248 - f1_12: 0.7517 - f1_13: 0.4486 - f1_14: 0.0194 - f1_16: 0.8006 - f1_17: 0.9577 - f1_18: 0.0000e+00 - f1_19: 0.9986 - f1_20: 0.0000e+00 - f1_21: 0.9493 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9929 - f1_26: 0.9457 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.5994 - f1_32: 0.9721 - f1_33: 0.0000e+00 - f1_34: 0.9675 - f1_35: 0.8275 - f1_36: 0.0000e+00 - f1_37: 0.7207 - f1_38: 0.8424 - f1_39: 0.9684 - f1_40: 0.6841 - f1_41: 0.6225 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.8586 - val_loss: 0.0508 - val_accuracy: 0.9858 - val_f1: 0.4856 - val_f1_1: 0.8665 - val_f1_2: 0.0000e+00 - val_f1_3: 0.8571 - val_f1_4: 0.8004 - val_f1_5: 0.6632 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9881 - val_f1_9: 0.0000e+00 - val_f1_11: 0.9159 - val_f1_12: 0.7287 - val_f1_13: 0.2232 - val_f1_14: 0.0121 - val_f1_16: 0.8031 - val_f1_17: 0.9354 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.9427 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9975 - val_f1_26: 0.9344 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.5610 - val_f1_32: 0.9693 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9669 - val_f1_35: 0.7905 - val_f1_36: 0.0000e+00 - val_f1_37: 0.6565 - val_f1_38: 0.8107 - val_f1_39: 0.9643 - val_f1_40: 0.6164 - val_f1_41: 0.6136 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.8058\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0421 - accuracy: 0.9886 - f1: 0.5097 - f1_1: 0.9008 - f1_2: 0.0000e+00 - f1_3: 0.8905 - f1_4: 0.8504 - f1_5: 0.7890 - f1_6: 0.0000e+00 - f1_7: 0.9937 - f1_9: 0.0000e+00 - f1_11: 0.9291 - f1_12: 0.7791 - f1_13: 0.4142 - f1_14: 0.0305 - f1_16: 0.8486 - f1_17: 0.9512 - f1_18: 0.0000e+00 - f1_19: 0.9986 - f1_20: 0.0000e+00 - f1_21: 0.9506 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9913 - f1_26: 0.9541 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.6176 - f1_32: 0.9737 - f1_33: 0.0000e+00 - f1_34: 0.9683 - f1_35: 0.8314 - f1_36: 0.0000e+00 - f1_37: 0.7214 - f1_38: 0.8455 - f1_39: 0.9769 - f1_40: 0.6940 - f1_41: 0.6278 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.8581 - val_loss: 0.0499 - val_accuracy: 0.9861 - val_f1: 0.4921 - val_f1_1: 0.8710 - val_f1_2: 0.0000e+00 - val_f1_3: 0.8497 - val_f1_4: 0.7795 - val_f1_5: 0.6632 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9933 - val_f1_9: 0.0000e+00 - val_f1_11: 0.9074 - val_f1_12: 0.7623 - val_f1_13: 0.3098 - val_f1_14: 0.1025 - val_f1_16: 0.8503 - val_f1_17: 0.9449 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.9416 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 1.0000 - val_f1_26: 0.9378 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.5788 - val_f1_32: 0.9666 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9689 - val_f1_35: 0.8171 - val_f1_36: 0.0000e+00 - val_f1_37: 0.6885 - val_f1_38: 0.8127 - val_f1_39: 0.9611 - val_f1_40: 0.5887 - val_f1_41: 0.5793 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.8096\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.0412 - accuracy: 0.9889 - f1: 0.5149 - f1_1: 0.9036 - f1_2: 0.0000e+00 - f1_3: 0.8903 - f1_4: 0.8576 - f1_5: 0.8083 - f1_6: 0.0000e+00 - f1_7: 0.9899 - f1_9: 0.0000e+00 - f1_11: 0.9290 - f1_12: 0.7736 - f1_13: 0.4967 - f1_14: 0.0882 - f1_16: 0.8311 - f1_17: 0.9615 - f1_18: 0.0000e+00 - f1_19: 0.9987 - f1_20: 0.0000e+00 - f1_21: 0.9529 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9941 - f1_26: 0.9540 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.6302 - f1_32: 0.9709 - f1_33: 0.0000e+00 - f1_34: 0.9700 - f1_35: 0.8374 - f1_36: 0.0000e+00 - f1_37: 0.7227 - f1_38: 0.8524 - f1_39: 0.9801 - f1_40: 0.6966 - f1_41: 0.6480 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.8588 - val_loss: 0.0489 - val_accuracy: 0.9863 - val_f1: 0.4927 - val_f1_1: 0.8691 - val_f1_2: 0.0000e+00 - val_f1_3: 0.8579 - val_f1_4: 0.7871 - val_f1_5: 0.6632 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9933 - val_f1_9: 0.0000e+00 - val_f1_11: 0.9114 - val_f1_12: 0.7425 - val_f1_13: 0.3141 - val_f1_14: 0.0217 - val_f1_16: 0.8602 - val_f1_17: 0.9560 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.9430 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 1.0000 - val_f1_26: 0.9464 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.5989 - val_f1_32: 0.9687 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9724 - val_f1_35: 0.8185 - val_f1_36: 0.0000e+00 - val_f1_37: 0.6800 - val_f1_38: 0.8159 - val_f1_39: 0.9676 - val_f1_40: 0.6284 - val_f1_41: 0.5896 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.8005\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0401 - accuracy: 0.9892 - f1: 0.5194 - f1_1: 0.9106 - f1_2: 0.0000e+00 - f1_3: 0.9030 - f1_4: 0.8598 - f1_5: 0.7954 - f1_6: 0.0000e+00 - f1_7: 0.9910 - f1_9: 0.0000e+00 - f1_11: 0.9282 - f1_12: 0.7936 - f1_13: 0.4804 - f1_14: 0.1171 - f1_16: 0.9053 - f1_17: 0.9651 - f1_18: 0.0000e+00 - f1_19: 0.9986 - f1_20: 0.0000e+00 - f1_21: 0.9526 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9926 - f1_26: 0.9550 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.6389 - f1_32: 0.9738 - f1_33: 0.0000e+00 - f1_34: 0.9710 - f1_35: 0.8389 - f1_36: 0.0000e+00 - f1_37: 0.7362 - f1_38: 0.8533 - f1_39: 0.9802 - f1_40: 0.7021 - f1_41: 0.6704 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.8610 - val_loss: 0.0488 - val_accuracy: 0.9862 - val_f1: 0.4949 - val_f1_1: 0.8749 - val_f1_2: 0.0000e+00 - val_f1_3: 0.8628 - val_f1_4: 0.8076 - val_f1_5: 0.6632 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9943 - val_f1_9: 0.0000e+00 - val_f1_11: 0.9204 - val_f1_12: 0.7784 - val_f1_13: 0.3384 - val_f1_14: 0.0866 - val_f1_16: 0.8218 - val_f1_17: 0.9450 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.9500 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 1.0000 - val_f1_26: 0.9507 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.6062 - val_f1_32: 0.9715 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9740 - val_f1_35: 0.7873 - val_f1_36: 0.0000e+00 - val_f1_37: 0.6723 - val_f1_38: 0.8203 - val_f1_39: 0.9616 - val_f1_40: 0.6136 - val_f1_41: 0.5712 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.8246\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0395 - accuracy: 0.9892 - f1: 0.5242 - f1_1: 0.9101 - f1_2: 0.0000e+00 - f1_3: 0.8994 - f1_4: 0.8587 - f1_5: 0.8269 - f1_6: 0.0000e+00 - f1_7: 0.9902 - f1_9: 0.0000e+00 - f1_11: 0.9341 - f1_12: 0.8043 - f1_13: 0.5343 - f1_14: 0.1752 - f1_16: 0.8930 - f1_17: 0.9673 - f1_18: 0.0000e+00 - f1_19: 0.9987 - f1_20: 0.0000e+00 - f1_21: 0.9558 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9949 - f1_26: 0.9562 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.6595 - f1_32: 0.9748 - f1_33: 0.0000e+00 - f1_34: 0.9712 - f1_35: 0.8385 - f1_36: 0.0000e+00 - f1_37: 0.7365 - f1_38: 0.8611 - f1_39: 0.9800 - f1_40: 0.7120 - f1_41: 0.6652 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.8709 - val_loss: 0.0478 - val_accuracy: 0.9865 - val_f1: 0.5008 - val_f1_1: 0.8778 - val_f1_2: 0.0000e+00 - val_f1_3: 0.8800 - val_f1_4: 0.7987 - val_f1_5: 0.6632 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9943 - val_f1_9: 0.0000e+00 - val_f1_11: 0.9272 - val_f1_12: 0.7898 - val_f1_13: 0.3461 - val_f1_14: 0.0682 - val_f1_16: 0.8807 - val_f1_17: 0.9525 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.9460 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 1.0000 - val_f1_26: 0.9499 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.6035 - val_f1_32: 0.9753 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9738 - val_f1_35: 0.7964 - val_f1_36: 0.0000e+00 - val_f1_37: 0.6933 - val_f1_38: 0.8206 - val_f1_39: 0.9613 - val_f1_40: 0.6424 - val_f1_41: 0.6816 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.8084\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0384 - accuracy: 0.9896 - f1: 0.5264 - f1_1: 0.9151 - f1_2: 0.0000e+00 - f1_3: 0.9012 - f1_4: 0.8674 - f1_5: 0.8140 - f1_6: 0.0000e+00 - f1_7: 0.9912 - f1_9: 0.0000e+00 - f1_11: 0.9359 - f1_12: 0.8042 - f1_13: 0.5277 - f1_14: 0.1739 - f1_16: 0.9072 - f1_17: 0.9678 - f1_18: 0.0000e+00 - f1_19: 0.9985 - f1_20: 0.0250 - f1_21: 0.9551 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9938 - f1_26: 0.9597 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.6611 - f1_32: 0.9778 - f1_33: 0.0000e+00 - f1_34: 0.9738 - f1_35: 0.8451 - f1_36: 0.0000e+00 - f1_37: 0.7423 - f1_38: 0.8602 - f1_39: 0.9824 - f1_40: 0.7138 - f1_41: 0.6893 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.8731 - val_loss: 0.0470 - val_accuracy: 0.9867 - val_f1: 0.5030 - val_f1_1: 0.8829 - val_f1_2: 0.0000e+00 - val_f1_3: 0.8826 - val_f1_4: 0.8013 - val_f1_5: 0.6691 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9943 - val_f1_9: 0.0000e+00 - val_f1_11: 0.9303 - val_f1_12: 0.7855 - val_f1_13: 0.3866 - val_f1_14: 0.0761 - val_f1_16: 0.8768 - val_f1_17: 0.9594 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.9492 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 1.0000 - val_f1_26: 0.9535 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.6073 - val_f1_32: 0.9817 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9756 - val_f1_35: 0.8080 - val_f1_36: 0.0000e+00 - val_f1_37: 0.6649 - val_f1_38: 0.8237 - val_f1_39: 0.9703 - val_f1_40: 0.6682 - val_f1_41: 0.6595 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.8118\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0375 - accuracy: 0.9899 - f1: 0.5295 - f1_1: 0.9151 - f1_2: 0.0000e+00 - f1_3: 0.9108 - f1_4: 0.8687 - f1_5: 0.8197 - f1_6: 0.0000e+00 - f1_7: 0.9925 - f1_9: 0.0000e+00 - f1_11: 0.9320 - f1_12: 0.8276 - f1_13: 0.5762 - f1_14: 0.2134 - f1_16: 0.8962 - f1_17: 0.9685 - f1_18: 0.0000e+00 - f1_19: 0.9986 - f1_20: 0.0000e+00 - f1_21: 0.9569 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9942 - f1_26: 0.9609 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.6573 - f1_32: 0.9803 - f1_33: 0.0000e+00 - f1_34: 0.9750 - f1_35: 0.8494 - f1_36: 0.0000e+00 - f1_37: 0.7518 - f1_38: 0.8687 - f1_39: 0.9838 - f1_40: 0.7200 - f1_41: 0.6851 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.8773 - val_loss: 0.0465 - val_accuracy: 0.9869 - val_f1: 0.5054 - val_f1_1: 0.8764 - val_f1_2: 0.0000e+00 - val_f1_3: 0.8794 - val_f1_4: 0.8059 - val_f1_5: 0.6719 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9943 - val_f1_9: 0.0000e+00 - val_f1_11: 0.9349 - val_f1_12: 0.7732 - val_f1_13: 0.3864 - val_f1_14: 0.1195 - val_f1_16: 0.8801 - val_f1_17: 0.9594 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.9485 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 1.0000 - val_f1_26: 0.9560 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.6313 - val_f1_32: 0.9835 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9757 - val_f1_35: 0.7947 - val_f1_36: 0.0000e+00 - val_f1_37: 0.7068 - val_f1_38: 0.8208 - val_f1_39: 0.9683 - val_f1_40: 0.6414 - val_f1_41: 0.6886 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.8182\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.0370 - accuracy: 0.9900 - f1: 0.5324 - f1_1: 0.9154 - f1_2: 0.0000e+00 - f1_3: 0.9147 - f1_4: 0.8680 - f1_5: 0.8142 - f1_6: 0.0000e+00 - f1_7: 0.9959 - f1_9: 0.0000e+00 - f1_11: 0.9351 - f1_12: 0.8101 - f1_13: 0.6236 - f1_14: 0.2146 - f1_16: 0.9282 - f1_17: 0.9711 - f1_18: 0.0000e+00 - f1_19: 0.9985 - f1_20: 0.0000e+00 - f1_21: 0.9571 - f1_22: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9939 - f1_26: 0.9610 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.6814 - f1_32: 0.9787 - f1_33: 0.0000e+00 - f1_34: 0.9750 - f1_35: 0.8516 - f1_36: 0.0000e+00 - f1_37: 0.7556 - f1_38: 0.8655 - f1_39: 0.9825 - f1_40: 0.7158 - f1_41: 0.7071 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.8816 - val_loss: 0.0456 - val_accuracy: 0.9872 - val_f1: 0.5073 - val_f1_1: 0.8793 - val_f1_2: 0.0000e+00 - val_f1_3: 0.8806 - val_f1_4: 0.8107 - val_f1_5: 0.6719 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9882 - val_f1_9: 0.0000e+00 - val_f1_11: 0.9252 - val_f1_12: 0.7881 - val_f1_13: 0.4373 - val_f1_14: 0.1266 - val_f1_16: 0.8822 - val_f1_17: 0.9619 - val_f1_18: 0.0000e+00 - val_f1_19: 1.0000 - val_f1_20: 0.0000e+00 - val_f1_21: 0.9503 - val_f1_22: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 1.0000 - val_f1_26: 0.9578 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.6290 - val_f1_32: 0.9835 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9759 - val_f1_35: 0.8066 - val_f1_36: 0.0000e+00 - val_f1_37: 0.7136 - val_f1_38: 0.8280 - val_f1_39: 0.9697 - val_f1_40: 0.6623 - val_f1_41: 0.6446 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.8189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrM_XiQ1Bdpp"
      },
      "source": [
        "loss: 0.1255 - accuracy: 0.9631\n",
        "\n",
        "\n",
        "loss: 0.0583 - accuracy: 0.9840 - val_loss: 0.0741 - val_accuracy: 0.9790\n",
        "\n",
        "loss: 0.0589 - accuracy: 0.9836 - val_loss: 0.0711 - val_accuracy: 0.9798"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#History f1 for class"
      ],
      "metadata": {
        "id": "87_kBxnCwm0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_val_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_val_list[i] = history.history['val_f1_{}'.format(i)][-1]\n",
        "f1_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_list[i] = history.history['f1_{}'.format(i)][-1]"
      ],
      "metadata": {
        "id": "nbNUZ6RoZ_qp"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index2tag = {value : key for (key, value) in tag2index.items()}\n",
        "index2tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHprJp5QaCIj",
        "outputId": "6f2fc153-a8b7-41a2-c53d-f40c5fbbf204"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '-PAD-',\n",
              " 1: 'VB',\n",
              " 2: 'WP$',\n",
              " 3: 'CD',\n",
              " 4: 'NNP',\n",
              " 5: 'WP',\n",
              " 6: '-RRB-',\n",
              " 7: 'POS',\n",
              " 8: \"''\",\n",
              " 9: 'UH',\n",
              " 10: '``',\n",
              " 11: 'VBZ',\n",
              " 12: 'VBP',\n",
              " 13: 'RP',\n",
              " 14: 'JJR',\n",
              " 15: ':',\n",
              " 16: 'WDT',\n",
              " 17: 'MD',\n",
              " 18: 'NNPS',\n",
              " 19: 'TO',\n",
              " 20: 'RBR',\n",
              " 21: 'IN',\n",
              " 22: 'PDT',\n",
              " 23: '.',\n",
              " 24: 'JJS',\n",
              " 25: '$',\n",
              " 26: 'PRP',\n",
              " 27: '#',\n",
              " 28: 'SYM',\n",
              " 29: '-LRB-',\n",
              " 30: 'VBG',\n",
              " 31: ',',\n",
              " 32: 'PRP$',\n",
              " 33: 'WRB',\n",
              " 34: 'DT',\n",
              " 35: 'NN',\n",
              " 36: 'LS',\n",
              " 37: 'JJ',\n",
              " 38: 'NNS',\n",
              " 39: 'CC',\n",
              " 40: 'VBN',\n",
              " 41: 'RB',\n",
              " 42: 'FW',\n",
              " 43: 'EX',\n",
              " 44: 'RBS',\n",
              " 45: 'VBD'}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- F1: {}'.format(index2tag[i], f1_list[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoPk9XRiaEgx",
        "outputId": "aa1e6470-015d-44ae-fc96-ea79e32a5145"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: VB --- F1: 0.9154009819030762\n",
            "Tag: WP$ --- F1: 0.0\n",
            "Tag: CD --- F1: 0.9146551489830017\n",
            "Tag: NNP --- F1: 0.8680282235145569\n",
            "Tag: WP --- F1: 0.8142470717430115\n",
            "Tag: -RRB- --- F1: 0.0\n",
            "Tag: POS --- F1: 0.9958662986755371\n",
            "Tag: UH --- F1: 0.0\n",
            "Tag: VBZ --- F1: 0.935062050819397\n",
            "Tag: VBP --- F1: 0.8100611567497253\n",
            "Tag: RP --- F1: 0.6235533356666565\n",
            "Tag: JJR --- F1: 0.2146080583333969\n",
            "Tag: WDT --- F1: 0.9282128810882568\n",
            "Tag: MD --- F1: 0.9711101651191711\n",
            "Tag: NNPS --- F1: 0.0\n",
            "Tag: TO --- F1: 0.998457670211792\n",
            "Tag: RBR --- F1: 0.0\n",
            "Tag: IN --- F1: 0.9571360349655151\n",
            "Tag: PDT --- F1: 0.0\n",
            "Tag: JJS --- F1: 0.0\n",
            "Tag: $ --- F1: 0.9938913583755493\n",
            "Tag: PRP --- F1: 0.9609610438346863\n",
            "Tag: # --- F1: 0.0\n",
            "Tag: SYM --- F1: 0.0\n",
            "Tag: -LRB- --- F1: 0.0\n",
            "Tag: VBG --- F1: 0.6813547611236572\n",
            "Tag: PRP$ --- F1: 0.9787431359291077\n",
            "Tag: WRB --- F1: 0.0\n",
            "Tag: DT --- F1: 0.9750412106513977\n",
            "Tag: NN --- F1: 0.8516353368759155\n",
            "Tag: LS --- F1: 0.0\n",
            "Tag: JJ --- F1: 0.7556238770484924\n",
            "Tag: NNS --- F1: 0.8654959201812744\n",
            "Tag: CC --- F1: 0.9825214147567749\n",
            "Tag: VBN --- F1: 0.7158292531967163\n",
            "Tag: RB --- F1: 0.7071256041526794\n",
            "Tag: FW --- F1: 0.0\n",
            "Tag: EX --- F1: 0.0\n",
            "Tag: RBS --- F1: 0.0\n",
            "Tag: VBD --- F1: 0.8815503120422363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- Val_F1: {}'.format(index2tag[i], f1_val_list[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AIxv0xZaG9M",
        "outputId": "0cb692a8-46d8-44e4-d93d-e17974e0eaf6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: VB --- Val_F1: 0.8793327212333679\n",
            "Tag: WP$ --- Val_F1: 0.0\n",
            "Tag: CD --- Val_F1: 0.8805593848228455\n",
            "Tag: NNP --- Val_F1: 0.8107401132583618\n",
            "Tag: WP --- Val_F1: 0.6718615293502808\n",
            "Tag: -RRB- --- Val_F1: 0.0\n",
            "Tag: POS --- Val_F1: 0.9881865978240967\n",
            "Tag: UH --- Val_F1: 0.0\n",
            "Tag: VBZ --- Val_F1: 0.9251892566680908\n",
            "Tag: VBP --- Val_F1: 0.7881254553794861\n",
            "Tag: RP --- Val_F1: 0.4372621774673462\n",
            "Tag: JJR --- Val_F1: 0.1265743374824524\n",
            "Tag: WDT --- Val_F1: 0.8821915984153748\n",
            "Tag: MD --- Val_F1: 0.9618918895721436\n",
            "Tag: NNPS --- Val_F1: 0.0\n",
            "Tag: TO --- Val_F1: 1.0\n",
            "Tag: RBR --- Val_F1: 0.0\n",
            "Tag: IN --- Val_F1: 0.9502565860748291\n",
            "Tag: PDT --- Val_F1: 0.0\n",
            "Tag: JJS --- Val_F1: 0.0\n",
            "Tag: $ --- Val_F1: 1.0\n",
            "Tag: PRP --- Val_F1: 0.9578094482421875\n",
            "Tag: # --- Val_F1: 0.0\n",
            "Tag: SYM --- Val_F1: 0.0\n",
            "Tag: -LRB- --- Val_F1: 0.0\n",
            "Tag: VBG --- Val_F1: 0.6290431618690491\n",
            "Tag: PRP$ --- Val_F1: 0.9835168719291687\n",
            "Tag: WRB --- Val_F1: 0.0\n",
            "Tag: DT --- Val_F1: 0.9759488701820374\n",
            "Tag: NN --- Val_F1: 0.8065626621246338\n",
            "Tag: LS --- Val_F1: 0.0\n",
            "Tag: JJ --- Val_F1: 0.7135850191116333\n",
            "Tag: NNS --- Val_F1: 0.8280302286148071\n",
            "Tag: CC --- Val_F1: 0.9696735143661499\n",
            "Tag: VBN --- Val_F1: 0.6623349785804749\n",
            "Tag: RB --- Val_F1: 0.6446226239204407\n",
            "Tag: FW --- Val_F1: 0.0\n",
            "Tag: EX --- Val_F1: 0.0\n",
            "Tag: RBS --- Val_F1: 0.0\n",
            "Tag: VBD --- Val_F1: 0.8188766241073608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Infos"
      ],
      "metadata": {
        "id": "tdQOOMeBaK6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 40 epochs, batch 128\n",
        "loss: 0.0370 - accuracy: 0.9900 - f1: 0.5324"
      ],
      "metadata": {
        "id": "sjumesASaNsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_without_point = []\n",
        "\n",
        "for i in test:\n",
        "  if i[0] != '':\n",
        "    if i[1] != '.' and i[1] != ',' and i[1] != '``' and i[1] != \"''\":\n",
        "      test_without_point += [i]\n",
        "  else: test_without_point += ''\n",
        "\n",
        "#print(test_without_point)\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in test_without_point:\n",
        "  if i != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    test_sentences.append(s)\n",
        "    test_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "test_sentences_X,  test_tags_y = [], []\n",
        "\n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        "\n",
        "for s in test_tags:\n",
        "    test_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ],
      "metadata": {
        "id": "sUZ9F2amtq-V"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_pred = model.predict(test_sentences_X)\n",
        "### (3) Get performance metrics after each fold and send to Neptune ###\n",
        "\n",
        "f1_test = f1(test_tags_y, y_val_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "b7uGU-eftvUI",
        "outputId": "b4d07b1d-3f7a-45bd-8a7e-34197d33ec4d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-78c679b5ea8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m### (3) Get performance metrics after each fold and send to Neptune ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mf1_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tags_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-baac7da70544>\u001b[0m in \u001b[0;36mf1\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mclass_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mno_punct_indexes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0my_true_single_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0my_pred_single_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mf1_single\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_single_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_single_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 2-dimensional, but 3 were indexed"
          ]
        }
      ]
    }
  ]
}