{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1_Lstm_single.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Due to**: 23/12/2021 (dd/mm/yyyy)\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Part-of Speech (POS) tagging as Sequence Labelling using Recurrent Neural Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4_wqPdlBcKS"
      },
      "source": [
        "# Intro\n",
        "\n",
        "In this assignment  we will ask you to perform POS tagging using neural architectures\n",
        "\n",
        "You are asked to follow these steps:\n",
        "*   Download the corpora and split it in training and test sets, structuring a dataframe.\n",
        "*   Embed the words using GloVe embeddings\n",
        "*   Create a baseline model, using a simple neural architecture\n",
        "*   Experiment doing small modifications to the baseline model, choose hyperparameters using the validation set\n",
        "*   Evaluate your two best model\n",
        "*   Analyze the errors of your model\n",
        "\n",
        "\n",
        "**Task**: given a corpus of documents, predict the POS tag for each word\n",
        "\n",
        "**Corpus**:\n",
        "Ignore the numeric value in the third column, use only the words/symbols and its label. \n",
        "The corpus is available at:\n",
        "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
        "\n",
        "**Splits**: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
        "\n",
        "\n",
        "**Features**: you MUST use GloVe embeddings as the only input features to the model.\n",
        "\n",
        "**Splitting**: you can decide to split documents into sentences or not, the choice is yours.\n",
        "\n",
        "**I/O structure**: The input data will have three dimensions: 1-documents/sentences, 2-token, 3-features; for the output there are 2 possibilities: if you use one-hot encoding it will be 1-documents/sentences, 2-token labels, 3-classes, if you use a single integer that indicates the number of the class it will be 1-documents/sentences, 2-token labels.\n",
        "\n",
        "**Baseline**: two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top; the choice of hyper-parameters is yours.\n",
        "\n",
        "**Architectures**: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and adding an additional dense layer; do not mix these variantions.\n",
        "\n",
        "\n",
        "**Training and Experiments**: all the experiments must involve only the training and validation sets.\n",
        "\n",
        "**Evaluation**: in the end, only the two best models of your choice (according to the validation set) must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech. DO NOT CONSIDER THE PUNCTUATION CLASSES.\n",
        "\n",
        "**Metrics**: the metric you must use to evaluate your final model is the F1-macro, WITHOUT considering punctuation/symbols classes; during the training process you can use accuracy because you can't use the F1 metric unless you use a single (gigantic) batch because there is no way to aggregate \"partial\" F1 scores computed on mini-batches.\n",
        "\n",
        "**Discussion and Error Analysis** : verify and discuss if the results on the test sets are coherent with those on the validation set; analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
        "\n",
        "**Report**: you are asked to deliver the code of your experiments and a small pdf report of about 2 pages; the pdf must begin with the names of the people of your team and a small abstract (4-5 lines) that sums up your findings.\n",
        "\n",
        "# Out Of Vocabulary (OOV) terms\n",
        "\n",
        "How to handle words that are not in GloVe vocabulary?\n",
        "You can handle them as you want (random embedding, placeholder, whatever!), but they must be STATIC embeddings (you cannot train them).\n",
        "\n",
        "But there is a very important caveat! As usual, the element of the test set must not influence the elements of the other splits!\n",
        "\n",
        "So, when you compute new embeddings for train+validation, you must forget about test documents.\n",
        "The motivation is to emulate a real-world scenario, where you select and train a model in the first stage, without knowing nothing about the testing environment.\n",
        "\n",
        "For implementation convenience, you CAN use a single vocabulary file/matrix/whatever. The principle of the previous point is that the embeddings inside that file/matrix must be generated independently for train and test splits.\n",
        "\n",
        "Basically in a real-world scenario, this is what would happen:\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Training of the model(s)\n",
        "5. Compute embeddings for terms OOV2 of the validation split \n",
        "6. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "7. Validation of the model(s)\n",
        "8. Compute embeddings for terms OOV3 of the test split \n",
        "9. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2+OOV3\n",
        "10. Testing of the final model\n",
        "\n",
        "In this case, where we already have all the documents, we can simplify the process a bit, but the procedure must remain rigorous.\n",
        "\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Compute embeddings for terms OOV2 of the validation split \n",
        "5. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "6. Compute embeddings for terms OOV3 of the test split \n",
        "7. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2\n",
        "8. Training of the model(s)\n",
        "9. Validation of the model(s)\n",
        "10. Testing of the final model\n",
        "\n",
        "Step 2 and step 6 must be completely independent of each other, for what concerns the method and the documents. But they can rely on the previous vocabulary (V1 for step 2 and V3 for step 6)\n",
        "THEREFORE if a word is present both in the training set and the test split and not in the starting vocabulary, its embedding is computed in step 2) and it is not considered OOV anymore in step 6).\n",
        "\n",
        "# Report\n",
        "The report must not be just a copy and paste of graphs and tables!\n",
        "\n",
        "The report must not be longer than 2 pages and must contain:\n",
        "* The names of the member of your team\n",
        "* A short abstract (4-5 lines) that sum ups everything\n",
        "* A general description of the task you have addressed and how you have addressed it\n",
        "* A short description of the models you have used\n",
        "* Some tables that sum up your findings in validation and test and a discussion of those results\n",
        "* The most relevant findings of your error analysis\n",
        "\n",
        "# Evaluation Criterion\n",
        "\n",
        "The goal of this assignment is not to prove you can find best model ever, but to face a common task, structure it correctly, and follow a correct and rigorous experimental procedure.\n",
        "In other words, we don't care if you final models are awful as long as you have followed the correct procedure and wrote a decent report.\n",
        "\n",
        "The score of the assignment will be computed roughly as follows\n",
        "* 1 point for the general setting of the problem\n",
        "* 1 point for the handling of OOV terms\n",
        "* 1 point for the models\n",
        "* 1 point for train-validation-test procedure\n",
        "* 2 point for the discussion of the results, error analysis, and report\n",
        "\n",
        "This distribution of scores is tentative and we may decide to alter it at any moment.\n",
        "We also reserve the right to assign a small bonus (0.5 points) to any assignment that is particularly worthy. Similarly, in case of grave errors, we may decide to assign an equivalent malus (-0.5 points).\n",
        "\n",
        "# Contacts\n",
        "\n",
        "In case of any doubt, question, issue, or help we highly recommend you to check the [course useful material](https://virtuale.unibo.it/pluginfile.php/1036039/mod_resource/content/2/NLP_Course_Useful_Material.pdf) for additional information, and to use the Virtuale forums to discuss with other students.\n",
        "\n",
        "You can always contact us at the following email addresses. To increase the probability of a prompt response, we reccomend you to write to both the teaching assistants.\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Andrea Galassi -> a.galassi@unibo.it\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it\n",
        "\n",
        "\n",
        "# FAQ\n",
        "* You can use a non-trainable Embedding layer to load the glove embeddings\n",
        "* You can use any library of your choice to implement the networks. Two options are tensorflow/keras or pythorch. Both these libraries have all the classes you need to implement these simple architectures and there are plenty of tutorials around, where you can learn how to use them."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Dataset"
      ],
      "metadata": {
        "id": "COWq7PIOWtYr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emdubW66fnlD"
      },
      "source": [
        "import os, shutil  #  file management\n",
        "import sys \n",
        "import pandas as pd  #  dataframe management\n",
        "import numpy as np  #  data manipulation\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request  #  download files\n",
        "import zipfile  #  unzip files\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B-w2pqyfpF1"
      },
      "source": [
        "from keras import metrics \n",
        "from tensorflow.python.keras.metrics import Metric\n",
        "from sklearn.metrics import f1_score\n",
        "import keras.backend as K"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8prqSTRX-TK",
        "outputId": "6e2b350c-d5f3-4c62-fc89-1efa20917693"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3xn1MZPgA_v",
        "outputId": "b4dbc00f-0c36-47a3-f5de-1bbc1e7b1354"
      },
      "source": [
        "dataset_folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\")\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "    os.makedirs(dataset_folder)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
        "\n",
        "dataset_path = os.path.join(dataset_folder, \"treebank.zip\")\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    urllib.request.urlretrieve(url, dataset_path)\n",
        "    print(\"Successful download\")\n",
        "\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_folder)\n",
        "print(\"Successful extraction\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful download\n",
            "Successful extraction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre Processing\n",
        "- splitting"
      ],
      "metadata": {
        "id": "iXl3LpVxW2h5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKbUsrVHhGAG"
      },
      "source": [
        "dataset_name = \"dependency_treebank\"\n",
        "\n",
        "\n",
        "folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\", dataset_name)\n",
        "\n",
        "pre_train = []\n",
        "pre_valid = []\n",
        "pre_test = []\n",
        "i = 1\n",
        "\n",
        "file_list = sorted(os.listdir(folder))\n",
        "\n",
        "for filename in file_list:\n",
        "  file_path = os.path.join(folder, filename)\n",
        "  if os.path.isfile(file_path):\n",
        "    # open the file\n",
        "      text = []\n",
        "      with open(file_path, mode='r', encoding='utf-8') as text_file:\n",
        "        text = text_file.read()\n",
        "        if i <= 100:\n",
        "          pre_train.append(text)\n",
        "        elif i <= 150:\n",
        "          pre_valid.append(text)\n",
        "        else:\n",
        "          pre_test.append(text)  \n",
        "  i+=1\n",
        "\n",
        "tr = []\n",
        "val = []\n",
        "tes = []\n",
        "\n",
        "for paragraph in pre_train:\n",
        "   tr.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_valid:\n",
        "   val.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_test:\n",
        "   tes.append(paragraph.split('\\n'))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUgbTXi-F7Sa"
      },
      "source": [
        "train = []\n",
        "valid = []\n",
        "test = []\n",
        "\n",
        "for i in tr:\n",
        "  for j in i:\n",
        "    train.append(j.split('\\t'))\n",
        "\n",
        "for i in val:\n",
        "  for j in i:\n",
        "    valid.append(j.split('\\t'))\n",
        "\n",
        "for i in tes:\n",
        "  for j in i:\n",
        "    test.append(j.split('\\t'))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmzVaRuQFp0M"
      },
      "source": [
        "train_sentences = []\n",
        "valid_sentences = []\n",
        "test_sentences = []\n",
        "train_tags = []\n",
        "valid_tags = []\n",
        "test_tags = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in train:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    train_sentences.append(s)\n",
        "    train_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in valid:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    valid_sentences.append(s)\n",
        "    valid_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in test:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    test_sentences.append(s)\n",
        "    test_tags.append(t)\n",
        "    s = []\n",
        "    t = []"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB65cUFLIeEr"
      },
      "source": [
        "flat_train = [item for sublist in train_sentences for item in sublist]\n",
        "flat_valid = [item for sublist in valid_sentences for item in sublist]\n",
        "flat_test = [item for sublist in test_sentences for item in sublist]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "- vocabularies\n",
        "- GloVe download"
      ],
      "metadata": {
        "id": "VQsxPZcXW9EF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZO6ga1EGHdh"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "train_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "train_tokenizer.fit_on_texts(train_sentences)                    # fit tokeniser on data\n",
        "train_encoded = train_tokenizer.texts_to_sequences(train_sentences)\n",
        "\n",
        "valid_tokenizer = Tokenizer()       \n",
        "valid_tokenizer.fit_on_texts(valid_sentences)                  # instantiate tokeniser\n",
        "valid_encoded = valid_tokenizer.texts_to_sequences(valid_sentences)\n",
        "\n",
        "test_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "test_tokenizer.fit_on_texts(test_sentences)                    # fit tokeniser on data\n",
        "test_encoded = test_tokenizer.texts_to_sequences(test_sentences)  # use the tokeniser to encode input sequence"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(train_tags)\n",
        "tags_encoded = tag_tokenizer.texts_to_sequences(train_tags)"
      ],
      "metadata": {
        "id": "1nC8ma_jXYtc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMsG0Yw1l50L",
        "outputId": "69f61c0a-bca6-4283-a3ee-244e21bf922f"
      },
      "source": [
        "voc = list(set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(valid_tokenizer.word_index.keys()) - set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(test_tokenizer.word_index.keys()) - set(voc))\n",
        "print(len(voc))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7404\n",
            "9901\n",
            "10947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS9RwnOtJuas"
      },
      "source": [
        "word_index = dict(zip(voc, range(2, len(voc)+3)))\n",
        "word_index['-PAD-'] = 0\n",
        "word_index['-OOV-'] = 1\n",
        "\n",
        "tags = set([item for sublist in train_tags for item in sublist])\n",
        "\n",
        "tag2index = {t: i + 1 for i, t in enumerate(tags)}\n",
        "tag2index['-PAD-'] = 0 "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoj4Zx51J2MU",
        "outputId": "f02f0829-b75d-4cfa-bdea-98cf053e01b5"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-17 15:18:41--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-12-17 15:18:41--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-12-17 15:18:42--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.10MB/s    in 2m 41s  \n",
            "\n",
            "2021-12-17 15:21:23 (5.10 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc-T6FcLKtS9",
        "outputId": "f0f80b70-124b-45c3-80b3-7ed42ff5cc63"
      },
      "source": [
        "path_to_glove_file = os.path.join(os.getcwd(), \"glove.6B.100d.txt\")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding\n",
        "- matrix creation\n",
        "- padding\n",
        "- one hot encoding of tags\n",
        "- punctuation identification"
      ],
      "metadata": {
        "id": "_x9MpHE6XB2l"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0mOStEPMeXt",
        "outputId": "2e19c24b-3c0d-45ad-8fac-00205e572d57"
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        embedding_matrix += np.random.uniform(low=-0.05, high=0.05, size=embedding_dim)\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 10271 words (678 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwoJ4hbYXFPq"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpYrHs0RMtQJ"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8EZM31Js06-",
        "outputId": "528c4751-727c-41aa-8f1b-f82b54d6062a"
      },
      "source": [
        "MAX_LENGTH = len(max(train_sentences, key=len))\n",
        "print(MAX_LENGTH)\n",
        "MAX_LENGTH2 = len(max(valid_sentences, key=len))\n",
        "print(MAX_LENGTH2)\n",
        "MAX_LENGTH3 = len(max(test_sentences, key=len))\n",
        "print(MAX_LENGTH3)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "249\n",
            "81\n",
            "58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSRa9SwTuNrX"
      },
      "source": [
        "train_sentences_X, valid_sentences_X, train_tags_y, valid_tags_y = [], [], [], []\n",
        " \n",
        "for s in train_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        "\n",
        "for s in valid_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    valid_sentences_X.append(s_int)\n",
        " \n",
        "for s in train_tags:\n",
        "    train_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "for s in valid_tags:\n",
        "    valid_tags_y.append([tag2index[t] for t in s])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf-XlSrStQx3"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_sentences_X = pad_sequences(valid_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_tags_y = pad_sequences(valid_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)"
      ],
      "metadata": {
        "id": "yc_u-8itYXQB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "point = [tag2index['.']]\n",
        "virg = [tag2index[',']]\n",
        "weird_apex = [tag2index['``']]\n",
        "single_apex = [tag2index[\"''\"]]\n",
        "two_dots = [tag2index[':']]\n",
        "pad = [tag2index['-PAD-']]\n",
        "\n",
        "punct_cat_classes = to_categorical([point, virg, weird_apex, single_apex, two_dots, pad], len(tag2index))\n",
        "punct_cat_classes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tvb1ZbVIYZ4Y",
        "outputId": "09813496-97a8-449a-ff31-19761ce11e74"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 1, 46)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cum_tags = np.zeros(46)\n",
        "for i in punct_cat_classes:\n",
        "  print(i[0])\n",
        "  cum_tags += i[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7oHM_vUYf4c",
        "outputId": "6a784ba0-db06-414d-b81f-45cd7955ed80"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "where_tags = np.where(np.logical_not(cum_tags))\n",
        "where_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5uT2QJrYi15",
        "outputId": "159ba13f-9b0e-4eed-edcf-6290756f72af"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 1,  2,  3,  4,  5,  6,  7,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
              "        20, 21, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
              "        39, 40, 41, 43, 44, 45]),)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_punct_indexes = where_tags[0]\n",
        "n_classes = len(no_punct_indexes)"
      ],
      "metadata": {
        "id": "nPBHVI2HYlxs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "cat_val_tags_y = to_categorical(valid_tags_y, len(tag2index))\n",
        "print(len(cat_train_tags_y), len(cat_val_tags_y))\n",
        "print(len(train_sentences_X), len(valid_sentences_X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SPgQ9YmYnbG",
        "outputId": "0c1027e8-2c38-4538-98db-9fe71e6e0cde"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1963 1299\n",
            "1963 1299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mini Data Vizualization Inset"
      ],
      "metadata": {
        "id": "426ajTPoYtaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_tags = np.zeros(46)\n",
        "for i in cat_train_tags_y:\n",
        "  for t in i:\n",
        "    distribution_tags += t"
      ],
      "metadata": {
        "id": "Jf_djyP3YvhV"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = sum(distribution_tags[1:])\n",
        "norm = [float(i)/s for i in distribution_tags[1:]]"
      ],
      "metadata": {
        "id": "8jqHRmS_Yyde"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt "
      ],
      "metadata": {
        "id": "-ksL6xl1Y2Sw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(np.arange(len(norm)),norm)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "o4EY5adaY3Z6",
        "outputId": "a59c3b27-f3c9-424f-cbc5-ccd2db46aeab"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#F1 Metric"
      ],
      "metadata": {
        "id": "yUvMJPihY4nK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric.\n",
        "\n",
        "    Only computes a batch-wise average of precision.\n",
        "\n",
        "    Computes the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric.\n",
        "\n",
        "    Only computes a batch-wise average of recall.\n",
        "\n",
        "    Computes the recall, a metric for multi-label classification of\n",
        "    how many relevant items are selected.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def f1_binary(y_true, y_pred):\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
        "\n",
        "def metric_wrapper(index):\n",
        "  def f1_for_class(y_true, y_pred):\n",
        "    #get only the desired class\n",
        "    true = y_true[:,:,index]\n",
        "    pred = y_pred[:,:,index]\n",
        "    #return dice per class\n",
        "    tmp = f1_binary(true,pred)\n",
        "    return tmp\n",
        "  f1_for_class.__name__ = 'f1_' + str(index)\n",
        "  return f1_for_class\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    result = 0.0\n",
        "    for class_id in no_punct_indexes:\n",
        "        y_true_single_class = y_true[:,:,class_id]\n",
        "        y_pred_single_class = y_pred[:,:,class_id]\n",
        "        f1_single = f1_binary(y_true_single_class, y_pred_single_class)\n",
        "        result += f1_single / float(n_classes)\n",
        "    return result"
      ],
      "metadata": {
        "id": "_4dwoTOuZCmi"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsVGByW_sdec",
        "outputId": "2b5afe0f-1363-4514-afd3-f1c6e70917f4"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(embedding_layer)\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(Dense(len(tag2index)))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy', f1, [metric_wrapper(i) for i in no_punct_indexes]])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 249, 100)          1094900   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 249, 512)         731136    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 249, 46)           23598     \n",
            "                                                                 \n",
            " activation (Activation)     (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,849,634\n",
            "Trainable params: 754,734\n",
            "Non-trainable params: 1,094,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "history = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=40, validation_data=(valid_sentences_X, cat_val_tags_y), callbacks=[callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n45419-PZSR7",
        "outputId": "861c2a5b-b02f-46b5-8fc3-ee7e7de7bb33"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "16/16 [==============================] - 92s 5s/step - loss: 0.7755 - accuracy: 0.8462 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.3447 - val_accuracy: 0.9178 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.3100 - accuracy: 0.9168 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2892 - val_accuracy: 0.9211 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.2793 - accuracy: 0.9260 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2702 - val_accuracy: 0.9308 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.2610 - accuracy: 0.9338 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2537 - val_accuracy: 0.9372 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.2437 - accuracy: 0.9409 - f1: 3.4899e-04 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0140 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2360 - val_accuracy: 0.9427 - val_f1: 0.0012 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0469 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.2244 - accuracy: 0.9450 - f1: 0.0050 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.1991 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0023 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2158 - val_accuracy: 0.9473 - val_f1: 0.0073 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.2706 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0210 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - 71s 5s/step - loss: 0.2030 - accuracy: 0.9491 - f1: 0.0161 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.4005 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0061 - f1_27: 0.0000e+00 - f1_28: 0.2375 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 5.9149e-04 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1945 - val_accuracy: 0.9522 - val_f1: 0.0205 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.3924 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0274 - val_f1_27: 0.0000e+00 - val_f1_28: 0.3999 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 4.7472e-04 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.1813 - accuracy: 0.9539 - f1: 0.0352 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.5082 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.3573 - f1_27: 0.0000e+00 - f1_28: 0.5236 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0197 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1735 - val_accuracy: 0.9561 - val_f1: 0.0475 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.5213 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0021 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0451 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.7102 - val_f1_27: 0.0000e+00 - val_f1_28: 0.5714 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0451 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0048 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - 71s 5s/step - loss: 0.1611 - accuracy: 0.9588 - f1: 0.0629 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.5882 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.1623 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0878 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.8428 - f1_27: 0.0000e+00 - f1_28: 0.6840 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.1419 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0082 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1556 - val_accuracy: 0.9606 - val_f1: 0.0851 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.6018 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.6756 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.2839 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.9008 - val_f1_27: 0.0000e+00 - val_f1_28: 0.6980 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.1936 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0483 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.1441 - accuracy: 0.9634 - f1: 0.0958 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.6358 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0097 - f1_12: 0.8150 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.2803 - f1_17: 0.0000e+00 - f1_18: 0.0043 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.9064 - f1_27: 0.0000e+00 - f1_28: 0.7640 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.3392 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0732 - f1_36: 0.0051 - f1_37: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1409 - val_accuracy: 0.9650 - val_f1: 0.1075 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.6461 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0129 - val_f1_12: 0.9798 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.4745 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0018 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.9144 - val_f1_27: 0.0000e+00 - val_f1_28: 0.7702 - val_f1_29: 0.0256 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.2741 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.1450 - val_f1_36: 0.0553 - val_f1_37: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.1301 - accuracy: 0.9675 - f1: 0.1298 - f1_1: 0.0179 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.6760 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0602 - f1_12: 0.9853 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.4313 - f1_17: 0.0000e+00 - f1_18: 0.0289 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.9118 - f1_27: 0.0000e+00 - f1_28: 0.8120 - f1_29: 0.3174 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.4606 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.2005 - f1_36: 0.2886 - f1_37: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1287 - val_accuracy: 0.9683 - val_f1: 0.1532 - val_f1_1: 0.1055 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.6628 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0831 - val_f1_12: 0.9957 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.5529 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0402 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.9115 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8062 - val_f1_29: 0.7056 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.3577 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.3259 - val_f1_36: 0.5790 - val_f1_37: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.1188 - accuracy: 0.9707 - f1: 0.1695 - f1_1: 0.1691 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.7054 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.1905 - f1_12: 0.9969 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.5037 - f1_17: 0.0000e+00 - f1_18: 0.1060 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.9102 - f1_27: 0.0000e+00 - f1_28: 0.8382 - f1_29: 0.8392 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.5260 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.3232 - f1_36: 0.6729 - f1_37: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1186 - val_accuracy: 0.9704 - val_f1: 0.1801 - val_f1_1: 0.2938 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.6861 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.1282 - val_f1_12: 1.0000 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.6006 - val_f1_17: 0.0000e+00 - val_f1_18: 0.1711 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.9132 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8292 - val_f1_29: 0.9494 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.4619 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.4481 - val_f1_36: 0.7227 - val_f1_37: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.1093 - accuracy: 0.9729 - f1: 0.1958 - f1_1: 0.3479 - f1_2: 0.0040 - f1_3: 0.0000e+00 - f1_4: 0.7253 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.2921 - f1_12: 0.9982 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0206 - f1_16: 0.5709 - f1_17: 0.0000e+00 - f1_18: 0.1958 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.1516 - f1_26: 0.9146 - f1_27: 0.0000e+00 - f1_28: 0.8547 - f1_29: 0.9576 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.5799 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.4332 - f1_36: 0.7874 - f1_37: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1105 - val_accuracy: 0.9720 - val_f1: 0.2069 - val_f1_1: 0.4647 - val_f1_2: 0.0475 - val_f1_3: 0.0000e+00 - val_f1_4: 0.6737 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.2145 - val_f1_12: 1.0000 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0232 - val_f1_16: 0.6530 - val_f1_17: 0.0000e+00 - val_f1_18: 0.3303 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.3217 - val_f1_26: 0.9136 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8423 - val_f1_29: 0.9745 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.4991 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.5323 - val_f1_36: 0.7647 - val_f1_37: 0.0000e+00 - val_f1_39: 0.0090 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.0130 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.1013 - accuracy: 0.9748 - f1: 0.2293 - f1_1: 0.5151 - f1_2: 0.0348 - f1_3: 0.0000e+00 - f1_4: 0.7468 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0026 - f1_10: 0.0000e+00 - f1_11: 0.3740 - f1_12: 0.9987 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.2078 - f1_16: 0.6291 - f1_17: 0.0000e+00 - f1_18: 0.2824 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.6234 - f1_26: 0.9163 - f1_27: 0.0000e+00 - f1_28: 0.8685 - f1_29: 0.9888 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.6197 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.5141 - f1_36: 0.8211 - f1_37: 0.0000e+00 - f1_39: 0.0258 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.0015 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1028 - val_accuracy: 0.9736 - val_f1: 0.2421 - val_f1_1: 0.5858 - val_f1_2: 0.2140 - val_f1_3: 0.0000e+00 - val_f1_4: 0.6894 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.3947 - val_f1_12: 1.0000 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.3162 - val_f1_16: 0.6830 - val_f1_17: 0.0000e+00 - val_f1_18: 0.3422 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.7545 - val_f1_26: 0.9173 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8581 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.5726 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.5403 - val_f1_36: 0.7718 - val_f1_37: 0.0000e+00 - val_f1_39: 0.0275 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.0205 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.0946 - accuracy: 0.9764 - f1: 0.2574 - f1_1: 0.6238 - f1_2: 0.1778 - f1_3: 0.0000e+00 - f1_4: 0.7596 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0049 - f1_10: 0.0000e+00 - f1_11: 0.4750 - f1_12: 0.9986 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.4137 - f1_16: 0.6604 - f1_17: 0.0000e+00 - f1_18: 0.3514 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.8439 - f1_26: 0.9184 - f1_27: 0.0000e+00 - f1_28: 0.8753 - f1_29: 0.9944 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.6527 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.5553 - f1_36: 0.8273 - f1_37: 0.0000e+00 - f1_39: 0.1335 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.0296 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.0969 - val_accuracy: 0.9749 - val_f1: 0.2649 - val_f1_1: 0.6079 - val_f1_2: 0.2217 - val_f1_3: 0.0000e+00 - val_f1_4: 0.7257 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0017 - val_f1_10: 0.0000e+00 - val_f1_11: 0.4538 - val_f1_12: 1.0000 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.5140 - val_f1_16: 0.7098 - val_f1_17: 0.0000e+00 - val_f1_18: 0.4248 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9224 - val_f1_26: 0.9229 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8662 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.6048 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.5976 - val_f1_36: 0.7737 - val_f1_37: 0.0000e+00 - val_f1_39: 0.2016 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.0501 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.0887 - accuracy: 0.9776 - f1: 0.2837 - f1_1: 0.6665 - f1_2: 0.4349 - f1_3: 0.0000e+00 - f1_4: 0.7760 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0554 - f1_9: 0.0109 - f1_10: 0.0000e+00 - f1_11: 0.5469 - f1_12: 0.9985 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.5496 - f1_16: 0.6943 - f1_17: 0.0000e+00 - f1_18: 0.4102 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.9078 - f1_26: 0.9240 - f1_27: 0.0000e+00 - f1_28: 0.8829 - f1_29: 0.9924 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.6758 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.6214 - f1_36: 0.8274 - f1_37: 0.0000e+00 - f1_39: 0.2926 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.0788 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.0913 - val_accuracy: 0.9763 - val_f1: 0.2875 - val_f1_1: 0.6544 - val_f1_2: 0.5377 - val_f1_3: 0.0000e+00 - val_f1_4: 0.7140 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.1648 - val_f1_9: 0.0122 - val_f1_10: 0.0000e+00 - val_f1_11: 0.4983 - val_f1_12: 1.0000 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6067 - val_f1_16: 0.7405 - val_f1_17: 0.0000e+00 - val_f1_18: 0.4065 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9303 - val_f1_26: 0.9268 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8693 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.6694 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.5982 - val_f1_36: 0.7745 - val_f1_37: 0.0000e+00 - val_f1_39: 0.3273 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.0715 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.0837 - accuracy: 0.9788 - f1: 0.3045 - f1_1: 0.7137 - f1_2: 0.6050 - f1_3: 0.0000e+00 - f1_4: 0.7838 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.2749 - f1_9: 0.0264 - f1_10: 0.0000e+00 - f1_11: 0.5910 - f1_12: 0.9987 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.6084 - f1_16: 0.7212 - f1_17: 0.0000e+00 - f1_18: 0.4473 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.9363 - f1_26: 0.9302 - f1_27: 0.0000e+00 - f1_28: 0.8882 - f1_29: 0.9907 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.6938 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.6384 - f1_36: 0.8195 - f1_37: 0.0000e+00 - f1_39: 0.3735 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.1397 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.0865 - val_accuracy: 0.9772 - val_f1: 0.3073 - val_f1_1: 0.7034 - val_f1_2: 0.6573 - val_f1_3: 0.0419 - val_f1_4: 0.7302 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.2375 - val_f1_9: 0.0256 - val_f1_10: 0.0000e+00 - val_f1_11: 0.5567 - val_f1_12: 1.0000 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6548 - val_f1_16: 0.7649 - val_f1_17: 0.0000e+00 - val_f1_18: 0.4405 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9398 - val_f1_26: 0.9327 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8836 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.6832 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.6048 - val_f1_36: 0.7754 - val_f1_37: 0.0000e+00 - val_f1_39: 0.4936 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.1685 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.0793 - accuracy: 0.9799 - f1: 0.3240 - f1_1: 0.7402 - f1_2: 0.7062 - f1_3: 0.1367 - f1_4: 0.7895 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.3849 - f1_9: 0.0570 - f1_10: 0.0000e+00 - f1_11: 0.6388 - f1_12: 0.9986 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.6715 - f1_16: 0.7446 - f1_17: 0.0000e+00 - f1_18: 0.4861 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.9352 - f1_26: 0.9356 - f1_27: 0.0000e+00 - f1_28: 0.8905 - f1_29: 0.9942 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.7071 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.6703 - f1_36: 0.8319 - f1_37: 0.0000e+00 - f1_39: 0.4259 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.2089 - f1_44: 0.0000e+00 - f1_45: 0.0066 - val_loss: 0.0829 - val_accuracy: 0.9783 - val_f1: 0.3279 - val_f1_1: 0.7360 - val_f1_2: 0.7181 - val_f1_3: 0.2744 - val_f1_4: 0.7601 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.4485 - val_f1_9: 0.0600 - val_f1_10: 0.0000e+00 - val_f1_11: 0.5968 - val_f1_12: 1.0000 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7220 - val_f1_16: 0.7717 - val_f1_17: 0.0000e+00 - val_f1_18: 0.4636 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9380 - val_f1_26: 0.9374 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8808 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.6538 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.6669 - val_f1_36: 0.7759 - val_f1_37: 0.0000e+00 - val_f1_39: 0.5020 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.2042 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0082\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.0754 - accuracy: 0.9809 - f1: 0.3438 - f1_1: 0.7724 - f1_2: 0.7398 - f1_3: 0.3527 - f1_4: 0.7919 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.5519 - f1_9: 0.0929 - f1_10: 0.0000e+00 - f1_11: 0.6927 - f1_12: 0.9985 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.7239 - f1_16: 0.7636 - f1_17: 0.0000e+00 - f1_18: 0.5178 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.9378 - f1_26: 0.9418 - f1_27: 0.0000e+00 - f1_28: 0.8972 - f1_29: 0.9918 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.7135 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.6901 - f1_36: 0.8279 - f1_37: 0.0000e+00 - f1_39: 0.4676 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.2673 - f1_44: 0.0000e+00 - f1_45: 0.0191 - val_loss: 0.0794 - val_accuracy: 0.9791 - val_f1: 0.3408 - val_f1_1: 0.7549 - val_f1_2: 0.7566 - val_f1_3: 0.3618 - val_f1_4: 0.7580 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.5012 - val_f1_9: 0.1016 - val_f1_10: 0.0000e+00 - val_f1_11: 0.6458 - val_f1_12: 1.0000 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7096 - val_f1_16: 0.7734 - val_f1_17: 0.0000e+00 - val_f1_18: 0.5568 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9410 - val_f1_26: 0.9411 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8886 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.6632 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.6844 - val_f1_36: 0.7800 - val_f1_37: 0.0000e+00 - val_f1_39: 0.5450 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.2539 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0162\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.0717 - accuracy: 0.9818 - f1: 0.3585 - f1_1: 0.7952 - f1_2: 0.8120 - f1_3: 0.4505 - f1_4: 0.8023 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.6063 - f1_9: 0.1519 - f1_10: 0.0000e+00 - f1_11: 0.7072 - f1_12: 0.9987 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.7739 - f1_16: 0.7682 - f1_17: 0.0000e+00 - f1_18: 0.5435 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.9438 - f1_26: 0.9430 - f1_27: 0.0000e+00 - f1_28: 0.9000 - f1_29: 0.9867 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.7338 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.7148 - f1_36: 0.8274 - f1_37: 0.0000e+00 - f1_39: 0.4822 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.3587 - f1_44: 0.0000e+00 - f1_45: 0.0398 - val_loss: 0.0764 - val_accuracy: 0.9797 - val_f1: 0.3566 - val_f1_1: 0.7521 - val_f1_2: 0.8289 - val_f1_3: 0.6295 - val_f1_4: 0.7758 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.5538 - val_f1_9: 0.1506 - val_f1_10: 0.0000e+00 - val_f1_11: 0.6674 - val_f1_12: 1.0000 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7542 - val_f1_16: 0.7940 - val_f1_17: 0.0000e+00 - val_f1_18: 0.4744 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9441 - val_f1_26: 0.9462 - val_f1_27: 0.0000e+00 - val_f1_28: 0.9018 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.6793 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.6907 - val_f1_36: 0.7921 - val_f1_37: 0.0000e+00 - val_f1_39: 0.5799 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.3219 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0307\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0685 - accuracy: 0.9825 - f1: 0.3761 - f1_1: 0.7951 - f1_2: 0.8490 - f1_3: 0.7004 - f1_4: 0.8119 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.6928 - f1_9: 0.1848 - f1_10: 0.0000e+00 - f1_11: 0.7311 - f1_12: 0.9986 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.7986 - f1_16: 0.7940 - f1_17: 0.0000e+00 - f1_18: 0.5523 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.9374 - f1_26: 0.9453 - f1_27: 0.0000e+00 - f1_28: 0.9060 - f1_29: 0.9923 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.7514 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.7323 - f1_36: 0.8349 - f1_37: 0.0000e+00 - f1_39: 0.5453 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.4227 - f1_44: 0.0000e+00 - f1_45: 0.0695 - val_loss: 0.0729 - val_accuracy: 0.9808 - val_f1: 0.3684 - val_f1_1: 0.7771 - val_f1_2: 0.8261 - val_f1_3: 0.6910 - val_f1_4: 0.7765 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.5737 - val_f1_9: 0.2179 - val_f1_10: 0.0000e+00 - val_f1_11: 0.6889 - val_f1_12: 1.0000 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.8146 - val_f1_16: 0.7939 - val_f1_17: 0.0000e+00 - val_f1_18: 0.5320 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9484 - val_f1_26: 0.9491 - val_f1_27: 0.0000e+00 - val_f1_28: 0.9029 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.7235 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.6895 - val_f1_36: 0.8116 - val_f1_37: 0.0000e+00 - val_f1_39: 0.5932 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.3871 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0409\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.0655 - accuracy: 0.9833 - f1: 0.3886 - f1_1: 0.8103 - f1_2: 0.8830 - f1_3: 0.7599 - f1_4: 0.8147 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.7413 - f1_9: 0.2641 - f1_10: 0.0000e+00 - f1_11: 0.7587 - f1_12: 0.9986 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8238 - f1_16: 0.7972 - f1_17: 0.0000e+00 - f1_18: 0.5769 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.9366 - f1_26: 0.9477 - f1_27: 0.0000e+00 - f1_28: 0.9064 - f1_29: 0.9936 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.7588 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.7479 - f1_36: 0.8479 - f1_37: 0.0000e+00 - f1_39: 0.5625 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.4783 - f1_44: 0.0000e+00 - f1_45: 0.1352 - val_loss: 0.0711 - val_accuracy: 0.9810 - val_f1: 0.3774 - val_f1_1: 0.7964 - val_f1_2: 0.8697 - val_f1_3: 0.8021 - val_f1_4: 0.7837 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.6335 - val_f1_9: 0.2445 - val_f1_10: 0.0000e+00 - val_f1_11: 0.7195 - val_f1_12: 1.0000 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.8191 - val_f1_16: 0.7858 - val_f1_17: 0.0000e+00 - val_f1_18: 0.5516 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9365 - val_f1_26: 0.9484 - val_f1_27: 0.0000e+00 - val_f1_28: 0.9028 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.7141 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.7095 - val_f1_36: 0.8198 - val_f1_37: 0.0000e+00 - val_f1_39: 0.5871 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.4431 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0326\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.0632 - accuracy: 0.9838 - f1: 0.3977 - f1_1: 0.8214 - f1_2: 0.8947 - f1_3: 0.8496 - f1_4: 0.8139 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.7736 - f1_9: 0.2958 - f1_10: 0.0000e+00 - f1_11: 0.7779 - f1_12: 0.9986 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8395 - f1_16: 0.7998 - f1_17: 0.0000e+00 - f1_18: 0.5945 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.9324 - f1_26: 0.9491 - f1_27: 0.0000e+00 - f1_28: 0.9084 - f1_29: 0.9897 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.7609 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.7600 - f1_36: 0.8715 - f1_37: 0.0000e+00 - f1_39: 0.5600 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.5338 - f1_44: 0.0000e+00 - f1_45: 0.1830 - val_loss: 0.0680 - val_accuracy: 0.9818 - val_f1: 0.3883 - val_f1_1: 0.8020 - val_f1_2: 0.9032 - val_f1_3: 0.8206 - val_f1_4: 0.7835 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.7312 - val_f1_9: 0.2723 - val_f1_10: 0.0000e+00 - val_f1_11: 0.7334 - val_f1_12: 1.0000 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.8394 - val_f1_16: 0.8185 - val_f1_17: 0.0000e+00 - val_f1_18: 0.5511 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9410 - val_f1_26: 0.9529 - val_f1_27: 0.0000e+00 - val_f1_28: 0.9095 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.7438 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.7074 - val_f1_36: 0.8359 - val_f1_37: 0.0000e+00 - val_f1_39: 0.5909 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.4714 - val_f1_44: 0.0000e+00 - val_f1_45: 0.1242\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.0606 - accuracy: 0.9844 - f1: 0.4072 - f1_1: 0.8346 - f1_2: 0.9170 - f1_3: 0.8630 - f1_4: 0.8258 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8341 - f1_9: 0.3211 - f1_10: 0.0000e+00 - f1_11: 0.7848 - f1_12: 0.9987 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8521 - f1_16: 0.8262 - f1_17: 0.0000e+00 - f1_18: 0.6184 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.9326 - f1_26: 0.9496 - f1_27: 0.0000e+00 - f1_28: 0.9153 - f1_29: 0.9948 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.7726 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.7711 - f1_36: 0.8821 - f1_37: 0.0000e+00 - f1_39: 0.5984 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.5495 - f1_44: 0.0000e+00 - f1_45: 0.2459 - val_loss: 0.0670 - val_accuracy: 0.9823 - val_f1: 0.3956 - val_f1_1: 0.7786 - val_f1_2: 0.8894 - val_f1_3: 0.8512 - val_f1_4: 0.7909 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.7282 - val_f1_9: 0.4067 - val_f1_10: 0.0000e+00 - val_f1_11: 0.7260 - val_f1_12: 1.0000 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.8640 - val_f1_16: 0.8285 - val_f1_17: 0.0000e+00 - val_f1_18: 0.5679 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9484 - val_f1_26: 0.9529 - val_f1_27: 0.0000e+00 - val_f1_28: 0.9085 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.6975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.7283 - val_f1_36: 0.8392 - val_f1_37: 0.0000e+00 - val_f1_39: 0.6263 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.5673 - val_f1_44: 0.0000e+00 - val_f1_45: 0.1255\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.0589 - accuracy: 0.9848 - f1: 0.4140 - f1_1: 0.8318 - f1_2: 0.9191 - f1_3: 0.8913 - f1_4: 0.8253 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8315 - f1_9: 0.3979 - f1_10: 0.0000e+00 - f1_11: 0.7918 - f1_12: 0.9987 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8634 - f1_16: 0.8374 - f1_17: 0.0000e+00 - f1_18: 0.6205 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.9337 - f1_26: 0.9509 - f1_27: 0.0000e+00 - f1_28: 0.9158 - f1_29: 0.9930 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.7720 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.7800 - f1_36: 0.8957 - f1_37: 0.0000e+00 - f1_39: 0.6222 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.5856 - f1_44: 0.0000e+00 - f1_45: 0.3033 - val_loss: 0.0639 - val_accuracy: 0.9826 - val_f1: 0.4006 - val_f1_1: 0.8067 - val_f1_2: 0.9177 - val_f1_3: 0.8606 - val_f1_4: 0.7655 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.7836 - val_f1_9: 0.3134 - val_f1_10: 0.0000e+00 - val_f1_11: 0.7321 - val_f1_12: 1.0000 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.8669 - val_f1_16: 0.8396 - val_f1_17: 0.0000e+00 - val_f1_18: 0.6192 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9484 - val_f1_26: 0.9550 - val_f1_27: 0.0000e+00 - val_f1_28: 0.9184 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.7752 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.7055 - val_f1_36: 0.8537 - val_f1_37: 0.0000e+00 - val_f1_39: 0.6177 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.5737 - val_f1_44: 0.0000e+00 - val_f1_45: 0.1739\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.0567 - accuracy: 0.9852 - f1: 0.4210 - f1_1: 0.8477 - f1_2: 0.9307 - f1_3: 0.9172 - f1_4: 0.8264 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8848 - f1_9: 0.3878 - f1_10: 0.0000e+00 - f1_11: 0.8041 - f1_12: 0.9987 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8740 - f1_16: 0.8354 - f1_17: 0.0148 - f1_18: 0.6403 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.9407 - f1_26: 0.9535 - f1_27: 0.0000e+00 - f1_28: 0.9208 - f1_29: 0.9947 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.7856 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.7917 - f1_36: 0.9136 - f1_37: 0.0000e+00 - f1_39: 0.6463 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.5867 - f1_44: 0.0000e+00 - f1_45: 0.3444 - val_loss: 0.0622 - val_accuracy: 0.9831 - val_f1: 0.4087 - val_f1_1: 0.8283 - val_f1_2: 0.9187 - val_f1_3: 0.9181 - val_f1_4: 0.7937 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.8085 - val_f1_9: 0.3918 - val_f1_10: 0.0000e+00 - val_f1_11: 0.7750 - val_f1_12: 1.0000 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.8692 - val_f1_16: 0.8449 - val_f1_17: 0.0101 - val_f1_18: 0.5632 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9484 - val_f1_26: 0.9550 - val_f1_27: 0.0000e+00 - val_f1_28: 0.9187 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.7727 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.7326 - val_f1_36: 0.8666 - val_f1_37: 0.0000e+00 - val_f1_39: 0.6316 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.5373 - val_f1_44: 0.0000e+00 - val_f1_45: 0.2670\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.0548 - accuracy: 0.9857 - f1: 0.4319 - f1_1: 0.8481 - f1_2: 0.9350 - f1_3: 0.9327 - f1_4: 0.8364 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.9007 - f1_9: 0.4662 - f1_10: 0.0000e+00 - f1_11: 0.8098 - f1_12: 0.9986 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8832 - f1_16: 0.8464 - f1_17: 0.1015 - f1_18: 0.6549 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.9478 - f1_26: 0.9542 - f1_27: 0.0000e+00 - f1_28: 0.9216 - f1_29: 0.9942 - f1_30: 0.0000e+00 - f1_31: 0.0513 - f1_32: 0.7911 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.7917 - f1_36: 0.9253 - f1_37: 0.0000e+00 - f1_39: 0.6727 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.6197 - f1_44: 0.0000e+00 - f1_45: 0.3928 - val_loss: 0.0607 - val_accuracy: 0.9837 - val_f1: 0.4165 - val_f1_1: 0.8228 - val_f1_2: 0.9192 - val_f1_3: 0.9353 - val_f1_4: 0.7868 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.8153 - val_f1_9: 0.3924 - val_f1_10: 0.0000e+00 - val_f1_11: 0.7711 - val_f1_12: 1.0000 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.8692 - val_f1_16: 0.8627 - val_f1_17: 0.1701 - val_f1_18: 0.6267 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9550 - val_f1_26: 0.9560 - val_f1_27: 0.0000e+00 - val_f1_28: 0.9212 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.7580 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.7774 - val_f1_36: 0.8720 - val_f1_37: 0.0000e+00 - val_f1_39: 0.6667 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.5471 - val_f1_44: 0.0000e+00 - val_f1_45: 0.2387\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.0532 - accuracy: 0.9860 - f1: 0.4390 - f1_1: 0.8595 - f1_2: 0.9408 - f1_3: 0.9450 - f1_4: 0.8356 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8986 - f1_9: 0.4763 - f1_10: 0.0000e+00 - f1_11: 0.8252 - f1_12: 0.9985 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8827 - f1_16: 0.8649 - f1_17: 0.1992 - f1_18: 0.6739 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.9463 - f1_26: 0.9568 - f1_27: 0.0000e+00 - f1_28: 0.9268 - f1_29: 0.9933 - f1_30: 0.0000e+00 - f1_31: 0.0686 - f1_32: 0.7985 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.7935 - f1_36: 0.9251 - f1_37: 0.0000e+00 - f1_39: 0.6931 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.6250 - f1_44: 0.0000e+00 - f1_45: 0.4308 - val_loss: 0.0589 - val_accuracy: 0.9841 - val_f1: 0.4266 - val_f1_1: 0.8314 - val_f1_2: 0.9163 - val_f1_3: 0.9335 - val_f1_4: 0.7869 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.8559 - val_f1_9: 0.4606 - val_f1_10: 0.0000e+00 - val_f1_11: 0.7929 - val_f1_12: 0.9995 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.8815 - val_f1_16: 0.8593 - val_f1_17: 0.2686 - val_f1_18: 0.6416 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9595 - val_f1_26: 0.9600 - val_f1_27: 0.0000e+00 - val_f1_28: 0.9285 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.7700 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.7542 - val_f1_36: 0.8845 - val_f1_37: 0.0000e+00 - val_f1_39: 0.6700 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.6079 - val_f1_44: 0.0000e+00 - val_f1_45: 0.3058\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.0516 - accuracy: 0.9864 - f1: 0.4448 - f1_1: 0.8641 - f1_2: 0.9402 - f1_3: 0.9455 - f1_4: 0.8408 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.9255 - f1_9: 0.5085 - f1_10: 0.0000e+00 - f1_11: 0.8279 - f1_12: 0.9986 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8863 - f1_16: 0.8568 - f1_17: 0.2873 - f1_18: 0.6758 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.9541 - f1_26: 0.9578 - f1_27: 0.0000e+00 - f1_28: 0.9271 - f1_29: 0.9756 - f1_30: 0.0000e+00 - f1_31: 0.1025 - f1_32: 0.8007 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.8047 - f1_36: 0.9333 - f1_37: 0.0000e+00 - f1_39: 0.6970 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.6339 - f1_44: 0.0000e+00 - f1_45: 0.4500 - val_loss: 0.0577 - val_accuracy: 0.9843 - val_f1: 0.4353 - val_f1_1: 0.8315 - val_f1_2: 0.9187 - val_f1_3: 0.9463 - val_f1_4: 0.8017 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.8596 - val_f1_9: 0.5530 - val_f1_10: 0.0000e+00 - val_f1_11: 0.7904 - val_f1_12: 0.9989 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.8838 - val_f1_16: 0.8792 - val_f1_17: 0.3845 - val_f1_18: 0.6202 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9708 - val_f1_26: 0.9612 - val_f1_27: 0.0000e+00 - val_f1_28: 0.9300 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0303 - val_f1_32: 0.7622 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.7564 - val_f1_36: 0.9217 - val_f1_37: 0.0000e+00 - val_f1_39: 0.6773 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.6254 - val_f1_44: 0.0000e+00 - val_f1_45: 0.3113\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.0501 - accuracy: 0.9868 - f1: 0.4566 - f1_1: 0.8713 - f1_2: 0.9391 - f1_3: 0.9588 - f1_4: 0.8470 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.9372 - f1_9: 0.5284 - f1_10: 0.0000e+00 - f1_11: 0.8364 - f1_12: 0.9986 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8986 - f1_16: 0.8754 - f1_17: 0.3754 - f1_18: 0.6804 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.9590 - f1_26: 0.9600 - f1_27: 0.0000e+00 - f1_28: 0.9309 - f1_29: 0.9930 - f1_30: 0.0000e+00 - f1_31: 0.2628 - f1_32: 0.8026 - f1_33: 0.0104 - f1_34: 0.0000e+00 - f1_35: 0.8141 - f1_36: 0.9479 - f1_37: 0.0000e+00 - f1_39: 0.7333 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.6584 - f1_44: 0.0000e+00 - f1_45: 0.4443 - val_loss: 0.0565 - val_accuracy: 0.9843 - val_f1: 0.4433 - val_f1_1: 0.8313 - val_f1_2: 0.9217 - val_f1_3: 0.9569 - val_f1_4: 0.7847 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.8739 - val_f1_9: 0.5554 - val_f1_10: 0.0000e+00 - val_f1_11: 0.7985 - val_f1_12: 0.9995 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.8767 - val_f1_16: 0.8628 - val_f1_17: 0.4337 - val_f1_18: 0.6610 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9674 - val_f1_26: 0.9583 - val_f1_27: 0.0000e+00 - val_f1_28: 0.9262 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.2429 - val_f1_32: 0.7957 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.7557 - val_f1_36: 0.9372 - val_f1_37: 0.0000e+00 - val_f1_39: 0.6562 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.6153 - val_f1_44: 0.0000e+00 - val_f1_45: 0.3243\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.0488 - accuracy: 0.9871 - f1: 0.4661 - f1_1: 0.8766 - f1_2: 0.9448 - f1_3: 0.9547 - f1_4: 0.8495 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.9323 - f1_9: 0.5659 - f1_10: 0.0000e+00 - f1_11: 0.8425 - f1_12: 0.9985 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8943 - f1_16: 0.8698 - f1_17: 0.4946 - f1_18: 0.6994 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.9593 - f1_26: 0.9600 - f1_27: 0.0000e+00 - f1_28: 0.9308 - f1_29: 0.9931 - f1_30: 0.0000e+00 - f1_31: 0.4301 - f1_32: 0.8073 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.8180 - f1_36: 0.9561 - f1_37: 0.0000e+00 - f1_39: 0.7321 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.6594 - f1_44: 0.0000e+00 - f1_45: 0.4737 - val_loss: 0.0554 - val_accuracy: 0.9848 - val_f1: 0.4513 - val_f1_1: 0.8365 - val_f1_2: 0.9186 - val_f1_3: 0.9515 - val_f1_4: 0.7942 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9036 - val_f1_9: 0.5518 - val_f1_10: 0.0000e+00 - val_f1_11: 0.8071 - val_f1_12: 0.9995 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.8970 - val_f1_16: 0.8719 - val_f1_17: 0.4402 - val_f1_18: 0.6159 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9817 - val_f1_26: 0.9610 - val_f1_27: 0.0000e+00 - val_f1_28: 0.9308 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.2529 - val_f1_32: 0.7791 - val_f1_33: 0.0455 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8063 - val_f1_36: 0.9227 - val_f1_37: 0.0000e+00 - val_f1_39: 0.6941 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.6061 - val_f1_44: 0.0000e+00 - val_f1_45: 0.4883\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.0475 - accuracy: 0.9873 - f1: 0.4756 - f1_1: 0.8799 - f1_2: 0.9454 - f1_3: 0.9570 - f1_4: 0.8515 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.9512 - f1_9: 0.5839 - f1_10: 0.0000e+00 - f1_11: 0.8478 - f1_12: 0.9987 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.8997 - f1_16: 0.8809 - f1_17: 0.5570 - f1_18: 0.6973 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.9727 - f1_26: 0.9628 - f1_27: 0.0000e+00 - f1_28: 0.9371 - f1_29: 0.9944 - f1_30: 0.0000e+00 - f1_31: 0.5433 - f1_32: 0.8151 - f1_33: 0.0608 - f1_34: 0.0000e+00 - f1_35: 0.8215 - f1_36: 0.9580 - f1_37: 0.0000e+00 - f1_39: 0.7395 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.6655 - f1_44: 0.0000e+00 - f1_45: 0.5032 - val_loss: 0.0543 - val_accuracy: 0.9850 - val_f1: 0.4599 - val_f1_1: 0.8483 - val_f1_2: 0.9181 - val_f1_3: 0.9574 - val_f1_4: 0.7973 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9095 - val_f1_9: 0.6078 - val_f1_10: 0.0000e+00 - val_f1_11: 0.7977 - val_f1_12: 0.9995 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.9006 - val_f1_16: 0.8846 - val_f1_17: 0.4842 - val_f1_18: 0.6715 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9817 - val_f1_26: 0.9615 - val_f1_27: 0.0000e+00 - val_f1_28: 0.9325 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.3691 - val_f1_32: 0.7707 - val_f1_33: 0.1017 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8049 - val_f1_36: 0.9405 - val_f1_37: 0.0000e+00 - val_f1_39: 0.7114 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.5939 - val_f1_44: 0.0000e+00 - val_f1_45: 0.4546\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.0463 - accuracy: 0.9876 - f1: 0.4821 - f1_1: 0.8860 - f1_2: 0.9489 - f1_3: 0.9591 - f1_4: 0.8535 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.9459 - f1_9: 0.5995 - f1_10: 0.0000e+00 - f1_11: 0.8541 - f1_12: 0.9986 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.9066 - f1_16: 0.8825 - f1_17: 0.6066 - f1_18: 0.7114 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.9743 - f1_26: 0.9639 - f1_27: 0.0000e+00 - f1_28: 0.9382 - f1_29: 0.9917 - f1_30: 0.0000e+00 - f1_31: 0.6086 - f1_32: 0.8169 - f1_33: 0.1045 - f1_34: 0.0000e+00 - f1_35: 0.8295 - f1_36: 0.9598 - f1_37: 0.0000e+00 - f1_39: 0.7462 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.6599 - f1_44: 0.0000e+00 - f1_45: 0.5369 - val_loss: 0.0532 - val_accuracy: 0.9853 - val_f1: 0.4684 - val_f1_1: 0.8507 - val_f1_2: 0.9389 - val_f1_3: 0.9510 - val_f1_4: 0.8031 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9166 - val_f1_9: 0.6255 - val_f1_10: 0.0000e+00 - val_f1_11: 0.8139 - val_f1_12: 0.9995 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.8954 - val_f1_16: 0.8882 - val_f1_17: 0.6387 - val_f1_18: 0.6433 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9911 - val_f1_26: 0.9648 - val_f1_27: 0.0000e+00 - val_f1_28: 0.9323 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.4249 - val_f1_32: 0.7778 - val_f1_33: 0.1863 - val_f1_34: 0.0000e+00 - val_f1_35: 0.7940 - val_f1_36: 0.9480 - val_f1_37: 0.0000e+00 - val_f1_39: 0.6711 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.6450 - val_f1_44: 0.0000e+00 - val_f1_45: 0.4400\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.0451 - accuracy: 0.9878 - f1: 0.4919 - f1_1: 0.8878 - f1_2: 0.9502 - f1_3: 0.9601 - f1_4: 0.8580 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.9474 - f1_9: 0.6267 - f1_10: 0.0000e+00 - f1_11: 0.8551 - f1_12: 0.9987 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.9117 - f1_16: 0.8914 - f1_17: 0.6655 - f1_18: 0.7099 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.9846 - f1_26: 0.9649 - f1_27: 0.0000e+00 - f1_28: 0.9412 - f1_29: 0.9937 - f1_30: 0.0000e+00 - f1_31: 0.7202 - f1_32: 0.8189 - f1_33: 0.2416 - f1_34: 0.0000e+00 - f1_35: 0.8353 - f1_36: 0.9675 - f1_37: 0.0000e+00 - f1_39: 0.7446 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.6744 - f1_44: 0.0000e+00 - f1_45: 0.5258 - val_loss: 0.0522 - val_accuracy: 0.9855 - val_f1: 0.4748 - val_f1_1: 0.8456 - val_f1_2: 0.9252 - val_f1_3: 0.9636 - val_f1_4: 0.8044 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9159 - val_f1_9: 0.5857 - val_f1_10: 0.0000e+00 - val_f1_11: 0.8094 - val_f1_12: 0.9995 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.8953 - val_f1_16: 0.8895 - val_f1_17: 0.6788 - val_f1_18: 0.6852 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9911 - val_f1_26: 0.9660 - val_f1_27: 0.0000e+00 - val_f1_28: 0.9353 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.5630 - val_f1_32: 0.7845 - val_f1_33: 0.1714 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8017 - val_f1_36: 0.9419 - val_f1_37: 0.0000e+00 - val_f1_39: 0.7224 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.6585 - val_f1_44: 0.0000e+00 - val_f1_45: 0.4618\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.0442 - accuracy: 0.9881 - f1: 0.4960 - f1_1: 0.8921 - f1_2: 0.9520 - f1_3: 0.9611 - f1_4: 0.8576 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.9522 - f1_9: 0.6351 - f1_10: 0.0000e+00 - f1_11: 0.8593 - f1_12: 0.9984 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.9134 - f1_16: 0.8923 - f1_17: 0.7288 - f1_18: 0.7191 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.9853 - f1_26: 0.9657 - f1_27: 0.0000e+00 - f1_28: 0.9426 - f1_29: 0.9942 - f1_30: 0.0000e+00 - f1_31: 0.6727 - f1_32: 0.8266 - f1_33: 0.3031 - f1_34: 0.0000e+00 - f1_35: 0.8371 - f1_36: 0.9687 - f1_37: 0.0000e+00 - f1_39: 0.7641 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.6821 - f1_44: 0.0000e+00 - f1_45: 0.5350 - val_loss: 0.0518 - val_accuracy: 0.9856 - val_f1: 0.4778 - val_f1_1: 0.8502 - val_f1_2: 0.9391 - val_f1_3: 0.9636 - val_f1_4: 0.7756 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9072 - val_f1_9: 0.6510 - val_f1_10: 0.0000e+00 - val_f1_11: 0.8143 - val_f1_12: 0.9995 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.9046 - val_f1_16: 0.8960 - val_f1_17: 0.6701 - val_f1_18: 0.6991 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9911 - val_f1_26: 0.9667 - val_f1_27: 0.0000e+00 - val_f1_28: 0.9330 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.5295 - val_f1_32: 0.7959 - val_f1_33: 0.2149 - val_f1_34: 0.0000e+00 - val_f1_35: 0.7847 - val_f1_36: 0.9556 - val_f1_37: 0.0000e+00 - val_f1_39: 0.7307 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.6365 - val_f1_44: 0.0000e+00 - val_f1_45: 0.5060\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.0434 - accuracy: 0.9882 - f1: 0.4998 - f1_1: 0.8901 - f1_2: 0.9463 - f1_3: 0.9619 - f1_4: 0.8571 - f1_5: 0.0000e+00 - f1_6: 0.0208 - f1_7: 0.9554 - f1_9: 0.6455 - f1_10: 0.0000e+00 - f1_11: 0.8620 - f1_12: 0.9986 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.9137 - f1_16: 0.8915 - f1_17: 0.7553 - f1_18: 0.7215 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.9915 - f1_26: 0.9680 - f1_27: 0.0000e+00 - f1_28: 0.9429 - f1_29: 0.9928 - f1_30: 0.0000e+00 - f1_31: 0.6616 - f1_32: 0.8259 - f1_33: 0.3290 - f1_34: 0.0000e+00 - f1_35: 0.8376 - f1_36: 0.9725 - f1_37: 0.0000e+00 - f1_39: 0.7802 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.6925 - f1_44: 0.0000e+00 - f1_45: 0.5767 - val_loss: 0.0504 - val_accuracy: 0.9859 - val_f1: 0.4827 - val_f1_1: 0.8550 - val_f1_2: 0.9370 - val_f1_3: 0.9603 - val_f1_4: 0.7986 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.9277 - val_f1_9: 0.6410 - val_f1_10: 0.0000e+00 - val_f1_11: 0.8065 - val_f1_12: 0.9995 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.9075 - val_f1_16: 0.8981 - val_f1_17: 0.7624 - val_f1_18: 0.6706 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9921 - val_f1_26: 0.9692 - val_f1_27: 0.0000e+00 - val_f1_28: 0.9327 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.5565 - val_f1_32: 0.7970 - val_f1_33: 0.2489 - val_f1_34: 0.0000e+00 - val_f1_35: 0.7938 - val_f1_36: 0.9590 - val_f1_37: 0.0000e+00 - val_f1_39: 0.7331 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.6705 - val_f1_44: 0.0000e+00 - val_f1_45: 0.4932\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.0424 - accuracy: 0.9885 - f1: 0.5081 - f1_1: 0.9008 - f1_2: 0.9485 - f1_3: 0.9677 - f1_4: 0.8593 - f1_5: 0.0000e+00 - f1_6: 0.0429 - f1_7: 0.9527 - f1_9: 0.6715 - f1_10: 0.0000e+00 - f1_11: 0.8611 - f1_12: 0.9988 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.9199 - f1_16: 0.8970 - f1_17: 0.8189 - f1_18: 0.7306 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.9900 - f1_26: 0.9678 - f1_27: 0.0000e+00 - f1_28: 0.9438 - f1_29: 0.9938 - f1_30: 0.0000e+00 - f1_31: 0.7564 - f1_32: 0.8282 - f1_33: 0.3955 - f1_34: 0.0000e+00 - f1_35: 0.8342 - f1_36: 0.9723 - f1_37: 0.0000e+00 - f1_39: 0.7820 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.7033 - f1_44: 0.0000e+00 - f1_45: 0.5881 - val_loss: 0.0504 - val_accuracy: 0.9857 - val_f1: 0.4872 - val_f1_1: 0.8561 - val_f1_2: 0.9453 - val_f1_3: 0.9545 - val_f1_4: 0.8098 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0364 - val_f1_7: 0.9306 - val_f1_9: 0.6094 - val_f1_10: 0.0000e+00 - val_f1_11: 0.8416 - val_f1_12: 0.9995 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.9021 - val_f1_16: 0.8780 - val_f1_17: 0.8055 - val_f1_18: 0.6223 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9921 - val_f1_26: 0.9695 - val_f1_27: 0.0000e+00 - val_f1_28: 0.9337 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.6057 - val_f1_32: 0.7923 - val_f1_33: 0.3398 - val_f1_34: 0.0000e+00 - val_f1_35: 0.7968 - val_f1_36: 0.9547 - val_f1_37: 0.0000e+00 - val_f1_39: 0.7493 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.6563 - val_f1_44: 0.0000e+00 - val_f1_45: 0.5081\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.0415 - accuracy: 0.9887 - f1: 0.5152 - f1_1: 0.9016 - f1_2: 0.9567 - f1_3: 0.9662 - f1_4: 0.8644 - f1_5: 0.0000e+00 - f1_6: 0.1071 - f1_7: 0.9625 - f1_9: 0.6774 - f1_10: 0.0000e+00 - f1_11: 0.8727 - f1_12: 0.9986 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.9267 - f1_16: 0.8961 - f1_17: 0.7882 - f1_18: 0.7248 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.9927 - f1_26: 0.9677 - f1_27: 0.0000e+00 - f1_28: 0.9451 - f1_29: 0.9933 - f1_30: 0.0000e+00 - f1_31: 0.7275 - f1_32: 0.8295 - f1_33: 0.5929 - f1_34: 0.0000e+00 - f1_35: 0.8413 - f1_36: 0.9764 - f1_37: 0.0000e+00 - f1_39: 0.7905 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.7072 - f1_44: 0.0000e+00 - f1_45: 0.6019 - val_loss: 0.0491 - val_accuracy: 0.9862 - val_f1: 0.4901 - val_f1_1: 0.8503 - val_f1_2: 0.9454 - val_f1_3: 0.9680 - val_f1_4: 0.8136 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0364 - val_f1_7: 0.9164 - val_f1_9: 0.6415 - val_f1_10: 0.0000e+00 - val_f1_11: 0.8268 - val_f1_12: 0.9995 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.9036 - val_f1_16: 0.8909 - val_f1_17: 0.7594 - val_f1_18: 0.6758 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9921 - val_f1_26: 0.9712 - val_f1_27: 0.0000e+00 - val_f1_28: 0.9384 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.6161 - val_f1_32: 0.7954 - val_f1_33: 0.3398 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8262 - val_f1_36: 0.9560 - val_f1_37: 0.0000e+00 - val_f1_39: 0.7595 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.6517 - val_f1_44: 0.0000e+00 - val_f1_45: 0.5344\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.0405 - accuracy: 0.9890 - f1: 0.5137 - f1_1: 0.8966 - f1_2: 0.9584 - f1_3: 0.9728 - f1_4: 0.8690 - f1_5: 0.0000e+00 - f1_6: 0.0347 - f1_7: 0.9509 - f1_9: 0.6962 - f1_10: 0.0000e+00 - f1_11: 0.8713 - f1_12: 0.9986 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.9237 - f1_16: 0.9025 - f1_17: 0.7950 - f1_18: 0.7346 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0179 - f1_25: 0.9940 - f1_26: 0.9707 - f1_27: 0.0000e+00 - f1_28: 0.9506 - f1_29: 0.9937 - f1_30: 0.0000e+00 - f1_31: 0.7288 - f1_32: 0.8360 - f1_33: 0.4927 - f1_34: 0.0000e+00 - f1_35: 0.8535 - f1_36: 0.9781 - f1_37: 0.0000e+00 - f1_39: 0.8011 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.7007 - f1_44: 0.0000e+00 - f1_45: 0.6264 - val_loss: 0.0484 - val_accuracy: 0.9864 - val_f1: 0.4948 - val_f1_1: 0.8558 - val_f1_2: 0.9495 - val_f1_3: 0.9659 - val_f1_4: 0.8113 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0364 - val_f1_7: 0.9317 - val_f1_9: 0.6977 - val_f1_10: 0.0000e+00 - val_f1_11: 0.8339 - val_f1_12: 0.9995 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.9051 - val_f1_16: 0.9038 - val_f1_17: 0.8247 - val_f1_18: 0.6962 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9921 - val_f1_26: 0.9733 - val_f1_27: 0.0000e+00 - val_f1_28: 0.9345 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.5857 - val_f1_32: 0.7943 - val_f1_33: 0.4328 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8163 - val_f1_36: 0.9619 - val_f1_37: 0.0000e+00 - val_f1_39: 0.7472 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.6474 - val_f1_44: 0.0000e+00 - val_f1_45: 0.4962\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.0399 - accuracy: 0.9891 - f1: 0.5226 - f1_1: 0.9096 - f1_2: 0.9571 - f1_3: 0.9675 - f1_4: 0.8660 - f1_5: 0.0000e+00 - f1_6: 0.2203 - f1_7: 0.9625 - f1_9: 0.7032 - f1_10: 0.0000e+00 - f1_11: 0.8701 - f1_12: 0.9970 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.9276 - f1_16: 0.8983 - f1_17: 0.8333 - f1_18: 0.7357 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0804 - f1_25: 0.9905 - f1_26: 0.9704 - f1_27: 0.0000e+00 - f1_28: 0.9452 - f1_29: 0.9940 - f1_30: 0.0000e+00 - f1_31: 0.7671 - f1_32: 0.8303 - f1_33: 0.5086 - f1_34: 0.0000e+00 - f1_35: 0.8527 - f1_36: 0.9769 - f1_37: 0.0000e+00 - f1_39: 0.8004 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.7112 - f1_44: 0.0000e+00 - f1_45: 0.6268 - val_loss: 0.0475 - val_accuracy: 0.9865 - val_f1: 0.4996 - val_f1_1: 0.8626 - val_f1_2: 0.9488 - val_f1_3: 0.9680 - val_f1_4: 0.8073 - val_f1_5: 0.0000e+00 - val_f1_6: 0.1566 - val_f1_7: 0.9368 - val_f1_9: 0.6870 - val_f1_10: 0.0000e+00 - val_f1_11: 0.8395 - val_f1_12: 0.9995 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.9057 - val_f1_16: 0.9036 - val_f1_17: 0.8324 - val_f1_18: 0.6917 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.9921 - val_f1_26: 0.9742 - val_f1_27: 0.0000e+00 - val_f1_28: 0.9346 - val_f1_29: 0.9975 - val_f1_30: 0.0000e+00 - val_f1_31: 0.5950 - val_f1_32: 0.8126 - val_f1_33: 0.4414 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8054 - val_f1_36: 0.9626 - val_f1_37: 0.0000e+00 - val_f1_39: 0.7473 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.6720 - val_f1_44: 0.0000e+00 - val_f1_45: 0.5079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrM_XiQ1Bdpp"
      },
      "source": [
        "loss: 0.1255 - accuracy: 0.9631\n",
        "\n",
        "\n",
        "loss: 0.0583 - accuracy: 0.9840 - val_loss: 0.0741 - val_accuracy: 0.9790\n",
        "\n",
        "loss: 0.0589 - accuracy: 0.9836 - val_loss: 0.0711 - val_accuracy: 0.9798"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#History f1 for class"
      ],
      "metadata": {
        "id": "87_kBxnCwm0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_val_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_val_list[i] = history.history['val_f1_{}'.format(i)][-1]\n",
        "f1_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_list[i] = history.history['f1_{}'.format(i)][-1]"
      ],
      "metadata": {
        "id": "nbNUZ6RoZ_qp"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index2tag = {value : key for (key, value) in tag2index.items()}\n",
        "index2tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHprJp5QaCIj",
        "outputId": "783a9412-1fce-47e3-e199-2be89b496dee"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '-PAD-',\n",
              " 1: 'VB',\n",
              " 2: 'PRP',\n",
              " 3: 'PRP$',\n",
              " 4: 'NNP',\n",
              " 5: 'JJR',\n",
              " 6: 'RBR',\n",
              " 7: 'MD',\n",
              " 8: ':',\n",
              " 9: 'RB',\n",
              " 10: 'WP$',\n",
              " 11: 'VBD',\n",
              " 12: 'TO',\n",
              " 13: 'SYM',\n",
              " 14: 'RBS',\n",
              " 15: 'VBZ',\n",
              " 16: 'CD',\n",
              " 17: 'WDT',\n",
              " 18: 'JJ',\n",
              " 19: '``',\n",
              " 20: 'PDT',\n",
              " 21: 'UH',\n",
              " 22: 'JJS',\n",
              " 23: 'EX',\n",
              " 24: \"''\",\n",
              " 25: 'POS',\n",
              " 26: 'DT',\n",
              " 27: '-LRB-',\n",
              " 28: 'IN',\n",
              " 29: '$',\n",
              " 30: 'FW',\n",
              " 31: 'WP',\n",
              " 32: 'NN',\n",
              " 33: 'RP',\n",
              " 34: '#',\n",
              " 35: 'NNS',\n",
              " 36: 'CC',\n",
              " 37: 'WRB',\n",
              " 38: '.',\n",
              " 39: 'VBP',\n",
              " 40: '-RRB-',\n",
              " 41: 'LS',\n",
              " 42: ',',\n",
              " 43: 'VBN',\n",
              " 44: 'NNPS',\n",
              " 45: 'VBG'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- F1: {}'.format(index2tag[i], f1_list[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoPk9XRiaEgx",
        "outputId": "373e7b10-a69b-481c-b924-35bc5cfb603f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: VB --- F1: 0.9096260666847229\n",
            "Tag: PRP --- F1: 0.9570958018302917\n",
            "Tag: PRP$ --- F1: 0.9675381183624268\n",
            "Tag: NNP --- F1: 0.8660444021224976\n",
            "Tag: JJR --- F1: 0.0\n",
            "Tag: RBR --- F1: 0.22034628689289093\n",
            "Tag: MD --- F1: 0.9624855518341064\n",
            "Tag: RB --- F1: 0.7032214999198914\n",
            "Tag: WP$ --- F1: 0.0\n",
            "Tag: VBD --- F1: 0.8701270818710327\n",
            "Tag: TO --- F1: 0.9969877600669861\n",
            "Tag: SYM --- F1: 0.0\n",
            "Tag: RBS --- F1: 0.0\n",
            "Tag: VBZ --- F1: 0.927582323551178\n",
            "Tag: CD --- F1: 0.898320198059082\n",
            "Tag: WDT --- F1: 0.833341658115387\n",
            "Tag: JJ --- F1: 0.7357279658317566\n",
            "Tag: PDT --- F1: 0.0\n",
            "Tag: UH --- F1: 0.0\n",
            "Tag: JJS --- F1: 0.0\n",
            "Tag: EX --- F1: 0.0803571343421936\n",
            "Tag: POS --- F1: 0.9905139803886414\n",
            "Tag: DT --- F1: 0.9703778624534607\n",
            "Tag: -LRB- --- F1: 0.0\n",
            "Tag: IN --- F1: 0.9451820850372314\n",
            "Tag: $ --- F1: 0.9939866662025452\n",
            "Tag: FW --- F1: 0.0\n",
            "Tag: WP --- F1: 0.7671281695365906\n",
            "Tag: NN --- F1: 0.830268919467926\n",
            "Tag: RP --- F1: 0.5085662007331848\n",
            "Tag: # --- F1: 0.0\n",
            "Tag: NNS --- F1: 0.8527275323867798\n",
            "Tag: CC --- F1: 0.9768719673156738\n",
            "Tag: WRB --- F1: 0.0\n",
            "Tag: VBP --- F1: 0.8003693222999573\n",
            "Tag: -RRB- --- F1: 0.0\n",
            "Tag: LS --- F1: 0.0\n",
            "Tag: VBN --- F1: 0.7111513614654541\n",
            "Tag: NNPS --- F1: 0.0\n",
            "Tag: VBG --- F1: 0.6268161535263062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- Val_F1: {}'.format(index2tag[i], f1_val_list[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AIxv0xZaG9M",
        "outputId": "8b6179d1-7c8b-4fab-e6ed-898cbd97d8e6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: VB --- Val_F1: 0.8625966906547546\n",
            "Tag: PRP --- Val_F1: 0.9487737417221069\n",
            "Tag: PRP$ --- Val_F1: 0.9680352807044983\n",
            "Tag: NNP --- Val_F1: 0.8072870373725891\n",
            "Tag: JJR --- Val_F1: 0.0\n",
            "Tag: RBR --- Val_F1: 0.15656565129756927\n",
            "Tag: MD --- Val_F1: 0.9368255138397217\n",
            "Tag: RB --- Val_F1: 0.6870418190956116\n",
            "Tag: WP$ --- Val_F1: 0.0\n",
            "Tag: VBD --- Val_F1: 0.8394569158554077\n",
            "Tag: TO --- Val_F1: 0.9994745254516602\n",
            "Tag: SYM --- Val_F1: 0.0\n",
            "Tag: RBS --- Val_F1: 0.0\n",
            "Tag: VBZ --- Val_F1: 0.9057473540306091\n",
            "Tag: CD --- Val_F1: 0.9036133289337158\n",
            "Tag: WDT --- Val_F1: 0.8323787450790405\n",
            "Tag: JJ --- Val_F1: 0.6917428374290466\n",
            "Tag: PDT --- Val_F1: 0.0\n",
            "Tag: UH --- Val_F1: 0.0\n",
            "Tag: JJS --- Val_F1: 0.0\n",
            "Tag: EX --- Val_F1: 0.0\n",
            "Tag: POS --- Val_F1: 0.9920594692230225\n",
            "Tag: DT --- Val_F1: 0.9742367267608643\n",
            "Tag: -LRB- --- Val_F1: 0.0\n",
            "Tag: IN --- Val_F1: 0.9345651865005493\n",
            "Tag: $ --- Val_F1: 0.9975429773330688\n",
            "Tag: FW --- Val_F1: 0.0\n",
            "Tag: WP --- Val_F1: 0.5950382947921753\n",
            "Tag: NN --- Val_F1: 0.8126346468925476\n",
            "Tag: RP --- Val_F1: 0.44141408801078796\n",
            "Tag: # --- Val_F1: 0.0\n",
            "Tag: NNS --- Val_F1: 0.805379331111908\n",
            "Tag: CC --- Val_F1: 0.962617814540863\n",
            "Tag: WRB --- Val_F1: 0.0\n",
            "Tag: VBP --- Val_F1: 0.7472675442695618\n",
            "Tag: -RRB- --- Val_F1: 0.0\n",
            "Tag: LS --- Val_F1: 0.0\n",
            "Tag: VBN --- Val_F1: 0.6719562411308289\n",
            "Tag: NNPS --- Val_F1: 0.0\n",
            "Tag: VBG --- Val_F1: 0.5079175233840942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Infos"
      ],
      "metadata": {
        "id": "tdQOOMeBaK6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 40 epochs, batch 128\n",
        "loss: 0.0370 - accuracy: 0.9900 - f1: 0.5324"
      ],
      "metadata": {
        "id": "sjumesASaNsh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Evaluation"
      ],
      "metadata": {
        "id": "zCyIRGDdS485"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences_X,  test_tags_y = [], []\n",
        "\n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        "\n",
        "for s in test_tags:\n",
        "    test_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ],
      "metadata": {
        "id": "sUZ9F2amtq-V"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_pred = model.predict(test_sentences_X)\n",
        "\n",
        "y_pred = np.zeros((y_val_pred.shape[0], y_val_pred.shape[1]), dtype=int)\n",
        "for i in range(len(y_val_pred)):\n",
        "  for j in range(len(y_val_pred[i])):\n",
        "    y_pred[i][j] = np.argmax(y_val_pred[i][j])"
      ],
      "metadata": {
        "id": "b7uGU-eftvUI"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the f1_score from sklearn"
      ],
      "metadata": {
        "id": "2_DoVNlxTDY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "tags_flat = test_tags_y.flatten()\n",
        "pred_flat = y_pred.flatten()\n",
        "\n",
        "f1_model = f1_score(tags_flat, pred_flat, labels = no_punct_indexes, average='macro', zero_division=0)\n",
        "f1_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWRGwHcaTeAh",
        "outputId": "33f959db-71df-47ea-d5b4-a4231ec3c3df"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5624085931063405"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}