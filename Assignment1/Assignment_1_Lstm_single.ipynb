{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1_Lstm_single.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Due to**: 23/12/2021 (dd/mm/yyyy)\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Part-of Speech (POS) tagging as Sequence Labelling using Recurrent Neural Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4_wqPdlBcKS"
      },
      "source": [
        "# Intro\n",
        "\n",
        "In this assignment  we will ask you to perform POS tagging using neural architectures\n",
        "\n",
        "You are asked to follow these steps:\n",
        "*   Download the corpora and split it in training and test sets, structuring a dataframe.\n",
        "*   Embed the words using GloVe embeddings\n",
        "*   Create a baseline model, using a simple neural architecture\n",
        "*   Experiment doing small modifications to the baseline model, choose hyperparameters using the validation set\n",
        "*   Evaluate your two best model\n",
        "*   Analyze the errors of your model\n",
        "\n",
        "\n",
        "**Task**: given a corpus of documents, predict the POS tag for each word\n",
        "\n",
        "**Corpus**:\n",
        "Ignore the numeric value in the third column, use only the words/symbols and its label. \n",
        "The corpus is available at:\n",
        "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
        "\n",
        "**Splits**: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
        "\n",
        "\n",
        "**Features**: you MUST use GloVe embeddings as the only input features to the model.\n",
        "\n",
        "**Splitting**: you can decide to split documents into sentences or not, the choice is yours.\n",
        "\n",
        "**I/O structure**: The input data will have three dimensions: 1-documents/sentences, 2-token, 3-features; for the output there are 2 possibilities: if you use one-hot encoding it will be 1-documents/sentences, 2-token labels, 3-classes, if you use a single integer that indicates the number of the class it will be 1-documents/sentences, 2-token labels.\n",
        "\n",
        "**Baseline**: two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top; the choice of hyper-parameters is yours.\n",
        "\n",
        "**Architectures**: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and adding an additional dense layer; do not mix these variantions.\n",
        "\n",
        "\n",
        "**Training and Experiments**: all the experiments must involve only the training and validation sets.\n",
        "\n",
        "**Evaluation**: in the end, only the two best models of your choice (according to the validation set) must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech. DO NOT CONSIDER THE PUNCTUATION CLASSES.\n",
        "\n",
        "**Metrics**: the metric you must use to evaluate your final model is the F1-macro, WITHOUT considering punctuation/symbols classes; during the training process you can use accuracy because you can't use the F1 metric unless you use a single (gigantic) batch because there is no way to aggregate \"partial\" F1 scores computed on mini-batches.\n",
        "\n",
        "**Discussion and Error Analysis** : verify and discuss if the results on the test sets are coherent with those on the validation set; analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
        "\n",
        "**Report**: you are asked to deliver the code of your experiments and a small pdf report of about 2 pages; the pdf must begin with the names of the people of your team and a small abstract (4-5 lines) that sums up your findings.\n",
        "\n",
        "# Out Of Vocabulary (OOV) terms\n",
        "\n",
        "How to handle words that are not in GloVe vocabulary?\n",
        "You can handle them as you want (random embedding, placeholder, whatever!), but they must be STATIC embeddings (you cannot train them).\n",
        "\n",
        "But there is a very important caveat! As usual, the element of the test set must not influence the elements of the other splits!\n",
        "\n",
        "So, when you compute new embeddings for train+validation, you must forget about test documents.\n",
        "The motivation is to emulate a real-world scenario, where you select and train a model in the first stage, without knowing nothing about the testing environment.\n",
        "\n",
        "For implementation convenience, you CAN use a single vocabulary file/matrix/whatever. The principle of the previous point is that the embeddings inside that file/matrix must be generated independently for train and test splits.\n",
        "\n",
        "Basically in a real-world scenario, this is what would happen:\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Training of the model(s)\n",
        "5. Compute embeddings for terms OOV2 of the validation split \n",
        "6. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "7. Validation of the model(s)\n",
        "8. Compute embeddings for terms OOV3 of the test split \n",
        "9. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2+OOV3\n",
        "10. Testing of the final model\n",
        "\n",
        "In this case, where we already have all the documents, we can simplify the process a bit, but the procedure must remain rigorous.\n",
        "\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Compute embeddings for terms OOV2 of the validation split \n",
        "5. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "6. Compute embeddings for terms OOV3 of the test split \n",
        "7. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2\n",
        "8. Training of the model(s)\n",
        "9. Validation of the model(s)\n",
        "10. Testing of the final model\n",
        "\n",
        "Step 2 and step 6 must be completely independent of each other, for what concerns the method and the documents. But they can rely on the previous vocabulary (V1 for step 2 and V3 for step 6)\n",
        "THEREFORE if a word is present both in the training set and the test split and not in the starting vocabulary, its embedding is computed in step 2) and it is not considered OOV anymore in step 6).\n",
        "\n",
        "# Report\n",
        "The report must not be just a copy and paste of graphs and tables!\n",
        "\n",
        "The report must not be longer than 2 pages and must contain:\n",
        "* The names of the member of your team\n",
        "* A short abstract (4-5 lines) that sum ups everything\n",
        "* A general description of the task you have addressed and how you have addressed it\n",
        "* A short description of the models you have used\n",
        "* Some tables that sum up your findings in validation and test and a discussion of those results\n",
        "* The most relevant findings of your error analysis\n",
        "\n",
        "# Evaluation Criterion\n",
        "\n",
        "The goal of this assignment is not to prove you can find best model ever, but to face a common task, structure it correctly, and follow a correct and rigorous experimental procedure.\n",
        "In other words, we don't care if you final models are awful as long as you have followed the correct procedure and wrote a decent report.\n",
        "\n",
        "The score of the assignment will be computed roughly as follows\n",
        "* 1 point for the general setting of the problem\n",
        "* 1 point for the handling of OOV terms\n",
        "* 1 point for the models\n",
        "* 1 point for train-validation-test procedure\n",
        "* 2 point for the discussion of the results, error analysis, and report\n",
        "\n",
        "This distribution of scores is tentative and we may decide to alter it at any moment.\n",
        "We also reserve the right to assign a small bonus (0.5 points) to any assignment that is particularly worthy. Similarly, in case of grave errors, we may decide to assign an equivalent malus (-0.5 points).\n",
        "\n",
        "# Contacts\n",
        "\n",
        "In case of any doubt, question, issue, or help we highly recommend you to check the [course useful material](https://virtuale.unibo.it/pluginfile.php/1036039/mod_resource/content/2/NLP_Course_Useful_Material.pdf) for additional information, and to use the Virtuale forums to discuss with other students.\n",
        "\n",
        "You can always contact us at the following email addresses. To increase the probability of a prompt response, we reccomend you to write to both the teaching assistants.\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Andrea Galassi -> a.galassi@unibo.it\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it\n",
        "\n",
        "\n",
        "# FAQ\n",
        "* You can use a non-trainable Embedding layer to load the glove embeddings\n",
        "* You can use any library of your choice to implement the networks. Two options are tensorflow/keras or pythorch. Both these libraries have all the classes you need to implement these simple architectures and there are plenty of tutorials around, where you can learn how to use them."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Dataset"
      ],
      "metadata": {
        "id": "COWq7PIOWtYr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emdubW66fnlD"
      },
      "source": [
        "import os, shutil  #  file management\n",
        "import sys \n",
        "import pandas as pd  #  dataframe management\n",
        "import numpy as np  #  data manipulation\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request  #  download files\n",
        "import zipfile  #  unzip files\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B-w2pqyfpF1"
      },
      "source": [
        "from keras import metrics \n",
        "from tensorflow.python.keras.metrics import Metric\n",
        "from sklearn.metrics import f1_score\n",
        "import keras.backend as K"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8prqSTRX-TK",
        "outputId": "066b383c-12b0-4ae8-c85a-be5cdbfc51ee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3xn1MZPgA_v",
        "outputId": "e218de5d-3a13-462f-9b22-c35d97c16e4c"
      },
      "source": [
        "dataset_folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\")\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "    os.makedirs(dataset_folder)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
        "\n",
        "dataset_path = os.path.join(dataset_folder, \"treebank.zip\")\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    urllib.request.urlretrieve(url, dataset_path)\n",
        "    print(\"Successful download\")\n",
        "\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_folder)\n",
        "print(\"Successful extraction\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful download\n",
            "Successful extraction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre Processing\n",
        "- splitting"
      ],
      "metadata": {
        "id": "iXl3LpVxW2h5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKbUsrVHhGAG"
      },
      "source": [
        "dataset_name = \"dependency_treebank\"\n",
        "\n",
        "\n",
        "folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\", dataset_name)\n",
        "\n",
        "pre_train = []\n",
        "pre_valid = []\n",
        "pre_test = []\n",
        "i = 1\n",
        "\n",
        "file_list = sorted(os.listdir(folder))\n",
        "\n",
        "for filename in file_list:\n",
        "  file_path = os.path.join(folder, filename)\n",
        "  if os.path.isfile(file_path):\n",
        "    # open the file\n",
        "      text = []\n",
        "      with open(file_path, mode='r', encoding='utf-8') as text_file:\n",
        "        text = text_file.read()\n",
        "        if i <= 100:\n",
        "          pre_train.append(text)\n",
        "        elif i <= 150:\n",
        "          pre_valid.append(text)\n",
        "        else:\n",
        "          pre_test.append(text)  \n",
        "  i+=1\n",
        "\n",
        "tr = []\n",
        "val = []\n",
        "tes = []\n",
        "\n",
        "for paragraph in pre_train:\n",
        "   tr.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_valid:\n",
        "   val.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_test:\n",
        "   tes.append(paragraph.split('\\n'))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUgbTXi-F7Sa"
      },
      "source": [
        "train = []\n",
        "valid = []\n",
        "test = []\n",
        "\n",
        "for i in tr:\n",
        "  for j in i:\n",
        "    train.append(j.split('\\t'))\n",
        "\n",
        "for i in val:\n",
        "  for j in i:\n",
        "    valid.append(j.split('\\t'))\n",
        "\n",
        "for i in tes:\n",
        "  for j in i:\n",
        "    test.append(j.split('\\t'))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmzVaRuQFp0M"
      },
      "source": [
        "train_sentences = []\n",
        "valid_sentences = []\n",
        "test_sentences = []\n",
        "train_tags = []\n",
        "valid_tags = []\n",
        "test_tags = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in train:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    train_sentences.append(s)\n",
        "    train_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in valid:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    valid_sentences.append(s)\n",
        "    valid_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in test:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    test_sentences.append(s)\n",
        "    test_tags.append(t)\n",
        "    s = []\n",
        "    t = []"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB65cUFLIeEr"
      },
      "source": [
        "flat_train = [item for sublist in train_sentences for item in sublist]\n",
        "flat_valid = [item for sublist in valid_sentences for item in sublist]\n",
        "flat_test = [item for sublist in test_sentences for item in sublist]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "- vocabularies\n",
        "- GloVe download"
      ],
      "metadata": {
        "id": "VQsxPZcXW9EF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZO6ga1EGHdh"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "train_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "train_tokenizer.fit_on_texts(train_sentences)                    # fit tokeniser on data\n",
        "train_encoded = train_tokenizer.texts_to_sequences(train_sentences)\n",
        "\n",
        "valid_tokenizer = Tokenizer()       \n",
        "valid_tokenizer.fit_on_texts(valid_sentences)                  # instantiate tokeniser\n",
        "valid_encoded = valid_tokenizer.texts_to_sequences(valid_sentences)\n",
        "\n",
        "test_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "test_tokenizer.fit_on_texts(test_sentences)                    # fit tokeniser on data\n",
        "test_encoded = test_tokenizer.texts_to_sequences(test_sentences)  # use the tokeniser to encode input sequence"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(train_tags)\n",
        "tags_encoded = tag_tokenizer.texts_to_sequences(train_tags)"
      ],
      "metadata": {
        "id": "1nC8ma_jXYtc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMsG0Yw1l50L",
        "outputId": "bd71cb55-c054-47f6-eac6-4c5417418691"
      },
      "source": [
        "voc = list(set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(valid_tokenizer.word_index.keys()) - set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(test_tokenizer.word_index.keys()) - set(voc))\n",
        "print(len(voc))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7404\n",
            "9901\n",
            "10947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS9RwnOtJuas"
      },
      "source": [
        "word_index = dict(zip(voc, range(2, len(voc)+3)))\n",
        "word_index['-PAD-'] = 0\n",
        "word_index['-OOV-'] = 1\n",
        "\n",
        "tags = set([item for sublist in train_tags for item in sublist])\n",
        "\n",
        "tag2index = {t: i + 1 for i, t in enumerate(tags)}\n",
        "tag2index['-PAD-'] = 0 "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoj4Zx51J2MU",
        "outputId": "d6e34c17-2ff2-4c29-eab6-265757124077"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-16 16:47:08--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-12-16 16:47:09--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-12-16 16:47:09--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 42s  \n",
            "\n",
            "2021-12-16 16:49:51 (5.08 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc-T6FcLKtS9",
        "outputId": "1559607b-9763-4f2d-cd19-6f7cff4770d0"
      },
      "source": [
        "path_to_glove_file = os.path.join(os.getcwd(), \"glove.6B.100d.txt\")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding\n",
        "- matrix creation\n",
        "- padding\n",
        "- one hot encoding of tags\n",
        "- punctuation identification"
      ],
      "metadata": {
        "id": "_x9MpHE6XB2l"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0mOStEPMeXt",
        "outputId": "26ba408b-4771-447c-8b84-54fd17f50857"
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        embedding_matrix += np.random.uniform(low=-0.05, high=0.05, size=embedding_dim)\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 10271 words (678 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwoJ4hbYXFPq"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpYrHs0RMtQJ"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8EZM31Js06-",
        "outputId": "72d9783c-6a39-4aaa-8c22-161a22c6cd0b"
      },
      "source": [
        "MAX_LENGTH = len(max(train_sentences, key=len))\n",
        "print(MAX_LENGTH)\n",
        "MAX_LENGTH2 = len(max(valid_sentences, key=len))\n",
        "print(MAX_LENGTH2)\n",
        "MAX_LENGTH3 = len(max(test_sentences, key=len))\n",
        "print(MAX_LENGTH3)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "249\n",
            "81\n",
            "58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSRa9SwTuNrX"
      },
      "source": [
        "train_sentences_X, valid_sentences_X, train_tags_y, valid_tags_y = [], [], [], []\n",
        " \n",
        "for s in train_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        "\n",
        "for s in valid_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    valid_sentences_X.append(s_int)\n",
        " \n",
        "for s in train_tags:\n",
        "    train_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "for s in valid_tags:\n",
        "    valid_tags_y.append([tag2index[t] for t in s])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf-XlSrStQx3"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_sentences_X = pad_sequences(valid_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_tags_y = pad_sequences(valid_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)"
      ],
      "metadata": {
        "id": "yc_u-8itYXQB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "point = [tag2index['.']]\n",
        "virg = [tag2index[',']]\n",
        "weird_apex = [tag2index['``']]\n",
        "single_apex = [tag2index[\"''\"]]\n",
        "two_dots = [tag2index[':']]\n",
        "pad = [tag2index['-PAD-']]\n",
        "\n",
        "punct_cat_classes = to_categorical([point, virg, weird_apex, single_apex, two_dots, pad], len(tag2index))\n",
        "punct_cat_classes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tvb1ZbVIYZ4Y",
        "outputId": "43151bc9-3698-4d88-9ac2-a19a1daa4d2e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 1, 46)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cum_tags = np.zeros(46)\n",
        "for i in punct_cat_classes:\n",
        "  print(i[0])\n",
        "  cum_tags += i[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7oHM_vUYf4c",
        "outputId": "9931f13f-be4b-4e64-8971-8d580aaed3c7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "where_tags = np.where(np.logical_not(cum_tags))\n",
        "where_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5uT2QJrYi15",
        "outputId": "86244c0b-e49e-4bb7-b579-6048b1f505c1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 1,  2,  3,  4,  5,  6,  8,  9, 10, 11, 13, 14, 15, 16, 17, 18, 19,\n",
              "        20, 21, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39,\n",
              "        40, 41, 42, 43, 44, 45]),)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_punct_indexes = where_tags[0]\n",
        "n_classes = len(no_punct_indexes)"
      ],
      "metadata": {
        "id": "nPBHVI2HYlxs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "cat_val_tags_y = to_categorical(valid_tags_y, len(tag2index))\n",
        "print(len(cat_train_tags_y), len(cat_val_tags_y))\n",
        "print(len(train_sentences_X), len(valid_sentences_X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SPgQ9YmYnbG",
        "outputId": "5ba5be7b-1ff7-4bf4-dd9a-c5dff4bf4ff9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1963 1299\n",
            "1963 1299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mini Data Vizualization Inset"
      ],
      "metadata": {
        "id": "426ajTPoYtaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_tags = np.zeros(46)\n",
        "for i in cat_train_tags_y:\n",
        "  for t in i:\n",
        "    distribution_tags += t"
      ],
      "metadata": {
        "id": "Jf_djyP3YvhV"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = sum(distribution_tags[1:])\n",
        "norm = [float(i)/s for i in distribution_tags[1:]]"
      ],
      "metadata": {
        "id": "8jqHRmS_Yyde"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt "
      ],
      "metadata": {
        "id": "-ksL6xl1Y2Sw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(np.arange(len(norm)),norm)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "o4EY5adaY3Z6",
        "outputId": "88074187-65a7-4414-8d02-aefd67314dd1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#F1 Metric"
      ],
      "metadata": {
        "id": "yUvMJPihY4nK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric.\n",
        "\n",
        "    Only computes a batch-wise average of precision.\n",
        "\n",
        "    Computes the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric.\n",
        "\n",
        "    Only computes a batch-wise average of recall.\n",
        "\n",
        "    Computes the recall, a metric for multi-label classification of\n",
        "    how many relevant items are selected.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def f1_binary(y_true, y_pred):\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
        "\n",
        "def metric_wrapper(index):\n",
        "  def f1_for_class(y_true, y_pred):\n",
        "    #get only the desired class\n",
        "    true = y_true[:,:,index]\n",
        "    pred = y_pred[:,:,index]\n",
        "    #return dice per class\n",
        "    tmp = f1_binary(true,pred)\n",
        "    return tmp\n",
        "  f1_for_class.__name__ = 'f1_' + str(index)\n",
        "  return f1_for_class\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    result = 0.0\n",
        "    for class_id in no_punct_indexes:\n",
        "        y_true_single_class = y_true[:,:,class_id]\n",
        "        y_pred_single_class = y_pred[:,:,class_id]\n",
        "        f1_single = f1_binary(y_true_single_class, y_pred_single_class)\n",
        "        result += f1_single / float(n_classes)\n",
        "    return result"
      ],
      "metadata": {
        "id": "_4dwoTOuZCmi"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsVGByW_sdec",
        "outputId": "a8d384fb-c99f-492f-e945-c7ab6c978067"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(embedding_layer)\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(Dense(len(tag2index)))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy', f1, [metric_wrapper(i) for i in no_punct_indexes]])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 249, 100)          1094900   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 249, 512)         731136    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 249, 46)           23598     \n",
            "                                                                 \n",
            " activation (Activation)     (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,849,634\n",
            "Trainable params: 754,734\n",
            "Non-trainable params: 1,094,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "history = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=40, validation_data=(valid_sentences_X, cat_val_tags_y), callbacks=[callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n45419-PZSR7",
        "outputId": "755615fb-7c06-4426-fae5-46939cd8d68f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "16/16 [==============================] - 70s 4s/step - loss: 0.7621 - accuracy: 0.8456 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.3491 - val_accuracy: 0.9167 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - 54s 3s/step - loss: 0.3127 - accuracy: 0.9164 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2909 - val_accuracy: 0.9197 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.2807 - accuracy: 0.9259 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2707 - val_accuracy: 0.9339 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.2609 - accuracy: 0.9362 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2535 - val_accuracy: 0.9390 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - 56s 4s/step - loss: 0.2420 - accuracy: 0.9424 - f1: 0.0012 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0464 - f1_45: 0.0000e+00 - val_loss: 0.2340 - val_accuracy: 0.9435 - val_f1: 0.0011 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0439 - val_f1_45: 0.0000e+00\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.2212 - accuracy: 0.9450 - f1: 0.0064 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.2552 - f1_45: 0.0000e+00 - val_loss: 0.2132 - val_accuracy: 0.9460 - val_f1: 0.0049 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0048 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.1838 - val_f1_45: 0.0066\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.1992 - accuracy: 0.9482 - f1: 0.0186 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 6.2901e-04 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.1764 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.4096 - f1_45: 0.1573 - val_loss: 0.1915 - val_accuracy: 0.9511 - val_f1: 0.0232 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0110 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0083 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.3334 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.2757 - val_f1_45: 0.2984\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.1778 - accuracy: 0.9534 - f1: 0.0423 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0276 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0229 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.6750 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.5235 - f1_45: 0.4428 - val_loss: 0.1712 - val_accuracy: 0.9568 - val_f1: 0.0528 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.2498 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0384 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0023 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.8077 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.4358 - val_f1_45: 0.5781\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - 56s 4s/step - loss: 0.1581 - accuracy: 0.9594 - f1: 0.0632 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.2632 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.1339 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0081 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 6.4433e-04 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.8568 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.6018 - f1_45: 0.6631 - val_loss: 0.1535 - val_accuracy: 0.9614 - val_f1: 0.0719 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.4656 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.2812 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0402 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.0077 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0111 - val_f1_41: 0.8732 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.4778 - val_f1_45: 0.7191\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.1410 - accuracy: 0.9646 - f1: 0.0882 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0300 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.4219 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.3127 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0835 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.0344 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.3520 - f1_41: 0.8858 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.6345 - f1_45: 0.7747 - val_loss: 0.1385 - val_accuracy: 0.9656 - val_f1: 0.1056 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0503 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0017 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.5658 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.3991 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.1643 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.0791 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.7725 - val_f1_41: 0.8892 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.5281 - val_f1_45: 0.7753\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - 56s 4s/step - loss: 0.1268 - accuracy: 0.9685 - f1: 0.1301 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.1370 - f1_10: 0.0000e+00 - f1_11: 0.1486 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.5293 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.4493 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.2368 - f1_32: 0.0278 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.1045 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.2601 - f1_40: 0.9058 - f1_41: 0.8996 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.6802 - f1_45: 0.8265 - val_loss: 0.1256 - val_accuracy: 0.9687 - val_f1: 0.1482 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.1258 - val_f1_10: 0.0000e+00 - val_f1_11: 0.3053 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.6032 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.5085 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.2068 - val_f1_32: 0.0274 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.1522 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.6880 - val_f1_40: 0.9730 - val_f1_41: 0.9002 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.6177 - val_f1_45: 0.8211\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - 56s 3s/step - loss: 0.1148 - accuracy: 0.9718 - f1: 0.1764 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.2677 - f1_10: 0.0000e+00 - f1_11: 0.5936 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.6048 - f1_19: 0.0000e+00 - f1_20: 0.0219 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.5375 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.3349 - f1_32: 0.1384 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.2149 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.8874 - f1_40: 0.9934 - f1_41: 0.9045 - f1_42: 0.0017 - f1_43: 0.0000e+00 - f1_44: 0.7016 - f1_45: 0.8547 - val_loss: 0.1152 - val_accuracy: 0.9714 - val_f1: 0.1859 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.2814 - val_f1_10: 0.0000e+00 - val_f1_11: 0.7044 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.6671 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0459 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.5477 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.3653 - val_f1_32: 0.2996 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.2006 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9131 - val_f1_40: 0.9976 - val_f1_41: 0.9041 - val_f1_42: 0.0145 - val_f1_43: 0.0000e+00 - val_f1_44: 0.6553 - val_f1_45: 0.8395\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.1049 - accuracy: 0.9743 - f1: 0.2060 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.4097 - f1_10: 0.0000e+00 - f1_11: 0.7915 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0108 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.6629 - f1_19: 0.0000e+00 - f1_20: 0.2028 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.5923 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.4538 - f1_32: 0.3524 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.2902 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9629 - f1_40: 0.9982 - f1_41: 0.9108 - f1_42: 0.0097 - f1_43: 0.0000e+00 - f1_44: 0.7258 - f1_45: 0.8663 - val_loss: 0.1065 - val_accuracy: 0.9735 - val_f1: 0.2126 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0070 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.3285 - val_f1_10: 0.0000e+00 - val_f1_11: 0.7612 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0309 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.7066 - val_f1_19: 0.0000e+00 - val_f1_20: 0.3386 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.5671 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.4638 - val_f1_32: 0.4482 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.3613 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9904 - val_f1_40: 0.9994 - val_f1_41: 0.9123 - val_f1_42: 0.0390 - val_f1_43: 0.0000e+00 - val_f1_44: 0.6944 - val_f1_45: 0.8561\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - 56s 4s/step - loss: 0.0968 - accuracy: 0.9759 - f1: 0.2329 - f1_1: 0.0000e+00 - f1_2: 0.0094 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.4848 - f1_10: 0.0000e+00 - f1_11: 0.8201 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0375 - f1_16: 0.1089 - f1_17: 0.0000e+00 - f1_18: 0.6933 - f1_19: 0.0000e+00 - f1_20: 0.4934 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.6402 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.5266 - f1_32: 0.5311 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.3743 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9882 - f1_40: 0.9986 - f1_41: 0.9154 - f1_42: 0.0690 - f1_43: 0.0000e+00 - f1_44: 0.7480 - f1_45: 0.8756 - val_loss: 0.0989 - val_accuracy: 0.9749 - val_f1: 0.2454 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0171 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.4825 - val_f1_10: 0.0000e+00 - val_f1_11: 0.7737 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.1039 - val_f1_16: 0.3627 - val_f1_17: 0.0000e+00 - val_f1_18: 0.7366 - val_f1_19: 0.0000e+00 - val_f1_20: 0.5532 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.6484 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.5208 - val_f1_32: 0.5094 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.3878 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9952 - val_f1_40: 1.0000 - val_f1_41: 0.9169 - val_f1_42: 0.2411 - val_f1_43: 0.0000e+00 - val_f1_44: 0.7032 - val_f1_45: 0.8636\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.0898 - accuracy: 0.9776 - f1: 0.2618 - f1_1: 0.0000e+00 - f1_2: 0.0701 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.5613 - f1_10: 0.0000e+00 - f1_11: 0.8212 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.1141 - f1_16: 0.4957 - f1_17: 0.0052 - f1_18: 0.7170 - f1_19: 0.0000e+00 - f1_20: 0.6575 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.6785 - f1_25: 0.0013 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.5763 - f1_32: 0.5844 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.4241 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9905 - f1_40: 0.9987 - f1_41: 0.9236 - f1_42: 0.2086 - f1_43: 0.0021 - f1_44: 0.7612 - f1_45: 0.8821 - val_loss: 0.0927 - val_accuracy: 0.9765 - val_f1: 0.2765 - val_f1_1: 0.0000e+00 - val_f1_2: 0.1312 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.4772 - val_f1_10: 0.0000e+00 - val_f1_11: 0.7737 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.1856 - val_f1_16: 0.7890 - val_f1_17: 0.0152 - val_f1_18: 0.7513 - val_f1_19: 0.0000e+00 - val_f1_20: 0.7243 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.6590 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.5757 - val_f1_32: 0.6114 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.4500 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9206 - val_f1_42: 0.4165 - val_f1_43: 0.0000e+00 - val_f1_44: 0.7100 - val_f1_45: 0.8728\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - 56s 4s/step - loss: 0.0839 - accuracy: 0.9790 - f1: 0.2930 - f1_1: 0.0000e+00 - f1_2: 0.2509 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0306 - f1_9: 0.6073 - f1_10: 0.0000e+00 - f1_11: 0.8294 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.2077 - f1_16: 0.8534 - f1_17: 0.0409 - f1_18: 0.7452 - f1_19: 0.0000e+00 - f1_20: 0.7184 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.6978 - f1_25: 0.0062 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.6346 - f1_32: 0.6747 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.4591 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9943 - f1_40: 0.9985 - f1_41: 0.9306 - f1_42: 0.3749 - f1_43: 0.0072 - f1_44: 0.7751 - f1_45: 0.8845 - val_loss: 0.0878 - val_accuracy: 0.9773 - val_f1: 0.2982 - val_f1_1: 0.0000e+00 - val_f1_2: 0.1453 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.1396 - val_f1_9: 0.5836 - val_f1_10: 0.0000e+00 - val_f1_11: 0.7756 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.1674 - val_f1_16: 0.9024 - val_f1_17: 0.2171 - val_f1_18: 0.7687 - val_f1_19: 0.0000e+00 - val_f1_20: 0.7647 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.6820 - val_f1_25: 0.0035 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.5675 - val_f1_32: 0.6839 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.4241 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9305 - val_f1_42: 0.5290 - val_f1_43: 0.0039 - val_f1_44: 0.7626 - val_f1_45: 0.8808\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.0792 - accuracy: 0.9801 - f1: 0.3195 - f1_1: 0.0000e+00 - f1_2: 0.3040 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.2429 - f1_9: 0.6466 - f1_10: 0.0000e+00 - f1_11: 0.8284 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.3124 - f1_16: 0.9204 - f1_17: 0.2852 - f1_18: 0.7611 - f1_19: 0.0000e+00 - f1_20: 0.7540 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.7171 - f1_25: 0.0251 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.6621 - f1_32: 0.7074 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.4964 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9942 - f1_40: 0.9986 - f1_41: 0.9369 - f1_42: 0.4808 - f1_43: 0.0375 - f1_44: 0.7766 - f1_45: 0.8924 - val_loss: 0.0839 - val_accuracy: 0.9784 - val_f1: 0.3219 - val_f1_1: 0.0000e+00 - val_f1_2: 0.4292 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.3958 - val_f1_9: 0.6035 - val_f1_10: 0.0000e+00 - val_f1_11: 0.7932 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.1939 - val_f1_16: 0.9363 - val_f1_17: 0.3107 - val_f1_18: 0.7751 - val_f1_19: 0.0000e+00 - val_f1_20: 0.7822 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.6792 - val_f1_25: 0.0217 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.6095 - val_f1_32: 0.6921 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.4399 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9372 - val_f1_42: 0.5984 - val_f1_43: 0.0328 - val_f1_44: 0.7673 - val_f1_45: 0.8823\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - 56s 4s/step - loss: 0.0747 - accuracy: 0.9811 - f1: 0.3459 - f1_1: 0.0000e+00 - f1_2: 0.3769 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.5310 - f1_9: 0.6747 - f1_10: 0.0000e+00 - f1_11: 0.8401 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.4150 - f1_16: 0.9173 - f1_17: 0.4387 - f1_18: 0.7830 - f1_19: 0.0000e+00 - f1_20: 0.7760 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.7290 - f1_25: 0.0726 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.6990 - f1_32: 0.7611 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.5239 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9925 - f1_40: 0.9986 - f1_41: 0.9399 - f1_42: 0.6122 - f1_43: 0.0704 - f1_44: 0.7847 - f1_45: 0.8985 - val_loss: 0.0792 - val_accuracy: 0.9794 - val_f1: 0.3431 - val_f1_1: 0.0000e+00 - val_f1_2: 0.5060 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.5325 - val_f1_9: 0.6040 - val_f1_10: 0.0000e+00 - val_f1_11: 0.8293 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.3302 - val_f1_16: 0.9449 - val_f1_17: 0.4498 - val_f1_18: 0.7879 - val_f1_19: 0.0000e+00 - val_f1_20: 0.7944 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.7231 - val_f1_25: 0.0263 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.6467 - val_f1_32: 0.7258 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.5025 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9409 - val_f1_42: 0.6803 - val_f1_43: 0.0432 - val_f1_44: 0.7739 - val_f1_45: 0.8862\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.0708 - accuracy: 0.9822 - f1: 0.3652 - f1_1: 0.0000e+00 - f1_2: 0.4706 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.7345 - f1_9: 0.6920 - f1_10: 0.0000e+00 - f1_11: 0.8669 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.4849 - f1_16: 0.9322 - f1_17: 0.5375 - f1_18: 0.7830 - f1_19: 0.0000e+00 - f1_20: 0.7854 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.7441 - f1_25: 0.0896 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.7237 - f1_32: 0.7775 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.5591 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9920 - f1_40: 0.9987 - f1_41: 0.9434 - f1_42: 0.6821 - f1_43: 0.1129 - f1_44: 0.7966 - f1_45: 0.9016 - val_loss: 0.0755 - val_accuracy: 0.9804 - val_f1: 0.3606 - val_f1_1: 0.0000e+00 - val_f1_2: 0.5191 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.7018 - val_f1_9: 0.6932 - val_f1_10: 0.0000e+00 - val_f1_11: 0.8372 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.3668 - val_f1_16: 0.9449 - val_f1_17: 0.5545 - val_f1_18: 0.7947 - val_f1_19: 0.0000e+00 - val_f1_20: 0.8080 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.7350 - val_f1_25: 0.1399 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.6604 - val_f1_32: 0.7475 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.4570 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9448 - val_f1_42: 0.7662 - val_f1_43: 0.0859 - val_f1_44: 0.7750 - val_f1_45: 0.8928\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - 56s 4s/step - loss: 0.0675 - accuracy: 0.9829 - f1: 0.3802 - f1_1: 0.0000e+00 - f1_2: 0.4998 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.8197 - f1_9: 0.7346 - f1_10: 0.0000e+00 - f1_11: 0.8703 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.5347 - f1_16: 0.9414 - f1_17: 0.5849 - f1_18: 0.7980 - f1_19: 0.0000e+00 - f1_20: 0.8028 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.7519 - f1_25: 0.1729 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.7321 - f1_32: 0.8039 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.5745 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9916 - f1_40: 0.9987 - f1_41: 0.9460 - f1_42: 0.7484 - f1_43: 0.1945 - f1_44: 0.8040 - f1_45: 0.9030 - val_loss: 0.0725 - val_accuracy: 0.9813 - val_f1: 0.3738 - val_f1_1: 0.0000e+00 - val_f1_2: 0.5483 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.7823 - val_f1_9: 0.7025 - val_f1_10: 0.0000e+00 - val_f1_11: 0.8447 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.3913 - val_f1_16: 0.9430 - val_f1_17: 0.6299 - val_f1_18: 0.8187 - val_f1_19: 0.0000e+00 - val_f1_20: 0.8067 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.7160 - val_f1_25: 0.2020 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.7267 - val_f1_32: 0.7808 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.5306 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9474 - val_f1_42: 0.8051 - val_f1_43: 0.1114 - val_f1_44: 0.7732 - val_f1_45: 0.8950\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.0645 - accuracy: 0.9837 - f1: 0.3919 - f1_1: 0.0000e+00 - f1_2: 0.5075 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.8697 - f1_9: 0.7500 - f1_10: 0.0000e+00 - f1_11: 0.8756 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.5614 - f1_16: 0.9417 - f1_17: 0.7063 - f1_18: 0.8139 - f1_19: 0.0000e+00 - f1_20: 0.8207 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.7630 - f1_25: 0.2072 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.7591 - f1_32: 0.8167 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.5828 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9926 - f1_40: 0.9987 - f1_41: 0.9483 - f1_42: 0.7860 - f1_43: 0.2580 - f1_44: 0.8090 - f1_45: 0.9093 - val_loss: 0.0695 - val_accuracy: 0.9817 - val_f1: 0.3858 - val_f1_1: 0.0000e+00 - val_f1_2: 0.5760 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.8185 - val_f1_9: 0.6758 - val_f1_10: 0.0000e+00 - val_f1_11: 0.8446 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.5094 - val_f1_16: 0.9462 - val_f1_17: 0.6893 - val_f1_18: 0.8323 - val_f1_19: 0.0000e+00 - val_f1_20: 0.8161 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.7824 - val_f1_25: 0.2275 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.1244 - val_f1_31: 0.6928 - val_f1_32: 0.8052 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.5447 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9504 - val_f1_42: 0.8149 - val_f1_43: 0.1230 - val_f1_44: 0.7628 - val_f1_45: 0.8970\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.0618 - accuracy: 0.9844 - f1: 0.4036 - f1_1: 0.0000e+00 - f1_2: 0.5824 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.8999 - f1_9: 0.7480 - f1_10: 0.0000e+00 - f1_11: 0.8715 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.5964 - f1_16: 0.9398 - f1_17: 0.7301 - f1_18: 0.8272 - f1_19: 0.0000e+00 - f1_20: 0.8260 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.7732 - f1_25: 0.2505 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0114 - f1_30: 0.0900 - f1_31: 0.7664 - f1_32: 0.8300 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.6060 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9943 - f1_40: 0.9987 - f1_41: 0.9517 - f1_42: 0.8195 - f1_43: 0.3053 - f1_44: 0.8135 - f1_45: 0.9107 - val_loss: 0.0670 - val_accuracy: 0.9827 - val_f1: 0.3984 - val_f1_1: 0.0000e+00 - val_f1_2: 0.6198 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.8590 - val_f1_9: 0.7144 - val_f1_10: 0.0000e+00 - val_f1_11: 0.8456 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.5663 - val_f1_16: 0.9462 - val_f1_17: 0.6844 - val_f1_18: 0.8499 - val_f1_19: 0.0000e+00 - val_f1_20: 0.8452 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.7746 - val_f1_25: 0.3161 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.2048 - val_f1_31: 0.7118 - val_f1_32: 0.7982 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.5673 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9525 - val_f1_42: 0.8033 - val_f1_43: 0.2218 - val_f1_44: 0.7506 - val_f1_45: 0.9060\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.0595 - accuracy: 0.9850 - f1: 0.4170 - f1_1: 0.0000e+00 - f1_2: 0.5915 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.9152 - f1_9: 0.7688 - f1_10: 0.0000e+00 - f1_11: 0.8820 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.6255 - f1_16: 0.9422 - f1_17: 0.7785 - f1_18: 0.8374 - f1_19: 0.0000e+00 - f1_20: 0.8542 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.7812 - f1_25: 0.3287 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0551 - f1_30: 0.2070 - f1_31: 0.7798 - f1_32: 0.8339 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.6152 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9943 - f1_40: 0.9986 - f1_41: 0.9537 - f1_42: 0.8323 - f1_43: 0.3716 - f1_44: 0.8221 - f1_45: 0.9130 - val_loss: 0.0654 - val_accuracy: 0.9828 - val_f1: 0.4088 - val_f1_1: 0.0000e+00 - val_f1_2: 0.6423 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.9177 - val_f1_9: 0.7349 - val_f1_10: 0.0000e+00 - val_f1_11: 0.8589 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.4982 - val_f1_16: 0.9483 - val_f1_17: 0.7781 - val_f1_18: 0.8564 - val_f1_19: 0.0000e+00 - val_f1_20: 0.8512 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.7353 - val_f1_25: 0.2702 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0303 - val_f1_30: 0.3750 - val_f1_31: 0.7613 - val_f1_32: 0.7824 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.5997 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9532 - val_f1_42: 0.8483 - val_f1_43: 0.2309 - val_f1_44: 0.7736 - val_f1_45: 0.9093\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.0573 - accuracy: 0.9853 - f1: 0.4358 - f1_1: 0.0000e+00 - f1_2: 0.6248 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.9358 - f1_9: 0.7897 - f1_10: 0.0000e+00 - f1_11: 0.8805 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.6223 - f1_16: 0.9330 - f1_17: 0.8046 - f1_18: 0.8417 - f1_19: 0.0000e+00 - f1_20: 0.8594 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.7883 - f1_25: 0.3561 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.2749 - f1_30: 0.5067 - f1_31: 0.7868 - f1_32: 0.8365 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.6297 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9942 - f1_40: 0.9986 - f1_41: 0.9569 - f1_42: 0.8718 - f1_43: 0.3981 - f1_44: 0.8264 - f1_45: 0.9161 - val_loss: 0.0627 - val_accuracy: 0.9835 - val_f1: 0.4231 - val_f1_1: 0.0000e+00 - val_f1_2: 0.6333 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.9369 - val_f1_9: 0.7486 - val_f1_10: 0.0000e+00 - val_f1_11: 0.8610 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.5238 - val_f1_16: 0.9501 - val_f1_17: 0.8427 - val_f1_18: 0.8682 - val_f1_19: 0.0000e+00 - val_f1_20: 0.8644 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.7832 - val_f1_25: 0.3752 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.1425 - val_f1_30: 0.4139 - val_f1_31: 0.7570 - val_f1_32: 0.8357 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.5614 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9546 - val_f1_42: 0.8601 - val_f1_43: 0.3386 - val_f1_44: 0.7683 - val_f1_45: 0.9086\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.0553 - accuracy: 0.9859 - f1: 0.4453 - f1_1: 0.0000e+00 - f1_2: 0.6272 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.9387 - f1_9: 0.7994 - f1_10: 0.0000e+00 - f1_11: 0.8848 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.6375 - f1_16: 0.9388 - f1_17: 0.8426 - f1_18: 0.8447 - f1_19: 0.0000e+00 - f1_20: 0.8760 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.7954 - f1_25: 0.3916 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.4056 - f1_30: 0.5126 - f1_31: 0.8056 - f1_32: 0.8554 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.6377 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9912 - f1_40: 0.9986 - f1_41: 0.9577 - f1_42: 0.8791 - f1_43: 0.4393 - f1_44: 0.8301 - f1_45: 0.9209 - val_loss: 0.0609 - val_accuracy: 0.9840 - val_f1: 0.4393 - val_f1_1: 0.0000e+00 - val_f1_2: 0.6579 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.9474 - val_f1_9: 0.7281 - val_f1_10: 0.0000e+00 - val_f1_11: 0.8701 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.5655 - val_f1_16: 0.9513 - val_f1_17: 0.7894 - val_f1_18: 0.8632 - val_f1_19: 0.0000e+00 - val_f1_20: 0.8730 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.7734 - val_f1_25: 0.4236 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.4097 - val_f1_30: 0.5931 - val_f1_31: 0.7652 - val_f1_32: 0.8333 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.6060 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9543 - val_f1_42: 0.8780 - val_f1_43: 0.3877 - val_f1_44: 0.7919 - val_f1_45: 0.9107\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.0535 - accuracy: 0.9863 - f1: 0.4576 - f1_1: 0.0000e+00 - f1_2: 0.6601 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.9617 - f1_9: 0.8137 - f1_10: 0.0000e+00 - f1_11: 0.8968 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.6550 - f1_16: 0.9422 - f1_17: 0.8598 - f1_18: 0.8617 - f1_19: 0.0000e+00 - f1_20: 0.8864 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.7932 - f1_25: 0.4595 - f1_26: 0.0125 - f1_27: 0.0000e+00 - f1_29: 0.5580 - f1_30: 0.5253 - f1_31: 0.8043 - f1_32: 0.8579 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.6472 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9930 - f1_40: 0.9985 - f1_41: 0.9600 - f1_42: 0.9056 - f1_43: 0.4948 - f1_44: 0.8331 - f1_45: 0.9244 - val_loss: 0.0600 - val_accuracy: 0.9839 - val_f1: 0.4466 - val_f1_1: 0.0000e+00 - val_f1_2: 0.6543 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.9567 - val_f1_9: 0.7750 - val_f1_10: 0.0000e+00 - val_f1_11: 0.8737 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.5842 - val_f1_16: 0.9622 - val_f1_17: 0.8637 - val_f1_18: 0.8572 - val_f1_19: 0.0000e+00 - val_f1_20: 0.8838 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.7720 - val_f1_25: 0.3362 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.5031 - val_f1_30: 0.6774 - val_f1_31: 0.7421 - val_f1_32: 0.8379 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.5876 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9560 - val_f1_42: 0.9109 - val_f1_43: 0.4159 - val_f1_44: 0.7971 - val_f1_45: 0.9185\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - 56s 4s/step - loss: 0.0519 - accuracy: 0.9865 - f1: 0.4687 - f1_1: 0.0000e+00 - f1_2: 0.6744 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.9636 - f1_9: 0.8151 - f1_10: 0.0000e+00 - f1_11: 0.9077 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.6586 - f1_16: 0.9434 - f1_17: 0.8707 - f1_18: 0.8679 - f1_19: 0.0000e+00 - f1_20: 0.8899 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.8067 - f1_25: 0.4355 - f1_26: 0.0565 - f1_27: 0.0000e+00 - f1_29: 0.6902 - f1_30: 0.6347 - f1_31: 0.8148 - f1_32: 0.8719 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.6658 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9944 - f1_40: 0.9986 - f1_41: 0.9597 - f1_42: 0.9297 - f1_43: 0.5359 - f1_44: 0.8338 - f1_45: 0.9287 - val_loss: 0.0577 - val_accuracy: 0.9846 - val_f1: 0.4586 - val_f1_1: 0.0000e+00 - val_f1_2: 0.6823 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.9687 - val_f1_9: 0.7617 - val_f1_10: 0.0000e+00 - val_f1_11: 0.8887 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6117 - val_f1_16: 0.9646 - val_f1_17: 0.8952 - val_f1_18: 0.8662 - val_f1_19: 0.0000e+00 - val_f1_20: 0.8873 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.7930 - val_f1_25: 0.4773 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.6205 - val_f1_30: 0.7101 - val_f1_31: 0.7678 - val_f1_32: 0.8473 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.6253 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9649 - val_f1_42: 0.8985 - val_f1_43: 0.4109 - val_f1_44: 0.7944 - val_f1_45: 0.9113\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - 56s 3s/step - loss: 0.0503 - accuracy: 0.9870 - f1: 0.4768 - f1_1: 0.0000e+00 - f1_2: 0.6883 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.9733 - f1_9: 0.8277 - f1_10: 0.0000e+00 - f1_11: 0.9220 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.6605 - f1_16: 0.9595 - f1_17: 0.8862 - f1_18: 0.8739 - f1_19: 0.0000e+00 - f1_20: 0.8979 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.8134 - f1_25: 0.5107 - f1_26: 0.0757 - f1_27: 0.0000e+00 - f1_29: 0.6504 - f1_30: 0.7458 - f1_31: 0.8194 - f1_32: 0.8711 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.6782 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9944 - f1_40: 0.9985 - f1_41: 0.9628 - f1_42: 0.9288 - f1_43: 0.5668 - f1_44: 0.8376 - f1_45: 0.9300 - val_loss: 0.0565 - val_accuracy: 0.9847 - val_f1: 0.4617 - val_f1_1: 0.0000e+00 - val_f1_2: 0.6856 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.9678 - val_f1_9: 0.7907 - val_f1_10: 0.0000e+00 - val_f1_11: 0.9179 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.5735 - val_f1_16: 0.9644 - val_f1_17: 0.8882 - val_f1_18: 0.8752 - val_f1_19: 0.0000e+00 - val_f1_20: 0.8808 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.7865 - val_f1_25: 0.4668 - val_f1_26: 0.0182 - val_f1_27: 0.0000e+00 - val_f1_29: 0.6205 - val_f1_30: 0.7443 - val_f1_31: 0.7778 - val_f1_32: 0.8524 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.6341 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9664 - val_f1_42: 0.9167 - val_f1_43: 0.4238 - val_f1_44: 0.8016 - val_f1_45: 0.9191\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.0488 - accuracy: 0.9873 - f1: 0.4853 - f1_1: 0.0000e+00 - f1_2: 0.7021 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.9759 - f1_9: 0.8355 - f1_10: 0.0000e+00 - f1_11: 0.9450 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.6839 - f1_16: 0.9611 - f1_17: 0.8944 - f1_18: 0.8882 - f1_19: 0.0000e+00 - f1_20: 0.8979 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.8172 - f1_25: 0.5137 - f1_26: 0.1503 - f1_27: 0.0000e+00 - f1_29: 0.7435 - f1_30: 0.7698 - f1_31: 0.8322 - f1_32: 0.8833 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.6813 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9948 - f1_40: 0.9988 - f1_41: 0.9646 - f1_42: 0.9397 - f1_43: 0.5666 - f1_44: 0.8408 - f1_45: 0.9311 - val_loss: 0.0553 - val_accuracy: 0.9849 - val_f1: 0.4651 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7193 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.9687 - val_f1_9: 0.7840 - val_f1_10: 0.0000e+00 - val_f1_11: 0.9177 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.5537 - val_f1_16: 0.9774 - val_f1_17: 0.8945 - val_f1_18: 0.8756 - val_f1_19: 0.0000e+00 - val_f1_20: 0.8860 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.8084 - val_f1_25: 0.4629 - val_f1_26: 0.0182 - val_f1_27: 0.0000e+00 - val_f1_29: 0.6508 - val_f1_30: 0.7318 - val_f1_31: 0.7651 - val_f1_32: 0.8499 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.6300 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9700 - val_f1_42: 0.9136 - val_f1_43: 0.5034 - val_f1_44: 0.8003 - val_f1_45: 0.9260\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.0476 - accuracy: 0.9875 - f1: 0.4907 - f1_1: 0.0000e+00 - f1_2: 0.7136 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.9828 - f1_9: 0.8451 - f1_10: 0.0000e+00 - f1_11: 0.9441 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.6875 - f1_16: 0.9711 - f1_17: 0.9193 - f1_18: 0.8850 - f1_19: 0.0000e+00 - f1_20: 0.9086 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.8153 - f1_25: 0.5510 - f1_26: 0.1916 - f1_27: 0.0000e+00 - f1_29: 0.7495 - f1_30: 0.7894 - f1_31: 0.8302 - f1_32: 0.8906 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.6848 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9952 - f1_40: 0.9985 - f1_41: 0.9654 - f1_42: 0.9409 - f1_43: 0.5892 - f1_44: 0.8431 - f1_45: 0.9365 - val_loss: 0.0544 - val_accuracy: 0.9852 - val_f1: 0.4693 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7127 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.9698 - val_f1_9: 0.7786 - val_f1_10: 0.0000e+00 - val_f1_11: 0.8923 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.5901 - val_f1_16: 0.9838 - val_f1_17: 0.8871 - val_f1_18: 0.8782 - val_f1_19: 0.0000e+00 - val_f1_20: 0.9009 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.7799 - val_f1_25: 0.5887 - val_f1_26: 0.0745 - val_f1_27: 0.0000e+00 - val_f1_29: 0.6508 - val_f1_30: 0.7193 - val_f1_31: 0.7952 - val_f1_32: 0.8597 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.6354 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9695 - val_f1_42: 0.9194 - val_f1_43: 0.4567 - val_f1_44: 0.8087 - val_f1_45: 0.9250\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.0465 - accuracy: 0.9878 - f1: 0.4972 - f1_1: 0.0000e+00 - f1_2: 0.7348 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.9783 - f1_9: 0.8418 - f1_10: 0.0000e+00 - f1_11: 0.9474 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.6782 - f1_16: 0.9756 - f1_17: 0.9125 - f1_18: 0.8895 - f1_19: 0.0000e+00 - f1_20: 0.9106 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.8177 - f1_25: 0.5705 - f1_26: 0.2872 - f1_27: 0.0000e+00 - f1_29: 0.8064 - f1_30: 0.8148 - f1_31: 0.8412 - f1_32: 0.8842 - f1_33: 0.0089 - f1_34: 0.0000e+00 - f1_36: 0.6999 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9945 - f1_40: 0.9986 - f1_41: 0.9670 - f1_42: 0.9443 - f1_43: 0.6028 - f1_44: 0.8441 - f1_45: 0.9384 - val_loss: 0.0530 - val_accuracy: 0.9856 - val_f1: 0.4774 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7211 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.9801 - val_f1_9: 0.8051 - val_f1_10: 0.0000e+00 - val_f1_11: 0.9312 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6148 - val_f1_16: 0.9900 - val_f1_17: 0.9151 - val_f1_18: 0.8805 - val_f1_19: 0.0000e+00 - val_f1_20: 0.9131 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.8001 - val_f1_25: 0.5391 - val_f1_26: 0.1276 - val_f1_27: 0.0000e+00 - val_f1_29: 0.6508 - val_f1_30: 0.7990 - val_f1_31: 0.7917 - val_f1_32: 0.8562 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.6689 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9708 - val_f1_42: 0.9136 - val_f1_43: 0.4956 - val_f1_44: 0.8064 - val_f1_45: 0.9267\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.0453 - accuracy: 0.9880 - f1: 0.5029 - f1_1: 0.0000e+00 - f1_2: 0.7378 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.9804 - f1_9: 0.8462 - f1_10: 0.0000e+00 - f1_11: 0.9518 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.7037 - f1_16: 0.9787 - f1_17: 0.9284 - f1_18: 0.8942 - f1_19: 0.0000e+00 - f1_20: 0.9125 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.8284 - f1_25: 0.5851 - f1_26: 0.4434 - f1_27: 0.0000e+00 - f1_29: 0.7551 - f1_30: 0.8312 - f1_31: 0.8445 - f1_32: 0.8930 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_36: 0.7053 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9951 - f1_40: 0.9985 - f1_41: 0.9684 - f1_42: 0.9468 - f1_43: 0.6036 - f1_44: 0.8463 - f1_45: 0.9382 - val_loss: 0.0521 - val_accuracy: 0.9857 - val_f1: 0.4812 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7244 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.9773 - val_f1_9: 0.8046 - val_f1_10: 0.0000e+00 - val_f1_11: 0.9408 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6400 - val_f1_16: 0.9824 - val_f1_17: 0.9223 - val_f1_18: 0.8790 - val_f1_19: 0.0000e+00 - val_f1_20: 0.9047 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.7937 - val_f1_25: 0.5133 - val_f1_26: 0.2022 - val_f1_27: 0.0000e+00 - val_f1_29: 0.6632 - val_f1_30: 0.8499 - val_f1_31: 0.8137 - val_f1_32: 0.8593 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.6473 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9711 - val_f1_42: 0.9256 - val_f1_43: 0.4987 - val_f1_44: 0.8116 - val_f1_45: 0.9271\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.0440 - accuracy: 0.9883 - f1: 0.5071 - f1_1: 0.0000e+00 - f1_2: 0.7658 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.9819 - f1_9: 0.8525 - f1_10: 0.0000e+00 - f1_11: 0.9580 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.7152 - f1_16: 0.9838 - f1_17: 0.9322 - f1_18: 0.9001 - f1_19: 0.0000e+00 - f1_20: 0.9119 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.8315 - f1_25: 0.6043 - f1_26: 0.3930 - f1_27: 0.0000e+00 - f1_29: 0.8125 - f1_30: 0.8318 - f1_31: 0.8417 - f1_32: 0.8873 - f1_33: 0.0210 - f1_34: 0.0000e+00 - f1_36: 0.7107 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9938 - f1_40: 0.9986 - f1_41: 0.9706 - f1_42: 0.9532 - f1_43: 0.6363 - f1_44: 0.8534 - f1_45: 0.9413 - val_loss: 0.0509 - val_accuracy: 0.9861 - val_f1: 0.4848 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7636 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.9752 - val_f1_9: 0.8325 - val_f1_10: 0.0000e+00 - val_f1_11: 0.9425 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6170 - val_f1_16: 0.9833 - val_f1_17: 0.9144 - val_f1_18: 0.8872 - val_f1_19: 0.0000e+00 - val_f1_20: 0.9121 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.8092 - val_f1_25: 0.5726 - val_f1_26: 0.1762 - val_f1_27: 0.0000e+00 - val_f1_29: 0.6541 - val_f1_30: 0.8623 - val_f1_31: 0.8084 - val_f1_32: 0.8589 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.6539 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9721 - val_f1_42: 0.9248 - val_f1_43: 0.5373 - val_f1_44: 0.8081 - val_f1_45: 0.9282\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.0430 - accuracy: 0.9886 - f1: 0.5095 - f1_1: 0.0000e+00 - f1_2: 0.7556 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.9829 - f1_9: 0.8534 - f1_10: 0.0000e+00 - f1_11: 0.9583 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.7131 - f1_16: 0.9862 - f1_17: 0.9361 - f1_18: 0.8991 - f1_19: 0.0000e+00 - f1_20: 0.9253 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.8332 - f1_25: 0.6302 - f1_26: 0.4244 - f1_27: 0.0000e+00 - f1_29: 0.7750 - f1_30: 0.8824 - f1_31: 0.8511 - f1_32: 0.8966 - f1_33: 0.0270 - f1_34: 0.0000e+00 - f1_36: 0.7149 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9912 - f1_40: 0.9986 - f1_41: 0.9702 - f1_42: 0.9550 - f1_43: 0.6215 - f1_44: 0.8562 - f1_45: 0.9431 - val_loss: 0.0504 - val_accuracy: 0.9861 - val_f1: 0.4884 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7530 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0303 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.9764 - val_f1_9: 0.7903 - val_f1_10: 0.0000e+00 - val_f1_11: 0.9533 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6441 - val_f1_16: 0.9824 - val_f1_17: 0.9141 - val_f1_18: 0.8821 - val_f1_19: 0.0000e+00 - val_f1_20: 0.8992 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.8202 - val_f1_25: 0.5762 - val_f1_26: 0.3349 - val_f1_27: 0.0000e+00 - val_f1_29: 0.6632 - val_f1_30: 0.8404 - val_f1_31: 0.8135 - val_f1_32: 0.8620 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.6707 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9736 - val_f1_42: 0.9312 - val_f1_43: 0.4923 - val_f1_44: 0.8067 - val_f1_45: 0.9273\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.0422 - accuracy: 0.9887 - f1: 0.5166 - f1_1: 0.0000e+00 - f1_2: 0.7804 - f1_3: 0.0000e+00 - f1_4: 0.0646 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.9834 - f1_9: 0.8655 - f1_10: 0.0000e+00 - f1_11: 0.9653 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.7205 - f1_16: 0.9833 - f1_17: 0.9253 - f1_18: 0.9059 - f1_19: 0.0000e+00 - f1_20: 0.9246 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.8323 - f1_25: 0.6457 - f1_26: 0.5441 - f1_27: 0.0000e+00 - f1_29: 0.7970 - f1_30: 0.8661 - f1_31: 0.8534 - f1_32: 0.9022 - f1_33: 0.0228 - f1_34: 0.0000e+00 - f1_36: 0.7169 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9941 - f1_40: 0.9985 - f1_41: 0.9700 - f1_42: 0.9566 - f1_43: 0.6469 - f1_44: 0.8526 - f1_45: 0.9444 - val_loss: 0.0497 - val_accuracy: 0.9863 - val_f1: 0.4918 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7630 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0303 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.9813 - val_f1_9: 0.8379 - val_f1_10: 0.0000e+00 - val_f1_11: 0.9470 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6225 - val_f1_16: 0.9841 - val_f1_17: 0.9174 - val_f1_18: 0.8935 - val_f1_19: 0.0000e+00 - val_f1_20: 0.9043 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.7934 - val_f1_25: 0.5714 - val_f1_26: 0.3349 - val_f1_27: 0.0000e+00 - val_f1_29: 0.6632 - val_f1_30: 0.8426 - val_f1_31: 0.8150 - val_f1_32: 0.8667 - val_f1_33: 0.0083 - val_f1_34: 0.0000e+00 - val_f1_36: 0.6801 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9745 - val_f1_42: 0.9317 - val_f1_43: 0.5569 - val_f1_44: 0.8175 - val_f1_45: 0.9364\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.0412 - accuracy: 0.9891 - f1: 0.5183 - f1_1: 0.0000e+00 - f1_2: 0.7850 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.9838 - f1_9: 0.8620 - f1_10: 0.0000e+00 - f1_11: 0.9690 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.7243 - f1_16: 0.9870 - f1_17: 0.9459 - f1_18: 0.9090 - f1_19: 0.0000e+00 - f1_20: 0.9249 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.8417 - f1_25: 0.6278 - f1_26: 0.5158 - f1_27: 0.0000e+00 - f1_29: 0.7983 - f1_30: 0.8744 - f1_31: 0.8562 - f1_32: 0.9068 - f1_33: 0.1186 - f1_34: 0.0000e+00 - f1_36: 0.7312 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9881 - f1_40: 0.9986 - f1_41: 0.9721 - f1_42: 0.9537 - f1_43: 0.6489 - f1_44: 0.8595 - f1_45: 0.9477 - val_loss: 0.0489 - val_accuracy: 0.9865 - val_f1: 0.4950 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7159 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0303 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.9812 - val_f1_9: 0.8067 - val_f1_10: 0.0000e+00 - val_f1_11: 0.9532 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6710 - val_f1_16: 0.9884 - val_f1_17: 0.9213 - val_f1_18: 0.8973 - val_f1_19: 0.0000e+00 - val_f1_20: 0.9109 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.7968 - val_f1_25: 0.6462 - val_f1_26: 0.3866 - val_f1_27: 0.0000e+00 - val_f1_29: 0.6632 - val_f1_30: 0.8796 - val_f1_31: 0.8222 - val_f1_32: 0.8742 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_36: 0.6646 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9750 - val_f1_42: 0.9318 - val_f1_43: 0.5312 - val_f1_44: 0.8223 - val_f1_45: 0.9345\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.0403 - accuracy: 0.9893 - f1: 0.5244 - f1_1: 0.0000e+00 - f1_2: 0.8027 - f1_3: 0.0000e+00 - f1_4: 0.0692 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.9801 - f1_9: 0.8730 - f1_10: 0.0000e+00 - f1_11: 0.9713 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.7478 - f1_16: 0.9894 - f1_17: 0.9454 - f1_18: 0.9111 - f1_19: 0.0000e+00 - f1_20: 0.9282 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.8417 - f1_25: 0.6724 - f1_26: 0.5642 - f1_27: 0.0000e+00 - f1_29: 0.8085 - f1_30: 0.8755 - f1_31: 0.8617 - f1_32: 0.9037 - f1_33: 0.1022 - f1_34: 0.0000e+00 - f1_36: 0.7332 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9925 - f1_40: 0.9986 - f1_41: 0.9728 - f1_42: 0.9608 - f1_43: 0.6638 - f1_44: 0.8594 - f1_45: 0.9475 - val_loss: 0.0479 - val_accuracy: 0.9866 - val_f1: 0.4969 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7539 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0303 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.9830 - val_f1_9: 0.8269 - val_f1_10: 0.0000e+00 - val_f1_11: 0.9554 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6390 - val_f1_16: 0.9903 - val_f1_17: 0.9494 - val_f1_18: 0.8934 - val_f1_19: 0.0000e+00 - val_f1_20: 0.9080 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.8182 - val_f1_25: 0.5672 - val_f1_26: 0.3822 - val_f1_27: 0.0000e+00 - val_f1_29: 0.6632 - val_f1_30: 0.8507 - val_f1_31: 0.8299 - val_f1_32: 0.8717 - val_f1_33: 0.0671 - val_f1_34: 0.0000e+00 - val_f1_36: 0.7000 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9752 - val_f1_42: 0.9431 - val_f1_43: 0.5341 - val_f1_44: 0.8080 - val_f1_45: 0.9382\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.0397 - accuracy: 0.9893 - f1: 0.5262 - f1_1: 0.0000e+00 - f1_2: 0.8027 - f1_3: 0.0000e+00 - f1_4: 0.1406 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.9826 - f1_9: 0.8785 - f1_10: 0.0000e+00 - f1_11: 0.9762 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.7241 - f1_16: 0.9848 - f1_17: 0.9484 - f1_18: 0.9106 - f1_19: 0.0000e+00 - f1_20: 0.9278 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.8424 - f1_25: 0.6791 - f1_26: 0.5434 - f1_27: 0.0000e+00 - f1_29: 0.7987 - f1_30: 0.8777 - f1_31: 0.8641 - f1_32: 0.9110 - f1_33: 0.1309 - f1_34: 0.0000e+00 - f1_36: 0.7327 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9951 - f1_40: 0.9985 - f1_41: 0.9739 - f1_42: 0.9627 - f1_43: 0.6538 - f1_44: 0.8570 - f1_45: 0.9491 - val_loss: 0.0473 - val_accuracy: 0.9870 - val_f1: 0.5047 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7745 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0722 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.9830 - val_f1_9: 0.8053 - val_f1_10: 0.0000e+00 - val_f1_11: 0.9620 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6830 - val_f1_16: 0.9903 - val_f1_17: 0.9348 - val_f1_18: 0.8978 - val_f1_19: 0.0000e+00 - val_f1_20: 0.9205 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.8111 - val_f1_25: 0.6811 - val_f1_26: 0.4117 - val_f1_27: 0.0000e+00 - val_f1_29: 0.6632 - val_f1_30: 0.8790 - val_f1_31: 0.8358 - val_f1_32: 0.8700 - val_f1_33: 0.0732 - val_f1_34: 0.0000e+00 - val_f1_36: 0.6893 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9756 - val_f1_42: 0.9630 - val_f1_43: 0.5713 - val_f1_44: 0.8047 - val_f1_45: 0.9364\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.0387 - accuracy: 0.9897 - f1: 0.5293 - f1_1: 0.0000e+00 - f1_2: 0.8157 - f1_3: 0.0000e+00 - f1_4: 0.0820 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.9848 - f1_9: 0.8738 - f1_10: 0.0000e+00 - f1_11: 0.9755 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.7407 - f1_16: 0.9891 - f1_17: 0.9490 - f1_18: 0.9188 - f1_19: 0.0000e+00 - f1_20: 0.9350 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.8479 - f1_25: 0.6992 - f1_26: 0.5989 - f1_27: 0.0000e+00 - f1_29: 0.7820 - f1_30: 0.9028 - f1_31: 0.8704 - f1_32: 0.9102 - f1_33: 0.1507 - f1_34: 0.0000e+00 - f1_36: 0.7388 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9928 - f1_40: 0.9986 - f1_41: 0.9747 - f1_42: 0.9627 - f1_43: 0.6649 - f1_44: 0.8638 - f1_45: 0.9478 - val_loss: 0.0470 - val_accuracy: 0.9869 - val_f1: 0.5037 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7584 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0519 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.9815 - val_f1_9: 0.8091 - val_f1_10: 0.0000e+00 - val_f1_11: 0.9577 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6553 - val_f1_16: 0.9874 - val_f1_17: 0.9350 - val_f1_18: 0.8932 - val_f1_19: 0.0000e+00 - val_f1_20: 0.9100 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.8004 - val_f1_25: 0.6669 - val_f1_26: 0.4073 - val_f1_27: 0.0000e+00 - val_f1_29: 0.6632 - val_f1_30: 0.8695 - val_f1_31: 0.8290 - val_f1_32: 0.8763 - val_f1_33: 0.0565 - val_f1_34: 0.0000e+00 - val_f1_36: 0.7120 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9765 - val_f1_42: 0.9638 - val_f1_43: 0.6263 - val_f1_44: 0.8238 - val_f1_45: 0.9381\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - 55s 3s/step - loss: 0.0379 - accuracy: 0.9899 - f1: 0.5361 - f1_1: 0.0000e+00 - f1_2: 0.8157 - f1_3: 0.0000e+00 - f1_4: 0.1979 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.9795 - f1_9: 0.8740 - f1_10: 0.0000e+00 - f1_11: 0.9757 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.7415 - f1_16: 0.9891 - f1_17: 0.9578 - f1_18: 0.9214 - f1_19: 0.0000e+00 - f1_20: 0.9311 - f1_21: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.8473 - f1_25: 0.7135 - f1_26: 0.6645 - f1_27: 0.0000e+00 - f1_29: 0.8246 - f1_30: 0.9064 - f1_31: 0.8690 - f1_32: 0.9128 - f1_33: 0.1591 - f1_34: 0.0000e+00 - f1_36: 0.7466 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9922 - f1_40: 0.9984 - f1_41: 0.9738 - f1_42: 0.9631 - f1_43: 0.6720 - f1_44: 0.8661 - f1_45: 0.9506 - val_loss: 0.0462 - val_accuracy: 0.9869 - val_f1: 0.5031 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7406 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0722 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.9815 - val_f1_9: 0.8329 - val_f1_10: 0.0000e+00 - val_f1_11: 0.9584 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6666 - val_f1_16: 0.9939 - val_f1_17: 0.9561 - val_f1_18: 0.8966 - val_f1_19: 0.0000e+00 - val_f1_20: 0.9167 - val_f1_21: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.8262 - val_f1_25: 0.5956 - val_f1_26: 0.4073 - val_f1_27: 0.0000e+00 - val_f1_29: 0.6632 - val_f1_30: 0.8865 - val_f1_31: 0.8193 - val_f1_32: 0.8786 - val_f1_33: 0.1131 - val_f1_34: 0.0000e+00 - val_f1_36: 0.6815 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.9975 - val_f1_40: 1.0000 - val_f1_41: 0.9781 - val_f1_42: 0.9638 - val_f1_43: 0.5342 - val_f1_44: 0.8252 - val_f1_45: 0.9380\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrM_XiQ1Bdpp"
      },
      "source": [
        "loss: 0.1255 - accuracy: 0.9631\n",
        "\n",
        "\n",
        "loss: 0.0583 - accuracy: 0.9840 - val_loss: 0.0741 - val_accuracy: 0.9790\n",
        "\n",
        "loss: 0.0589 - accuracy: 0.9836 - val_loss: 0.0711 - val_accuracy: 0.9798"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#History f1 for class"
      ],
      "metadata": {
        "id": "87_kBxnCwm0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_val_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_val_list[i] = history.history['val_f1_{}'.format(i)][-1]\n",
        "f1_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_list[i] = history.history['f1_{}'.format(i)][-1]"
      ],
      "metadata": {
        "id": "nbNUZ6RoZ_qp"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index2tag = {value : key for (key, value) in tag2index.items()}\n",
        "index2tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHprJp5QaCIj",
        "outputId": "4c94a06c-9018-4ae6-baca-52a08128ac80"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '-PAD-',\n",
              " 1: 'RBS',\n",
              " 2: 'VBP',\n",
              " 3: 'PDT',\n",
              " 4: 'RBR',\n",
              " 5: 'LS',\n",
              " 6: 'EX',\n",
              " 7: '.',\n",
              " 8: 'PRP$',\n",
              " 9: 'VBD',\n",
              " 10: 'JJS',\n",
              " 11: 'CC',\n",
              " 12: '``',\n",
              " 13: 'NNPS',\n",
              " 14: '#',\n",
              " 15: 'VBN',\n",
              " 16: 'POS',\n",
              " 17: 'MD',\n",
              " 18: 'CD',\n",
              " 19: 'WRB',\n",
              " 20: 'VBZ',\n",
              " 21: '-RRB-',\n",
              " 22: \"''\",\n",
              " 23: 'UH',\n",
              " 24: 'NN',\n",
              " 25: 'RB',\n",
              " 26: 'RP',\n",
              " 27: 'SYM',\n",
              " 28: ',',\n",
              " 29: 'WP',\n",
              " 30: 'WDT',\n",
              " 31: 'NNS',\n",
              " 32: 'VB',\n",
              " 33: 'JJR',\n",
              " 34: 'WP$',\n",
              " 35: ':',\n",
              " 36: 'JJ',\n",
              " 37: '-LRB-',\n",
              " 38: 'FW',\n",
              " 39: '$',\n",
              " 40: 'TO',\n",
              " 41: 'DT',\n",
              " 42: 'PRP',\n",
              " 43: 'VBG',\n",
              " 44: 'NNP',\n",
              " 45: 'IN'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- F1: {}'.format(index2tag[i], f1_list[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoPk9XRiaEgx",
        "outputId": "cbd43b52-5933-4cc3-d8be-5d6750da6574"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: RBS --- F1: 0.0\n",
            "Tag: VBP --- F1: 0.8157486319541931\n",
            "Tag: PDT --- F1: 0.0\n",
            "Tag: RBR --- F1: 0.19791662693023682\n",
            "Tag: LS --- F1: 0.0\n",
            "Tag: EX --- F1: 0.0\n",
            "Tag: PRP$ --- F1: 0.9794678092002869\n",
            "Tag: VBD --- F1: 0.8740100264549255\n",
            "Tag: JJS --- F1: 0.0\n",
            "Tag: CC --- F1: 0.9757417440414429\n",
            "Tag: NNPS --- F1: 0.0\n",
            "Tag: # --- F1: 0.0\n",
            "Tag: VBN --- F1: 0.7414681315422058\n",
            "Tag: POS --- F1: 0.9891156554222107\n",
            "Tag: MD --- F1: 0.9577773809432983\n",
            "Tag: CD --- F1: 0.9213674068450928\n",
            "Tag: WRB --- F1: 0.0\n",
            "Tag: VBZ --- F1: 0.9310685396194458\n",
            "Tag: -RRB- --- F1: 0.0\n",
            "Tag: UH --- F1: 0.0\n",
            "Tag: NN --- F1: 0.8472992777824402\n",
            "Tag: RB --- F1: 0.713545024394989\n",
            "Tag: RP --- F1: 0.6645159125328064\n",
            "Tag: SYM --- F1: 0.0\n",
            "Tag: WP --- F1: 0.8245948553085327\n",
            "Tag: WDT --- F1: 0.9063559174537659\n",
            "Tag: NNS --- F1: 0.8689863681793213\n",
            "Tag: VB --- F1: 0.9128196835517883\n",
            "Tag: JJR --- F1: 0.15914501249790192\n",
            "Tag: WP$ --- F1: 0.0\n",
            "Tag: JJ --- F1: 0.7465896010398865\n",
            "Tag: -LRB- --- F1: 0.0\n",
            "Tag: FW --- F1: 0.0\n",
            "Tag: $ --- F1: 0.9921833276748657\n",
            "Tag: TO --- F1: 0.9983798265457153\n",
            "Tag: DT --- F1: 0.9737866520881653\n",
            "Tag: PRP --- F1: 0.9630961418151855\n",
            "Tag: VBG --- F1: 0.6719732880592346\n",
            "Tag: NNP --- F1: 0.8660573959350586\n",
            "Tag: IN --- F1: 0.950634777545929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- Val_F1: {}'.format(index2tag[i], f1_val_list[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AIxv0xZaG9M",
        "outputId": "36fcca28-d9dd-42de-fb2d-d90ff38c0b08"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: RBS --- Val_F1: 0.0\n",
            "Tag: VBP --- Val_F1: 0.7406047582626343\n",
            "Tag: PDT --- Val_F1: 0.0\n",
            "Tag: RBR --- Val_F1: 0.07215006649494171\n",
            "Tag: LS --- Val_F1: 0.0\n",
            "Tag: EX --- Val_F1: 0.0\n",
            "Tag: PRP$ --- Val_F1: 0.9814789295196533\n",
            "Tag: VBD --- Val_F1: 0.8329290151596069\n",
            "Tag: JJS --- Val_F1: 0.0\n",
            "Tag: CC --- Val_F1: 0.9583572149276733\n",
            "Tag: NNPS --- Val_F1: 0.0\n",
            "Tag: # --- Val_F1: 0.0\n",
            "Tag: VBN --- Val_F1: 0.666554868221283\n",
            "Tag: POS --- Val_F1: 0.9939132928848267\n",
            "Tag: MD --- Val_F1: 0.9560943841934204\n",
            "Tag: CD --- Val_F1: 0.8965994119644165\n",
            "Tag: WRB --- Val_F1: 0.0\n",
            "Tag: VBZ --- Val_F1: 0.9167433381080627\n",
            "Tag: -RRB- --- Val_F1: 0.0\n",
            "Tag: UH --- Val_F1: 0.0\n",
            "Tag: NN --- Val_F1: 0.826198160648346\n",
            "Tag: RB --- Val_F1: 0.5956461429595947\n",
            "Tag: RP --- Val_F1: 0.40732595324516296\n",
            "Tag: SYM --- Val_F1: 0.0\n",
            "Tag: WP --- Val_F1: 0.6632034778594971\n",
            "Tag: WDT --- Val_F1: 0.886470377445221\n",
            "Tag: NNS --- Val_F1: 0.819266676902771\n",
            "Tag: VB --- Val_F1: 0.8786352276802063\n",
            "Tag: JJR --- Val_F1: 0.11305873841047287\n",
            "Tag: WP$ --- Val_F1: 0.0\n",
            "Tag: JJ --- Val_F1: 0.6814725399017334\n",
            "Tag: -LRB- --- Val_F1: 0.0\n",
            "Tag: FW --- Val_F1: 0.0\n",
            "Tag: $ --- Val_F1: 0.9975429773330688\n",
            "Tag: TO --- Val_F1: 1.0\n",
            "Tag: DT --- Val_F1: 0.978132426738739\n",
            "Tag: PRP --- Val_F1: 0.9638340473175049\n",
            "Tag: VBG --- Val_F1: 0.5342015624046326\n",
            "Tag: NNP --- Val_F1: 0.8252038359642029\n",
            "Tag: IN --- Val_F1: 0.9379787445068359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Infos"
      ],
      "metadata": {
        "id": "tdQOOMeBaK6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 40 epochs, batch 128\n",
        "loss: 0.0370 - accuracy: 0.9900 - f1: 0.5324"
      ],
      "metadata": {
        "id": "sjumesASaNsh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Evaluation"
      ],
      "metadata": {
        "id": "zCyIRGDdS485"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences_X,  test_tags_y = [], []\n",
        "\n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        "\n",
        "for s in test_tags:\n",
        "    test_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ],
      "metadata": {
        "id": "sUZ9F2amtq-V"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "f1 function written for the test evaluation"
      ],
      "metadata": {
        "id": "8YcziTQ0TAQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_score_single(y_true, y_pred):\n",
        "    y_true = set(y_true)\n",
        "    y_pred = set(y_pred)\n",
        "    cross_size = len(y_true & y_pred)\n",
        "    if cross_size == 0: return 0.\n",
        "    p = 1. * cross_size / len(y_pred)\n",
        "    r = 1. * cross_size / len(y_true)\n",
        "    return 2 * p * r / (p + r)\n",
        "    \n",
        "def f1_test(y_true, y_pred):\n",
        "    return np.mean([f1_score_single(x, y) for x, y in zip(y_true, y_pred)])"
      ],
      "metadata": {
        "id": "eDtMgfzA95AG"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_pred = model.predict(test_sentences_X)\n",
        "\n",
        "y_pred = np.zeros((y_val_pred.shape[0], y_val_pred.shape[1]), dtype=int)\n",
        "for i in range(len(y_val_pred)):\n",
        "  for j in range(len(y_val_pred[i])):\n",
        "    y_pred[i][j] = np.argmax(y_val_pred[i][j])\n",
        "\n",
        "\n",
        "f1_val = f1_test(test_tags_y, y_pred)"
      ],
      "metadata": {
        "id": "b7uGU-eftvUI"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF73WVsD9-W9",
        "outputId": "ac969861-d63f-471c-8c95-dd61b4175ced"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9360187836067922"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the f1_score from sklearn"
      ],
      "metadata": {
        "id": "2_DoVNlxTDY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "tags_flat = test_tags_y.flatten()\n",
        "pred_flat = y_pred.flatten()\n",
        "\n",
        "print(f1_score(tags_flat, pred_flat, average='weighted'))\n",
        "print(f1_score(tags_flat, pred_flat, average='macro'))\n",
        "print(f1_score(tags_flat, pred_flat, average='micro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWRGwHcaTeAh",
        "outputId": "725f4468-a0c2-4a0e-ecc4-a483d060d7f8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9869249870167318\n",
            "0.6972288732166144\n",
            "0.9874651982161776\n"
          ]
        }
      ]
    }
  ]
}