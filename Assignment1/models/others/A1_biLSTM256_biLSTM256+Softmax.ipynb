{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_biLSTM256_biLSTM256+Softmax.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Due to**: 23/12/2021 (dd/mm/yyyy)\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Part-of Speech (POS) tagging as Sequence Labelling using Recurrent Neural Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4_wqPdlBcKS"
      },
      "source": [
        "# Intro\n",
        "\n",
        "In this assignment  we will ask you to perform POS tagging using neural architectures\n",
        "\n",
        "You are asked to follow these steps:\n",
        "*   Download the corpora and split it in training and test sets, structuring a dataframe.\n",
        "*   Embed the words using GloVe embeddings\n",
        "*   Create a baseline model, using a simple neural architecture\n",
        "*   Experiment doing small modifications to the baseline model, choose hyperparameters using the validation set\n",
        "*   Evaluate your two best model\n",
        "*   Analyze the errors of your model\n",
        "\n",
        "\n",
        "**Task**: given a corpus of documents, predict the POS tag for each word\n",
        "\n",
        "**Corpus**:\n",
        "Ignore the numeric value in the third column, use only the words/symbols and its label. \n",
        "The corpus is available at:\n",
        "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
        "\n",
        "**Splits**: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
        "\n",
        "\n",
        "**Features**: you MUST use GloVe embeddings as the only input features to the model.\n",
        "\n",
        "**Splitting**: you can decide to split documents into sentences or not, the choice is yours.\n",
        "\n",
        "**I/O structure**: The input data will have three dimensions: 1-documents/sentences, 2-token, 3-features; for the output there are 2 possibilities: if you use one-hot encoding it will be 1-documents/sentences, 2-token labels, 3-classes, if you use a single integer that indicates the number of the class it will be 1-documents/sentences, 2-token labels.\n",
        "\n",
        "**Baseline**: two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top; the choice of hyper-parameters is yours.\n",
        "\n",
        "**Architectures**: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and adding an additional dense layer; do not mix these variantions.\n",
        "\n",
        "\n",
        "**Training and Experiments**: all the experiments must involve only the training and validation sets.\n",
        "\n",
        "**Evaluation**: in the end, only the two best models of your choice (according to the validation set) must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech. DO NOT CONSIDER THE PUNCTUATION CLASSES.\n",
        "\n",
        "**Metrics**: the metric you must use to evaluate your final model is the F1-macro, WITHOUT considering punctuation/symbols classes; during the training process you can use accuracy because you can't use the F1 metric unless you use a single (gigantic) batch because there is no way to aggregate \"partial\" F1 scores computed on mini-batches.\n",
        "\n",
        "**Discussion and Error Analysis** : verify and discuss if the results on the test sets are coherent with those on the validation set; analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
        "\n",
        "**Report**: you are asked to deliver the code of your experiments and a small pdf report of about 2 pages; the pdf must begin with the names of the people of your team and a small abstract (4-5 lines) that sums up your findings.\n",
        "\n",
        "# Out Of Vocabulary (OOV) terms\n",
        "\n",
        "How to handle words that are not in GloVe vocabulary?\n",
        "You can handle them as you want (random embedding, placeholder, whatever!), but they must be STATIC embeddings (you cannot train them).\n",
        "\n",
        "But there is a very important caveat! As usual, the element of the test set must not influence the elements of the other splits!\n",
        "\n",
        "So, when you compute new embeddings for train+validation, you must forget about test documents.\n",
        "The motivation is to emulate a real-world scenario, where you select and train a model in the first stage, without knowing nothing about the testing environment.\n",
        "\n",
        "For implementation convenience, you CAN use a single vocabulary file/matrix/whatever. The principle of the previous point is that the embeddings inside that file/matrix must be generated independently for train and test splits.\n",
        "\n",
        "Basically in a real-world scenario, this is what would happen:\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Training of the model(s)\n",
        "5. Compute embeddings for terms OOV2 of the validation split \n",
        "6. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "7. Validation of the model(s)\n",
        "8. Compute embeddings for terms OOV3 of the test split \n",
        "9. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2+OOV3\n",
        "10. Testing of the final model\n",
        "\n",
        "In this case, where we already have all the documents, we can simplify the process a bit, but the procedure must remain rigorous.\n",
        "\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Compute embeddings for terms OOV2 of the validation split \n",
        "5. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "6. Compute embeddings for terms OOV3 of the test split \n",
        "7. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2\n",
        "8. Training of the model(s)\n",
        "9. Validation of the model(s)\n",
        "10. Testing of the final model\n",
        "\n",
        "Step 2 and step 6 must be completely independent of each other, for what concerns the method and the documents. But they can rely on the previous vocabulary (V1 for step 2 and V3 for step 6)\n",
        "THEREFORE if a word is present both in the training set and the test split and not in the starting vocabulary, its embedding is computed in step 2) and it is not considered OOV anymore in step 6).\n",
        "\n",
        "# Report\n",
        "The report must not be just a copy and paste of graphs and tables!\n",
        "\n",
        "The report must not be longer than 2 pages and must contain:\n",
        "* The names of the member of your team\n",
        "* A short abstract (4-5 lines) that sum ups everything\n",
        "* A general description of the task you have addressed and how you have addressed it\n",
        "* A short description of the models you have used\n",
        "* Some tables that sum up your findings in validation and test and a discussion of those results\n",
        "* The most relevant findings of your error analysis\n",
        "\n",
        "# Evaluation Criterion\n",
        "\n",
        "The goal of this assignment is not to prove you can find best model ever, but to face a common task, structure it correctly, and follow a correct and rigorous experimental procedure.\n",
        "In other words, we don't care if you final models are awful as long as you have followed the correct procedure and wrote a decent report.\n",
        "\n",
        "The score of the assignment will be computed roughly as follows\n",
        "* 1 point for the general setting of the problem\n",
        "* 1 point for the handling of OOV terms\n",
        "* 1 point for the models\n",
        "* 1 point for train-validation-test procedure\n",
        "* 2 point for the discussion of the results, error analysis, and report\n",
        "\n",
        "This distribution of scores is tentative and we may decide to alter it at any moment.\n",
        "We also reserve the right to assign a small bonus (0.5 points) to any assignment that is particularly worthy. Similarly, in case of grave errors, we may decide to assign an equivalent malus (-0.5 points).\n",
        "\n",
        "# Contacts\n",
        "\n",
        "In case of any doubt, question, issue, or help we highly recommend you to check the [course useful material](https://virtuale.unibo.it/pluginfile.php/1036039/mod_resource/content/2/NLP_Course_Useful_Material.pdf) for additional information, and to use the Virtuale forums to discuss with other students.\n",
        "\n",
        "You can always contact us at the following email addresses. To increase the probability of a prompt response, we reccomend you to write to both the teaching assistants.\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Andrea Galassi -> a.galassi@unibo.it\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it\n",
        "\n",
        "\n",
        "# FAQ\n",
        "* You can use a non-trainable Embedding layer to load the glove embeddings\n",
        "* You can use any library of your choice to implement the networks. Two options are tensorflow/keras or pythorch. Both these libraries have all the classes you need to implement these simple architectures and there are plenty of tutorials around, where you can learn how to use them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emdubW66fnlD"
      },
      "source": [
        "import os, shutil  #  file management\n",
        "import sys \n",
        "import pandas as pd  #  dataframe management\n",
        "import numpy as np  #  data manipulation\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request  #  download files\n",
        "import zipfile  #  unzip files\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B-w2pqyfpF1"
      },
      "source": [
        "from keras import metrics \n",
        "from tensorflow.python.keras.metrics import Metric\n",
        "from sklearn.metrics import f1_score\n",
        "import keras.backend as K"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysrb-wBHr6RJ",
        "outputId": "c8fb7b32-95e5-4d60-da21-f7a92e9ac48f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3xn1MZPgA_v",
        "outputId": "a0313cd5-dd87-4fd8-adeb-b16e5344a64c"
      },
      "source": [
        "dataset_folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\")\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "    os.makedirs(dataset_folder)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
        "\n",
        "dataset_path = os.path.join(dataset_folder, \"treebank.zip\")\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    urllib.request.urlretrieve(url, dataset_path)\n",
        "    print(\"Successful download\")\n",
        "\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_folder)\n",
        "print(\"Successful extraction\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful download\n",
            "Successful extraction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre Processing\n",
        "- splitting"
      ],
      "metadata": {
        "id": "b35jQn8Ar8lu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKbUsrVHhGAG"
      },
      "source": [
        "dataset_name = \"dependency_treebank\"\n",
        "\n",
        "\n",
        "folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\", dataset_name)\n",
        "\n",
        "pre_train = []\n",
        "pre_valid = []\n",
        "pre_test = []\n",
        "i = 1\n",
        "\n",
        "file_list = sorted(os.listdir(folder))\n",
        "\n",
        "for filename in file_list:\n",
        "  file_path = os.path.join(folder, filename)\n",
        "  if os.path.isfile(file_path):\n",
        "    # open the file\n",
        "      text = []\n",
        "      with open(file_path, mode='r', encoding='utf-8') as text_file:\n",
        "        text = text_file.read()\n",
        "        if i <= 100:\n",
        "          pre_train.append(text)\n",
        "        elif i <= 150:\n",
        "          pre_valid.append(text)\n",
        "        else:\n",
        "          pre_test.append(text)  \n",
        "  i+=1\n",
        "\n",
        "tr = []\n",
        "val = []\n",
        "tes = []\n",
        "\n",
        "for paragraph in pre_train:\n",
        "   tr.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_valid:\n",
        "   val.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_test:\n",
        "   tes.append(paragraph.split('\\n'))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUgbTXi-F7Sa"
      },
      "source": [
        "train = []\n",
        "valid = []\n",
        "test = []\n",
        "\n",
        "for i in tr:\n",
        "  for j in i:\n",
        "    train.append(j.split('\\t'))\n",
        "\n",
        "for i in val:\n",
        "  for j in i:\n",
        "    valid.append(j.split('\\t'))\n",
        "\n",
        "for i in tes:\n",
        "  for j in i:\n",
        "    test.append(j.split('\\t'))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmzVaRuQFp0M"
      },
      "source": [
        "train_sentences = []\n",
        "valid_sentences = []\n",
        "test_sentences = []\n",
        "train_tags = []\n",
        "valid_tags = []\n",
        "test_tags = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in train:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    train_sentences.append(s)\n",
        "    train_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in valid:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    valid_sentences.append(s)\n",
        "    valid_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in test:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    test_sentences.append(s)\n",
        "    test_tags.append(t)\n",
        "    s = []\n",
        "    t = []"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB65cUFLIeEr"
      },
      "source": [
        "flat_train = [item for sublist in train_sentences for item in sublist]\n",
        "flat_valid = [item for sublist in valid_sentences for item in sublist]\n",
        "flat_test = [item for sublist in test_sentences for item in sublist]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "- vocabularies\n",
        "- GloVe download"
      ],
      "metadata": {
        "id": "K7P8MXGEr_37"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZO6ga1EGHdh"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "train_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "train_tokenizer.fit_on_texts(train_sentences)                    # fit tokeniser on data\n",
        "train_encoded = train_tokenizer.texts_to_sequences(train_sentences)\n",
        "\n",
        "valid_tokenizer = Tokenizer()       \n",
        "valid_tokenizer.fit_on_texts(valid_sentences)                  # instantiate tokeniser\n",
        "valid_encoded = valid_tokenizer.texts_to_sequences(valid_sentences)\n",
        "\n",
        "test_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "test_tokenizer.fit_on_texts(test_sentences)                    # fit tokeniser on data\n",
        "test_encoded = test_tokenizer.texts_to_sequences(test_sentences)  # use the tokeniser to encode input sequence"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(train_tags)\n",
        "tags_encoded = tag_tokenizer.texts_to_sequences(train_tags)"
      ],
      "metadata": {
        "id": "Bo2p8VbQsHAl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMsG0Yw1l50L",
        "outputId": "7740fe22-6718-4865-bc8e-35d42ead1e95"
      },
      "source": [
        "voc = list(set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(valid_tokenizer.word_index.keys()) - set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(test_tokenizer.word_index.keys()) - set(voc))\n",
        "print(len(voc))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7404\n",
            "9901\n",
            "10947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS9RwnOtJuas"
      },
      "source": [
        "word_index = dict(zip(voc, range(2, len(voc)+3)))\n",
        "word_index['-PAD-'] = 0\n",
        "word_index['-OOV-'] = 1\n",
        "\n",
        "tags = set([item for sublist in train_tags for item in sublist])\n",
        "\n",
        "tag2index = {t: i + 1 for i, t in enumerate(tags)}\n",
        "tag2index['-PAD-'] = 0 "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoj4Zx51J2MU",
        "outputId": "3944c177-90e1-41bc-d131-c25340a0e516"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-15 09:01:52--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-12-15 09:01:54--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-12-15 09:01:54--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.08MB/s    in 2m 45s  \n",
            "\n",
            "2021-12-15 09:04:39 (4.98 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc-T6FcLKtS9",
        "outputId": "ec1875b7-d0f2-41f1-99b3-d3cf0cc69492"
      },
      "source": [
        "path_to_glove_file = os.path.join(os.getcwd(), \"glove.6B.100d.txt\")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding\n",
        "- matrix creation\n",
        "- padding\n",
        "- one hot encoding of tags\n",
        "- punctuation identification"
      ],
      "metadata": {
        "id": "ulxMYBgGsN6C"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0mOStEPMeXt",
        "outputId": "cf66f9d2-ee54-4e8d-f2e0-fb1fd974c119"
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        embedding_matrix += np.random.uniform(low=-0.05, high=0.05, size=embedding_dim)\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 10271 words (678 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpYrHs0RMtQJ"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8EZM31Js06-",
        "outputId": "04fb9b00-3b3e-4080-ffd5-d1a8b9c2c684"
      },
      "source": [
        "MAX_LENGTH = len(max(train_sentences, key=len))\n",
        "print(MAX_LENGTH)\n",
        "MAX_LENGTH2 = len(max(valid_sentences, key=len))\n",
        "print(MAX_LENGTH2)\n",
        "MAX_LENGTH3 = len(max(test_sentences, key=len))\n",
        "print(MAX_LENGTH3)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "249\n",
            "81\n",
            "58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSRa9SwTuNrX"
      },
      "source": [
        "train_sentences_X, valid_sentences_X, test_sentences_X, train_tags_y, valid_tags_y, test_tags_y = [], [], [], [], [], []\n",
        " \n",
        "for s in train_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        "\n",
        "for s in valid_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    valid_sentences_X.append(s_int)\n",
        "\n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        " \n",
        "for s in train_tags:\n",
        "    train_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "for s in valid_tags:\n",
        "    valid_tags_y.append([tag2index[t] for t in s])\n",
        " \n",
        "for s in test_tags:\n",
        "    test_tags_y.append([tag2index[t] for t in s])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf-XlSrStQx3"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_sentences_X = pad_sequences(valid_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_tags_y = pad_sequences(valid_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)"
      ],
      "metadata": {
        "id": "haU2V9_OsaT4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "point = [tag2index['.']]\n",
        "virg = [tag2index[',']]\n",
        "weird_apex = [tag2index['``']]\n",
        "single_apex = [tag2index[\"''\"]]\n",
        "two_dots = [tag2index[':']]\n",
        "pad = [tag2index['-PAD-']]\n",
        "\n",
        "punct_cat_classes = to_categorical([point, virg, weird_apex, single_apex, two_dots, pad], len(tag2index))\n",
        "punct_cat_classes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1fP60F5sbxY",
        "outputId": "d41f33e1-8042-430e-d5b3-047fd81cc3cc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 1, 46)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cum_tags = np.zeros(46)\n",
        "for i in punct_cat_classes:\n",
        "  print(i[0])\n",
        "  cum_tags += i[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D470uljIse7C",
        "outputId": "61507ae5-f40d-4c33-c5b3-9430b181fdd8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "where_tags = np.where(np.logical_not(cum_tags))\n",
        "where_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-_xiWoUsgkQ",
        "outputId": "dd97bde1-4e9a-4bf9-8498-ca56d041329b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 12, 13, 14, 15, 17, 18, 19,\n",
              "        20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 35, 36, 37, 38,\n",
              "        39, 40, 41, 42, 43, 45]),)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_punct_indexes = where_tags[0]\n",
        "n_classes = len(no_punct_indexes)"
      ],
      "metadata": {
        "id": "7gYzdFn6si5W"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "cat_val_tags_y = to_categorical(valid_tags_y, len(tag2index))\n",
        "print(len(cat_train_tags_y), len(cat_val_tags_y))\n",
        "print(len(train_sentences_X), len(valid_sentences_X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPNGFTrfsmLS",
        "outputId": "685ad425-79cf-4f85-b81e-4467152305a5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1963 1299\n",
            "1963 1299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mini Data Vizualization Inset"
      ],
      "metadata": {
        "id": "WN8YXAJKsnkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_tags = np.zeros(46)\n",
        "for i in cat_train_tags_y:\n",
        "  for t in i:\n",
        "    distribution_tags += t"
      ],
      "metadata": {
        "id": "QZtQDYMPsqSV"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = sum(distribution_tags[1:])\n",
        "norm = [float(i)/s for i in distribution_tags[1:]]"
      ],
      "metadata": {
        "id": "o9dXfc6hss5V"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt "
      ],
      "metadata": {
        "id": "RXVJYmoIswcb"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(np.arange(len(norm)),norm)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "XkDh9ZE-sxoZ",
        "outputId": "d41474fb-02ca-4c88-f6da-5cac286bee7d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAARMElEQVR4nO3df6xfdX3H8edrRdBpBIU7oy2sNdQtZTo2a3GZcwYiK8NRlxUpuokLS7fEZi5qXN0SxM4lsCziEvnDRtgQ5oCwud2MuoaJiYtB7AUVVhjzgihFJuWHOGYQC+/98T2N3365cE+5t/fefr7PR3Jzz/mcz/ne9/fTe1/fT8/3nPNNVSFJatdPLXYBkqRDy6CXpMYZ9JLUOINekhpn0EtS445Y7AJGHXfccbVy5crFLkOSDiu33HLLQ1U1MdO2JRf0K1euZGpqarHLkKTDSpJvP9s2D91IUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjltyVsZK038qt18/Yfu9FZy5wJYc3Z/SS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1rlfQJ1mf5K4k00m2zrD9zUluTbIvycah9pOT3JRkd5Lbkpwzn8VLkmY3a9AnWQZcCpwBrAHOTbJmpNt3gPcAnx1p/yHw7qo6CVgPfCLJMXMtWpLUX59PmFoHTFfVPQBJrgY2AHfs71BV93bbnh7esar+e2j5u0keBCaA78+5cklSL30O3SwH7hta39O1HZQk64Ajgbtn2LY5yVSSqb179x7sQ0uSnsOCvBmb5JXAlcDvV9XTo9urantVra2qtRMTEwtRkiSNjT5Bfz9w/ND6iq6tlyQvBa4H/ryqvnJw5UmS5qpP0O8CVidZleRIYBMw2efBu/6fAz5TVdc9/zIlSc/XrEFfVfuALcBO4E7g2qranWRbkrMAkrwhyR7gbOBTSXZ3u78DeDPwniRf775OPiTPRJI0oz5n3VBVO4AdI20XDC3vYnBIZ3S/q4Cr5lijJGkOvDJWkhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuN63QJBGmcrt14/Y/u9F525wJVIz48zeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGud59JI0BzNdZ7HUrrFwRi9JjTPoJalxBr0kNa5X0CdZn+SuJNNJts6w/c1Jbk2yL8nGkW3nJflm93XefBUuSepn1qBPsgy4FDgDWAOcm2TNSLfvAO8BPjuy78uBjwCnAOuAjyR52dzLliT11WdGvw6Yrqp7qupJ4Gpgw3CHqrq3qm4Dnh7Z9zeAG6rqkap6FLgBWD8PdUuSeuoT9MuB+4bW93RtffTaN8nmJFNJpvbu3dvzoSVJfSyJN2OrantVra2qtRMTE4tdjiQ1pU/Q3w8cP7S+omvrYy77SpLmQZ+g3wWsTrIqyZHAJmCy5+PvBE5P8rLuTdjTuzZJ0gKZNeirah+whUFA3wlcW1W7k2xLchZAkjck2QOcDXwqye5u30eAv2DwYrEL2Na1SZIWSK973VTVDmDHSNsFQ8u7GByWmWnfy4HL51CjJGkOlsSbsZKkQ8egl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXG9PnhE0sFbufX6Z7Tde9GZi1CJxp0zeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktS4XkGfZH2Su5JMJ9k6w/ajklzTbb85ycqu/QVJrkhye5I7k3x4fsuXJM1m1qBPsgy4FDgDWAOcm2TNSLfzgUer6kTgEuDirv1s4Kiqei3weuAP978ISJIWRp8Z/TpguqruqaongauBDSN9NgBXdMvXAaclCVDAi5McAbwIeBL4wbxULknqpU/QLwfuG1rf07XN2Keq9gGPAccyCP3/Ax4AvgP8dVU9MvoDkmxOMpVkau/evQf9JCRJz+5Qvxm7DngKeBWwCvhAklePdqqq7VW1tqrWTkxMHOKSJGm89An6+4Hjh9ZXdG0z9ukO0xwNPAy8E/i3qvpxVT0IfBlYO9eiJUn99Qn6XcDqJKuSHAlsAiZH+kwC53XLG4Ebq6oYHK45FSDJi4E3Av81H4VLkvqZNei7Y+5bgJ3AncC1VbU7ybYkZ3XdLgOOTTINvB/YfwrmpcBLkuxm8ILxt1V123w/CUnSs+t1m+Kq2gHsGGm7YGj5CQanUo7u9/hM7ZKkheOVsZLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJalyvDwfXzFZuvf4ZbfdedOYiVCJpmH+bB3JGL0mN6xX0SdYnuSvJdJKtM2w/Ksk13fabk6wc2va6JDcl2Z3k9iQvnL/yJUmzmTXokywDLgXOANYA5yZZM9LtfODRqjoRuAS4uNv3COAq4I+q6iTgLcCP5616SdKs+szo1wHTVXVPVT0JXA1sGOmzAbiiW74OOC1JgNOB26rqGwBV9XBVPTU/pUuS+ugT9MuB+4bW93RtM/apqn3AY8CxwGuASrIzya1JPjTTD0iyOclUkqm9e/ce7HOQJD2HQ/1m7BHAm4B3dd9/O8lpo52qantVra2qtRMTE4e4JEkaL32C/n7g+KH1FV3bjH264/JHAw8zmP1/qaoeqqofAjuAX55r0ZKk/voE/S5gdZJVSY4ENgGTI30mgfO65Y3AjVVVwE7gtUl+unsB+HXgjvkpXZLUx6wXTFXVviRbGIT2MuDyqtqdZBswVVWTwGXAlUmmgUcYvBhQVY8m+TiDF4sCdlTVM69kkCQdMr2ujK2qHQwOuwy3XTC0/ARw9rPsexWDUywlSYvAK2MlqXEGvSQ1zqCXpMZ590pJ88a7Ri5NzuglqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDWuV9AnWZ/kriTTSbbOsP2oJNd0229OsnJk+wlJHk/ywfkpW5LU16yfGZtkGXAp8FZgD7AryWRV3THU7Xzg0ao6Mckm4GLgnKHtHwc+P39l63DjZ4lKi6fPjH4dMF1V91TVk8DVwIaRPhuAK7rl64DTkgQgyduBbwG756dkSdLB6BP0y4H7htb3dG0z9qmqfcBjwLFJXgL8KfDR5/oBSTYnmUoytXfv3r61S5J6ONRvxl4IXFJVjz9Xp6raXlVrq2rtxMTEIS5JksbLrMfogfuB44fWV3RtM/XZk+QI4GjgYeAUYGOSvwKOAZ5O8kRVfXLOlUuSeukT9LuA1UlWMQj0TcA7R/pMAucBNwEbgRurqoBf298hyYXA44a8JC2sWYO+qvYl2QLsBJYBl1fV7iTbgKmqmgQuA65MMg08wuDFQJK0BPSZ0VNVO4AdI20XDC0/AZw9y2Nc+DzqkyTNkVfGSlLjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMb1Or1SaoV30dQ4ckYvSY1zRj8LZ4CSDndjE/QzBTYY2pLa56EbSWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaNzbn0T8Xz7GX1DJn9JLUOGf0WrL8n5ZatpC3V3FGL0mNM+glqXG9gj7J+iR3JZlOsnWG7UcluabbfnOSlV37W5PckuT27vup81u+JGk2sx6jT7IMuBR4K7AH2JVksqruGOp2PvBoVZ2YZBNwMXAO8BDwW1X13SS/AOwEls/3kxh33kr58OO/mRZSnxn9OmC6qu6pqieBq4ENI302AFd0y9cBpyVJVX2tqr7bte8GXpTkqPkoXJLUT5+zbpYD9w2t7wFOebY+VbUvyWPAsQxm9Pv9DnBrVf3o+Zcr6XDl/2IWz4KcXpnkJAaHc05/lu2bgc0AJ5xwwkKUJEljo8+hm/uB44fWV3RtM/ZJcgRwNPBwt74C+Bzw7qq6e6YfUFXbq2ptVa2dmJg4uGcgSXpOfYJ+F7A6yaokRwKbgMmRPpPAed3yRuDGqqokxwDXA1ur6svzVbQkqb9Zg76q9gFbGJwxcydwbVXtTrItyVldt8uAY5NMA+8H9p+CuQU4Ebggyde7r5+Z92chSXpWvY7RV9UOYMdI2wVDy08AZ8+w38eAj82xRknSHHhlrCQ1zpuaaVF54zLp0DPo9QyGr9QWD91IUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjvDJW88ZPEJo7r0o+9Bby93Sp/E0Y9FIDlkqgaGny0I0kNc4ZvQ6KM0fp8GPQN85gluShG0lqnEEvSY3z0I2kw5KHJftzRi9JjXNGr8PSUpnNLZU6NHctX6zmjF6SGtdrRp9kPfA3wDLg01V10cj2o4DPAK8HHgbOqap7u20fBs4HngL+uKp2zlv1Y8SZY3+tjtVSmXEulTrU36xBn2QZcCnwVmAPsCvJZFXdMdTtfODRqjoxySbgYuCcJGuATcBJwKuAf0/ymqp6ar6fiKSZLfUXvsPhheNwqPG59JnRrwOmq+oegCRXAxuA4aDfAFzYLV8HfDJJuvarq+pHwLeSTHePd9P8lK/n63D/xdXc+TswPlJVz90h2Qisr6o/6NZ/DzilqrYM9fnPrs+ebv1u4BQG4f+Vqrqqa78M+HxVXTfyMzYDm7vVnwPumvtT4zjgoXl4nFY4HgdyPA7keBzocByPn62qiZk2LImzbqpqO7B9Ph8zyVRVrZ3PxzycOR4HcjwO5HgcqLXx6HPWzf3A8UPrK7q2GfskOQI4msGbsn32lSQdQn2CfhewOsmqJEcyeHN1cqTPJHBet7wRuLEGx4QmgU1JjkqyClgNfHV+Spck9THroZuq2pdkC7CTwemVl1fV7iTbgKmqmgQuA67s3mx9hMGLAV2/axm8cbsPeO8CnnEzr4eCGuB4HMjxOJDjcaCmxmPWN2MlSYc3r4yVpMYZ9JLUuCaDPsn6JHclmU6ydbHrWWhJLk/yYHd9w/62lye5Ick3u+8vW8waF1KS45N8MckdSXYneV/XPpZjkuSFSb6a5BvdeHy0a1+V5Obu7+aa7uSLsZBkWZKvJfnXbr2psWgu6Idu2XAGsAY4t7sVwzj5O2D9SNtW4AtVtRr4Qrc+LvYBH6iqNcAbgfd2vxPjOiY/Ak6tql8ETgbWJ3kjg1uXXFJVJwKPMri1ybh4H3Dn0HpTY9Fc0DN0y4aqehLYf8uGsVFVX2Jw9tOwDcAV3fIVwNsXtKhFVFUPVNWt3fL/MviDXs6YjkkNPN6tvqD7KuBUBrcwgTEajyQrgDOBT3frobGxaDHolwP3Da3v6drG3Suq6oFu+X+AVyxmMYslyUrgl4CbGeMx6Q5VfB14ELgBuBv4flXt67qM09/NJ4APAU9368fS2Fi0GPSaRXcx29idV5vkJcA/An9SVT8Y3jZuY1JVT1XVyQyuVl8H/Pwil7QokrwNeLCqblnsWg6lJXGvm3nmbRdm9r0kr6yqB5K8ksFMbmwkeQGDkP/7qvqnrnmsxwSgqr6f5IvArwDHJDmim8mOy9/NrwJnJflN4IXASxl89kZTY9HijL7PLRvG0fBtKs4D/mURa1lQ3THXy4A7q+rjQ5vGckySTCQ5plt+EYPPmrgT+CKDW5jAmIxHVX24qlZU1UoGWXFjVb2LxsaiyStju1fnT/CTWzb85SKXtKCS/APwFga3Wv0e8BHgn4FrgROAbwPvqKrRN2yblORNwH8At/OT47B/xuA4/diNSZLXMXiDcRmDyd61VbUtyasZnLzwcuBrwO92nyUxFpK8BfhgVb2ttbFoMuglST/R4qEbSdIQg16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ17v8B3U4lq5UwYIYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#F1 Metric"
      ],
      "metadata": {
        "id": "_bPXjdl5szXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric.\n",
        "\n",
        "    Only computes a batch-wise average of precision.\n",
        "\n",
        "    Computes the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric.\n",
        "\n",
        "    Only computes a batch-wise average of recall.\n",
        "\n",
        "    Computes the recall, a metric for multi-label classification of\n",
        "    how many relevant items are selected.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def f1_binary(y_true, y_pred):\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
        "\n",
        "def metric_wrapper(index):\n",
        "  def f1_for_class(y_true, y_pred):\n",
        "    #get only the desired class\n",
        "    true = y_true[:,:,index]\n",
        "    pred = y_pred[:,:,index]\n",
        "    #return dice per class\n",
        "    tmp = f1_binary(true,pred)\n",
        "    return tmp\n",
        "  f1_for_class.__name__ = 'f1_' + str(index)\n",
        "  return f1_for_class\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    result = 0.0\n",
        "    for class_id in no_punct_indexes:\n",
        "        y_true_single_class = y_true[:,:,class_id]\n",
        "        y_pred_single_class = y_pred[:,:,class_id]\n",
        "        f1_single = f1_binary(y_true_single_class, y_pred_single_class)\n",
        "        result += f1_single / float(n_classes)\n",
        "    return result"
      ],
      "metadata": {
        "id": "kL2FW8K7DSdy"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model compile and fit"
      ],
      "metadata": {
        "id": "uwvKLBBos8eS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsVGByW_sdec",
        "outputId": "6a9daeb1-1d01-4292-d1c4-4781ed0d2c7c"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(embedding_layer)\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(Dense(len(tag2index)))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=Adam(0.001),\n",
        "             metrics=['accuracy', f1, [metric_wrapper(i) for i in no_punct_indexes]])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 249, 100)          1094900   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 249, 512)         731136    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 249, 512)         1574912   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense (Dense)               (None, 249, 46)           23598     \n",
            "                                                                 \n",
            " activation (Activation)     (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,424,546\n",
            "Trainable params: 2,329,646\n",
            "Non-trainable params: 1,094,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "history = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=40, validation_data=(valid_sentences_X, cat_val_tags_y), callbacks=[callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlP9hYnOtFxK",
        "outputId": "886ddf32-9fd7-41e1-c661-e74f0344f6d2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "16/16 [==============================] - 246s 14s/step - loss: 0.6600 - accuracy: 0.8497 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.3079 - val_accuracy: 0.9156 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - 221s 14s/step - loss: 0.2949 - accuracy: 0.9173 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2849 - val_accuracy: 0.9178 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - 220s 14s/step - loss: 0.2794 - accuracy: 0.9210 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2757 - val_accuracy: 0.9234 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - 222s 14s/step - loss: 0.2691 - accuracy: 0.9248 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2648 - val_accuracy: 0.9266 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - 227s 14s/step - loss: 0.2559 - accuracy: 0.9280 - f1: 0.0027 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.1080 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2490 - val_accuracy: 0.9302 - val_f1: 0.0013 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0521 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - 219s 14s/step - loss: 0.2366 - accuracy: 0.9342 - f1: 0.0067 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.2694 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2262 - val_accuracy: 0.9391 - val_f1: 0.0069 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.2738 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 3.7258e-04 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - 222s 14s/step - loss: 0.2093 - accuracy: 0.9440 - f1: 0.0133 - f1_1: 0.0000e+00 - f1_2: 0.0305 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.4208 - f1_24: 0.0000e+00 - f1_25: 0.0522 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0108 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0194 - f1_45: 0.0000e+00 - val_loss: 0.1957 - val_accuracy: 0.9491 - val_f1: 0.0225 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0836 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.4557 - val_f1_24: 0.0000e+00 - val_f1_25: 0.2052 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0527 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.1018 - val_f1_45: 0.0000e+00\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - 223s 14s/step - loss: 0.1757 - accuracy: 0.9535 - f1: 0.0499 - f1_1: 0.0000e+00 - f1_2: 0.1852 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0243 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.5681 - f1_24: 0.0000e+00 - f1_25: 0.5969 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.1623 - f1_38: 0.0000e+00 - f1_39: 0.0145 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.4466 - f1_45: 0.0000e+00 - val_loss: 0.1621 - val_accuracy: 0.9575 - val_f1: 0.0693 - val_f1_1: 0.0000e+00 - val_f1_2: 0.3387 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 9.6200e-04 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.1676 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.5333 - val_f1_24: 0.0000e+00 - val_f1_25: 0.8567 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.2387 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0762 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.5584 - val_f1_45: 0.0000e+00\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - 222s 14s/step - loss: 0.1451 - accuracy: 0.9610 - f1: 0.0964 - f1_1: 0.0000e+00 - f1_2: 0.3705 - f1_3: 0.0646 - f1_4: 0.0000e+00 - f1_5: 0.0392 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.5128 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0342 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0013 - f1_22: 0.0000e+00 - f1_23: 0.6452 - f1_24: 0.0000e+00 - f1_25: 0.8872 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.3752 - f1_38: 0.0000e+00 - f1_39: 0.1767 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.7448 - f1_45: 0.0045 - val_loss: 0.1372 - val_accuracy: 0.9615 - val_f1: 0.1190 - val_f1_1: 0.0000e+00 - val_f1_2: 0.5175 - val_f1_3: 0.2227 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0888 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9157 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0492 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0281 - val_f1_22: 0.0000e+00 - val_f1_23: 0.6322 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9060 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.5604 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0584 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.7820 - val_f1_45: 0.0000e+00\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - 227s 14s/step - loss: 0.1229 - accuracy: 0.9669 - f1: 0.1509 - f1_1: 0.0000e+00 - f1_2: 0.5079 - f1_3: 0.3869 - f1_4: 0.0000e+00 - f1_5: 0.1913 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9384 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.1906 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.4455 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0781 - f1_22: 0.0000e+00 - f1_23: 0.7017 - f1_24: 0.0000e+00 - f1_25: 0.9092 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.0076 - f1_36: 0.0000e+00 - f1_37: 0.4974 - f1_38: 0.0000e+00 - f1_39: 0.3270 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.8456 - f1_45: 0.0080 - val_loss: 0.1177 - val_accuracy: 0.9679 - val_f1: 0.1699 - val_f1_1: 0.0000e+00 - val_f1_2: 0.5639 - val_f1_3: 0.4597 - val_f1_4: 0.0000e+00 - val_f1_5: 0.2445 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9768 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.3953 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.5885 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0940 - val_f1_22: 0.0000e+00 - val_f1_23: 0.6853 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9202 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.0312 - val_f1_36: 0.0000e+00 - val_f1_37: 0.6008 - val_f1_38: 0.0000e+00 - val_f1_39: 0.3532 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.8424 - val_f1_45: 0.0409\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - 230s 14s/step - loss: 0.1061 - accuracy: 0.9716 - f1: 0.2059 - f1_1: 0.0000e+00 - f1_2: 0.5955 - f1_3: 0.6252 - f1_4: 0.0000e+00 - f1_5: 0.3319 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9921 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.6718 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.7402 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.2037 - f1_22: 0.0000e+00 - f1_23: 0.7362 - f1_24: 0.0000e+00 - f1_25: 0.9239 - f1_26: 0.0000e+00 - f1_27: 0.1517 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.1463 - f1_36: 0.0000e+00 - f1_37: 0.6121 - f1_38: 0.0000e+00 - f1_39: 0.5092 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.8740 - f1_45: 0.1210 - val_loss: 0.1041 - val_accuracy: 0.9713 - val_f1: 0.2220 - val_f1_1: 0.0000e+00 - val_f1_2: 0.6530 - val_f1_3: 0.5911 - val_f1_4: 0.0000e+00 - val_f1_5: 0.2539 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9952 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.7911 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.7776 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.3182 - val_f1_22: 0.0000e+00 - val_f1_23: 0.7184 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9328 - val_f1_26: 0.0027 - val_f1_27: 0.4387 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.1699 - val_f1_36: 0.0000e+00 - val_f1_37: 0.6429 - val_f1_38: 0.0000e+00 - val_f1_39: 0.4644 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.8773 - val_f1_45: 0.2508\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - 231s 15s/step - loss: 0.0937 - accuracy: 0.9750 - f1: 0.2535 - f1_1: 0.0000e+00 - f1_2: 0.6714 - f1_3: 0.6854 - f1_4: 0.0045 - f1_5: 0.4001 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9962 - f1_12: 0.0000e+00 - f1_13: 0.0171 - f1_14: 0.8874 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.8208 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.3739 - f1_22: 0.0000e+00 - f1_23: 0.7599 - f1_24: 0.0000e+00 - f1_25: 0.9339 - f1_26: 0.0345 - f1_27: 0.6471 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.3414 - f1_36: 0.0000e+00 - f1_37: 0.6573 - f1_38: 0.0000e+00 - f1_39: 0.6101 - f1_40: 0.0130 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.8888 - f1_45: 0.3968 - val_loss: 0.0945 - val_accuracy: 0.9739 - val_f1: 0.2730 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7086 - val_f1_3: 0.7460 - val_f1_4: 0.0036 - val_f1_5: 0.3801 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9987 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0905 - val_f1_14: 0.9400 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.7853 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.4413 - val_f1_22: 0.0000e+00 - val_f1_23: 0.7454 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9394 - val_f1_26: 0.0710 - val_f1_27: 0.8760 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.5659 - val_f1_36: 0.0000e+00 - val_f1_37: 0.5834 - val_f1_38: 0.0000e+00 - val_f1_39: 0.6101 - val_f1_40: 0.0107 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.8859 - val_f1_45: 0.5368\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - 233s 15s/step - loss: 0.0839 - accuracy: 0.9776 - f1: 0.2962 - f1_1: 0.0000e+00 - f1_2: 0.7234 - f1_3: 0.7446 - f1_4: 0.0389 - f1_5: 0.4718 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9985 - f1_12: 0.0000e+00 - f1_13: 0.1869 - f1_14: 0.9806 - f1_15: 0.0038 - f1_17: 0.0000e+00 - f1_18: 0.8374 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.5089 - f1_22: 0.0000e+00 - f1_23: 0.7815 - f1_24: 0.0000e+00 - f1_25: 0.9422 - f1_26: 0.1414 - f1_27: 0.8856 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.5805 - f1_36: 0.0000e+00 - f1_37: 0.6994 - f1_38: 0.0000e+00 - f1_39: 0.6654 - f1_40: 0.1342 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.9034 - f1_45: 0.6181 - val_loss: 0.0852 - val_accuracy: 0.9769 - val_f1: 0.3091 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7318 - val_f1_3: 0.7352 - val_f1_4: 0.0352 - val_f1_5: 0.3866 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.4320 - val_f1_14: 0.9708 - val_f1_15: 0.0098 - val_f1_17: 0.0000e+00 - val_f1_18: 0.8205 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.5742 - val_f1_22: 0.0000e+00 - val_f1_23: 0.7388 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9457 - val_f1_26: 0.1831 - val_f1_27: 0.9225 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.7508 - val_f1_36: 0.0000e+00 - val_f1_37: 0.6930 - val_f1_38: 0.0000e+00 - val_f1_39: 0.6475 - val_f1_40: 0.3066 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.8994 - val_f1_45: 0.5811\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - 230s 14s/step - loss: 0.0759 - accuracy: 0.9797 - f1: 0.3360 - f1_1: 0.0000e+00 - f1_2: 0.7664 - f1_3: 0.7825 - f1_4: 0.1020 - f1_5: 0.5134 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9976 - f1_12: 0.0000e+00 - f1_13: 0.3500 - f1_14: 0.9925 - f1_15: 0.2097 - f1_17: 0.0062 - f1_18: 0.8572 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.6004 - f1_22: 0.0000e+00 - f1_23: 0.7910 - f1_24: 0.0000e+00 - f1_25: 0.9470 - f1_26: 0.2537 - f1_27: 0.9180 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.7277 - f1_36: 0.0000e+00 - f1_37: 0.7227 - f1_38: 0.0000e+00 - f1_39: 0.7150 - f1_40: 0.5565 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.9092 - f1_45: 0.7199 - val_loss: 0.0816 - val_accuracy: 0.9767 - val_f1: 0.3320 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7685 - val_f1_3: 0.7462 - val_f1_4: 0.0717 - val_f1_5: 0.2889 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.3064 - val_f1_14: 0.9975 - val_f1_15: 0.2817 - val_f1_17: 0.0000e+00 - val_f1_18: 0.8263 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.6620 - val_f1_22: 0.0000e+00 - val_f1_23: 0.7483 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9513 - val_f1_26: 0.1904 - val_f1_27: 0.9563 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.8141 - val_f1_36: 0.0000e+00 - val_f1_37: 0.7261 - val_f1_38: 0.0000e+00 - val_f1_39: 0.6318 - val_f1_40: 0.6150 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9070 - val_f1_45: 0.7888\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - 232s 15s/step - loss: 0.0696 - accuracy: 0.9812 - f1: 0.3663 - f1_1: 0.0000e+00 - f1_2: 0.7914 - f1_3: 0.8018 - f1_4: 0.1733 - f1_5: 0.5492 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9987 - f1_12: 0.0000e+00 - f1_13: 0.4120 - f1_14: 0.9947 - f1_15: 0.5137 - f1_17: 0.0166 - f1_18: 0.8760 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.6862 - f1_22: 0.0000e+00 - f1_23: 0.8024 - f1_24: 0.0000e+00 - f1_25: 0.9487 - f1_26: 0.3557 - f1_27: 0.9264 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.8210 - f1_36: 0.0000e+00 - f1_37: 0.7602 - f1_38: 0.0000e+00 - f1_39: 0.7528 - f1_40: 0.7413 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.9160 - f1_45: 0.8143 - val_loss: 0.0733 - val_accuracy: 0.9799 - val_f1: 0.3597 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7811 - val_f1_3: 0.7571 - val_f1_4: 0.2265 - val_f1_5: 0.5471 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.5975 - val_f1_14: 0.9975 - val_f1_15: 0.4340 - val_f1_17: 0.0062 - val_f1_18: 0.8467 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.6432 - val_f1_22: 0.0000e+00 - val_f1_23: 0.7707 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9530 - val_f1_26: 0.2759 - val_f1_27: 0.9563 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.8385 - val_f1_36: 0.0000e+00 - val_f1_37: 0.7129 - val_f1_38: 0.0000e+00 - val_f1_39: 0.6972 - val_f1_40: 0.6205 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9128 - val_f1_45: 0.8135\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - 231s 15s/step - loss: 0.0642 - accuracy: 0.9827 - f1: 0.3875 - f1_1: 0.0000e+00 - f1_2: 0.8176 - f1_3: 0.8119 - f1_4: 0.2715 - f1_5: 0.5915 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9986 - f1_12: 0.0000e+00 - f1_13: 0.5126 - f1_14: 0.9940 - f1_15: 0.6263 - f1_17: 0.0662 - f1_18: 0.8947 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.7214 - f1_22: 0.0000e+00 - f1_23: 0.8178 - f1_24: 0.0000e+00 - f1_25: 0.9502 - f1_26: 0.4408 - f1_27: 0.9406 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.8673 - f1_36: 0.0000e+00 - f1_37: 0.7694 - f1_38: 0.0200 - f1_39: 0.7778 - f1_40: 0.8452 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.9194 - f1_45: 0.8453 - val_loss: 0.0698 - val_accuracy: 0.9802 - val_f1: 0.3781 - val_f1_1: 0.0000e+00 - val_f1_2: 0.8068 - val_f1_3: 0.8111 - val_f1_4: 0.3286 - val_f1_5: 0.4599 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.5206 - val_f1_14: 0.9975 - val_f1_15: 0.6449 - val_f1_17: 0.0414 - val_f1_18: 0.8745 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.7137 - val_f1_22: 0.0000e+00 - val_f1_23: 0.7732 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9517 - val_f1_26: 0.2296 - val_f1_27: 0.9680 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.8965 - val_f1_36: 0.0000e+00 - val_f1_37: 0.7748 - val_f1_38: 0.0000e+00 - val_f1_39: 0.6560 - val_f1_40: 0.9196 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9108 - val_f1_45: 0.8458\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - 231s 15s/step - loss: 0.0601 - accuracy: 0.9835 - f1: 0.4083 - f1_1: 0.0000e+00 - f1_2: 0.8294 - f1_3: 0.8343 - f1_4: 0.3808 - f1_5: 0.6299 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9987 - f1_12: 0.0000e+00 - f1_13: 0.5587 - f1_14: 0.9938 - f1_15: 0.7497 - f1_17: 0.1230 - f1_18: 0.9118 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.7461 - f1_22: 0.0000e+00 - f1_23: 0.8230 - f1_24: 0.0000e+00 - f1_25: 0.9514 - f1_26: 0.4757 - f1_27: 0.9492 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.9055 - f1_36: 0.0000e+00 - f1_37: 0.7766 - f1_38: 0.2090 - f1_39: 0.7813 - f1_40: 0.9077 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.9237 - f1_45: 0.8712 - val_loss: 0.0656 - val_accuracy: 0.9816 - val_f1: 0.4015 - val_f1_1: 0.0000e+00 - val_f1_2: 0.8260 - val_f1_3: 0.8260 - val_f1_4: 0.3125 - val_f1_5: 0.5104 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6517 - val_f1_14: 0.9975 - val_f1_15: 0.7262 - val_f1_17: 0.1378 - val_f1_18: 0.8846 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.7257 - val_f1_22: 0.0000e+00 - val_f1_23: 0.7743 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9576 - val_f1_26: 0.4412 - val_f1_27: 0.9780 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.8984 - val_f1_36: 0.0000e+00 - val_f1_37: 0.7343 - val_f1_38: 0.2249 - val_f1_39: 0.7377 - val_f1_40: 0.9017 - val_f1_41: 0.0303 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9205 - val_f1_45: 0.8628\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - 231s 15s/step - loss: 0.0556 - accuracy: 0.9850 - f1: 0.4250 - f1_1: 0.0000e+00 - f1_2: 0.8565 - f1_3: 0.8421 - f1_4: 0.4146 - f1_5: 0.6439 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9987 - f1_12: 0.0000e+00 - f1_13: 0.6154 - f1_14: 0.9934 - f1_15: 0.8253 - f1_17: 0.1981 - f1_18: 0.9283 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.7759 - f1_22: 0.0000e+00 - f1_23: 0.8271 - f1_24: 0.0000e+00 - f1_25: 0.9534 - f1_26: 0.5672 - f1_27: 0.9628 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.9171 - f1_36: 0.0000e+00 - f1_37: 0.7972 - f1_38: 0.2886 - f1_39: 0.8109 - f1_40: 0.9306 - f1_41: 0.0357 - f1_42: 0.0000e+00 - f1_43: 0.9330 - f1_45: 0.8847 - val_loss: 0.0611 - val_accuracy: 0.9827 - val_f1: 0.4194 - val_f1_1: 0.0000e+00 - val_f1_2: 0.8474 - val_f1_3: 0.8286 - val_f1_4: 0.2623 - val_f1_5: 0.5074 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6555 - val_f1_14: 0.9975 - val_f1_15: 0.8218 - val_f1_17: 0.1949 - val_f1_18: 0.9153 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.7475 - val_f1_22: 0.0000e+00 - val_f1_23: 0.7882 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9596 - val_f1_26: 0.4840 - val_f1_27: 0.9829 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.9068 - val_f1_36: 0.0000e+00 - val_f1_37: 0.7952 - val_f1_38: 0.4849 - val_f1_39: 0.7704 - val_f1_40: 0.9275 - val_f1_41: 0.0955 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9290 - val_f1_45: 0.8728\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - 229s 14s/step - loss: 0.0523 - accuracy: 0.9858 - f1: 0.4458 - f1_1: 0.0000e+00 - f1_2: 0.8711 - f1_3: 0.8521 - f1_4: 0.4699 - f1_5: 0.6494 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9987 - f1_12: 0.0000e+00 - f1_13: 0.6544 - f1_14: 0.9943 - f1_15: 0.8613 - f1_17: 0.3066 - f1_18: 0.9332 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.7917 - f1_22: 0.0000e+00 - f1_23: 0.8421 - f1_24: 0.0000e+00 - f1_25: 0.9556 - f1_26: 0.6059 - f1_27: 0.9775 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.9352 - f1_36: 0.0000e+00 - f1_37: 0.8099 - f1_38: 0.5749 - f1_39: 0.8212 - f1_40: 0.9402 - f1_41: 0.1608 - f1_42: 0.0000e+00 - f1_43: 0.9359 - f1_45: 0.8917 - val_loss: 0.0579 - val_accuracy: 0.9837 - val_f1: 0.4435 - val_f1_1: 0.0000e+00 - val_f1_2: 0.8550 - val_f1_3: 0.8389 - val_f1_4: 0.3881 - val_f1_5: 0.5899 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6888 - val_f1_14: 0.9975 - val_f1_15: 0.8856 - val_f1_17: 0.2836 - val_f1_18: 0.9035 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.7632 - val_f1_22: 0.0000e+00 - val_f1_23: 0.7855 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9612 - val_f1_26: 0.5760 - val_f1_27: 0.9881 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.9222 - val_f1_36: 0.0000e+00 - val_f1_37: 0.8074 - val_f1_38: 0.6983 - val_f1_39: 0.7546 - val_f1_40: 0.9551 - val_f1_41: 0.2881 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9314 - val_f1_45: 0.8775\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - 230s 14s/step - loss: 0.0496 - accuracy: 0.9865 - f1: 0.4609 - f1_1: 0.0000e+00 - f1_2: 0.8880 - f1_3: 0.8561 - f1_4: 0.5063 - f1_5: 0.6836 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9986 - f1_12: 0.0000e+00 - f1_13: 0.7128 - f1_14: 0.9939 - f1_15: 0.8854 - f1_17: 0.3850 - f1_18: 0.9513 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.8062 - f1_22: 0.0000e+00 - f1_23: 0.8415 - f1_24: 0.0000e+00 - f1_25: 0.9562 - f1_26: 0.6247 - f1_27: 0.9785 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.9419 - f1_36: 0.0000e+00 - f1_37: 0.8117 - f1_38: 0.6983 - f1_39: 0.8311 - f1_40: 0.9542 - f1_41: 0.2876 - f1_42: 0.0000e+00 - f1_43: 0.9423 - f1_45: 0.9027 - val_loss: 0.0574 - val_accuracy: 0.9840 - val_f1: 0.4500 - val_f1_1: 0.0000e+00 - val_f1_2: 0.8706 - val_f1_3: 0.8493 - val_f1_4: 0.6521 - val_f1_5: 0.6059 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6194 - val_f1_14: 0.9975 - val_f1_15: 0.8523 - val_f1_17: 0.3188 - val_f1_18: 0.9448 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.7851 - val_f1_22: 0.0000e+00 - val_f1_23: 0.7819 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9623 - val_f1_26: 0.5634 - val_f1_27: 0.9899 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.9391 - val_f1_36: 0.0000e+00 - val_f1_37: 0.7862 - val_f1_38: 0.6618 - val_f1_39: 0.7837 - val_f1_40: 0.9443 - val_f1_41: 0.2643 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9277 - val_f1_45: 0.8982\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - 227s 14s/step - loss: 0.0474 - accuracy: 0.9871 - f1: 0.4749 - f1_1: 0.0000e+00 - f1_2: 0.8954 - f1_3: 0.8703 - f1_4: 0.5248 - f1_5: 0.6911 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9986 - f1_12: 0.0000e+00 - f1_13: 0.7049 - f1_14: 0.9937 - f1_15: 0.9053 - f1_17: 0.4687 - f1_18: 0.9620 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.8264 - f1_22: 0.0000e+00 - f1_23: 0.8535 - f1_24: 0.0625 - f1_25: 0.9594 - f1_26: 0.6327 - f1_27: 0.9801 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.9461 - f1_36: 0.0000e+00 - f1_37: 0.8267 - f1_38: 0.7605 - f1_39: 0.8397 - f1_40: 0.9617 - f1_41: 0.4775 - f1_42: 0.0000e+00 - f1_43: 0.9453 - f1_45: 0.9091 - val_loss: 0.0537 - val_accuracy: 0.9849 - val_f1: 0.4665 - val_f1_1: 0.0000e+00 - val_f1_2: 0.8732 - val_f1_3: 0.8502 - val_f1_4: 0.5510 - val_f1_5: 0.6578 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.7405 - val_f1_14: 0.9975 - val_f1_15: 0.8691 - val_f1_17: 0.3248 - val_f1_18: 0.9488 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.7831 - val_f1_22: 0.0000e+00 - val_f1_23: 0.7971 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9638 - val_f1_26: 0.6190 - val_f1_27: 0.9917 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.9159 - val_f1_36: 0.0000e+00 - val_f1_37: 0.8037 - val_f1_38: 0.8477 - val_f1_39: 0.7901 - val_f1_40: 0.9661 - val_f1_41: 0.5365 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9317 - val_f1_45: 0.8996\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - 231s 15s/step - loss: 0.0449 - accuracy: 0.9878 - f1: 0.4851 - f1_1: 0.0000e+00 - f1_2: 0.8994 - f1_3: 0.8826 - f1_4: 0.6131 - f1_5: 0.7078 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9986 - f1_12: 0.0000e+00 - f1_13: 0.7344 - f1_14: 0.9919 - f1_15: 0.9281 - f1_17: 0.5019 - f1_18: 0.9660 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.8218 - f1_22: 0.0000e+00 - f1_23: 0.8550 - f1_24: 0.0835 - f1_25: 0.9613 - f1_26: 0.6579 - f1_27: 0.9853 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.9528 - f1_36: 0.0000e+00 - f1_37: 0.8320 - f1_38: 0.8013 - f1_39: 0.8512 - f1_40: 0.9655 - f1_41: 0.5544 - f1_42: 0.0000e+00 - f1_43: 0.9443 - f1_45: 0.9146 - val_loss: 0.0524 - val_accuracy: 0.9851 - val_f1: 0.4723 - val_f1_1: 0.0000e+00 - val_f1_2: 0.8746 - val_f1_3: 0.8649 - val_f1_4: 0.5831 - val_f1_5: 0.5887 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6787 - val_f1_14: 0.9975 - val_f1_15: 0.9302 - val_f1_17: 0.4548 - val_f1_18: 0.9569 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.7780 - val_f1_22: 0.0000e+00 - val_f1_23: 0.7996 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9624 - val_f1_26: 0.6553 - val_f1_27: 0.9914 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.9455 - val_f1_36: 0.0000e+00 - val_f1_37: 0.8090 - val_f1_38: 0.8655 - val_f1_39: 0.7924 - val_f1_40: 0.9760 - val_f1_41: 0.5466 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9399 - val_f1_45: 0.8993\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - 230s 14s/step - loss: 0.0428 - accuracy: 0.9883 - f1: 0.5004 - f1_1: 0.0000e+00 - f1_2: 0.9041 - f1_3: 0.8813 - f1_4: 0.6418 - f1_5: 0.7188 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9986 - f1_12: 0.0000e+00 - f1_13: 0.7824 - f1_14: 0.9952 - f1_15: 0.9238 - f1_17: 0.5341 - f1_18: 0.9730 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.8373 - f1_22: 0.0000e+00 - f1_23: 0.8575 - f1_24: 0.2262 - f1_25: 0.9627 - f1_26: 0.6969 - f1_27: 0.9944 - f1_28: 0.0000e+00 - f1_30: 0.0139 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.9548 - f1_36: 0.0000e+00 - f1_37: 0.8362 - f1_38: 0.8591 - f1_39: 0.8534 - f1_40: 0.9731 - f1_41: 0.7266 - f1_42: 0.0000e+00 - f1_43: 0.9515 - f1_45: 0.9205 - val_loss: 0.0507 - val_accuracy: 0.9855 - val_f1: 0.4779 - val_f1_1: 0.0000e+00 - val_f1_2: 0.8765 - val_f1_3: 0.8666 - val_f1_4: 0.5418 - val_f1_5: 0.6360 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.7420 - val_f1_14: 0.9975 - val_f1_15: 0.9413 - val_f1_17: 0.4815 - val_f1_18: 0.9734 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8069 - val_f1_22: 0.0000e+00 - val_f1_23: 0.8127 - val_f1_24: 0.0303 - val_f1_25: 0.9665 - val_f1_26: 0.6287 - val_f1_27: 0.9914 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_35: 0.9427 - val_f1_36: 0.0000e+00 - val_f1_37: 0.7889 - val_f1_38: 0.8467 - val_f1_39: 0.8232 - val_f1_40: 0.9741 - val_f1_41: 0.6021 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9395 - val_f1_45: 0.9040\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - 228s 14s/step - loss: 0.0412 - accuracy: 0.9887 - f1: 0.5050 - f1_1: 0.0000e+00 - f1_2: 0.9099 - f1_3: 0.8918 - f1_4: 0.6565 - f1_5: 0.7282 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9986 - f1_12: 0.0000e+00 - f1_13: 0.7736 - f1_14: 0.9917 - f1_15: 0.9431 - f1_17: 0.6055 - f1_18: 0.9748 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.8446 - f1_22: 0.0000e+00 - f1_23: 0.8654 - f1_24: 0.2501 - f1_25: 0.9660 - f1_26: 0.6860 - f1_27: 0.9911 - f1_28: 0.0000e+00 - f1_30: 0.0513 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_35: 0.9556 - f1_36: 0.0000e+00 - f1_37: 0.8366 - f1_38: 0.8455 - f1_39: 0.8617 - f1_40: 0.9817 - f1_41: 0.7119 - f1_42: 0.0000e+00 - f1_43: 0.9550 - f1_45: 0.9242 - val_loss: 0.0491 - val_accuracy: 0.9858 - val_f1: 0.4857 - val_f1_1: 0.0000e+00 - val_f1_2: 0.8926 - val_f1_3: 0.8700 - val_f1_4: 0.6068 - val_f1_5: 0.6426 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.7576 - val_f1_14: 0.9975 - val_f1_15: 0.9295 - val_f1_17: 0.4425 - val_f1_18: 0.9749 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8045 - val_f1_22: 0.0000e+00 - val_f1_23: 0.7899 - val_f1_24: 0.1636 - val_f1_25: 0.9700 - val_f1_26: 0.6665 - val_f1_27: 0.9936 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0364 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0364 - val_f1_35: 0.9514 - val_f1_36: 0.0000e+00 - val_f1_37: 0.8237 - val_f1_38: 0.8627 - val_f1_39: 0.7680 - val_f1_40: 0.9739 - val_f1_41: 0.6208 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9418 - val_f1_45: 0.9099\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - 229s 14s/step - loss: 0.0396 - accuracy: 0.9891 - f1: 0.5154 - f1_1: 0.0000e+00 - f1_2: 0.9183 - f1_3: 0.8945 - f1_4: 0.6793 - f1_5: 0.7470 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9984 - f1_12: 0.0000e+00 - f1_13: 0.8017 - f1_14: 0.9942 - f1_15: 0.9496 - f1_17: 0.6016 - f1_18: 0.9757 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.8563 - f1_22: 0.0000e+00 - f1_23: 0.8678 - f1_24: 0.3326 - f1_25: 0.9680 - f1_26: 0.7166 - f1_27: 0.9954 - f1_28: 0.0000e+00 - f1_30: 0.0579 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0880 - f1_35: 0.9620 - f1_36: 0.0312 - f1_37: 0.8440 - f1_38: 0.8658 - f1_39: 0.8559 - f1_40: 0.9799 - f1_41: 0.7537 - f1_42: 0.0000e+00 - f1_43: 0.9552 - f1_45: 0.9259 - val_loss: 0.0515 - val_accuracy: 0.9852 - val_f1: 0.4824 - val_f1_1: 0.0000e+00 - val_f1_2: 0.8831 - val_f1_3: 0.8689 - val_f1_4: 0.7173 - val_f1_5: 0.5807 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.7249 - val_f1_14: 0.9975 - val_f1_15: 0.9397 - val_f1_17: 0.4175 - val_f1_18: 0.9720 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8064 - val_f1_22: 0.0000e+00 - val_f1_23: 0.7964 - val_f1_24: 0.1091 - val_f1_25: 0.9636 - val_f1_26: 0.5457 - val_f1_27: 0.9936 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0678 - val_f1_35: 0.9582 - val_f1_36: 0.0000e+00 - val_f1_37: 0.8142 - val_f1_38: 0.8896 - val_f1_39: 0.7838 - val_f1_40: 0.9855 - val_f1_41: 0.6273 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9350 - val_f1_45: 0.9166\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - 229s 14s/step - loss: 0.0381 - accuracy: 0.9895 - f1: 0.5251 - f1_1: 0.0000e+00 - f1_2: 0.9247 - f1_3: 0.9087 - f1_4: 0.7028 - f1_5: 0.7377 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9985 - f1_12: 0.0000e+00 - f1_13: 0.8050 - f1_14: 0.9942 - f1_15: 0.9505 - f1_17: 0.6196 - f1_18: 0.9783 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.8660 - f1_22: 0.0000e+00 - f1_23: 0.8698 - f1_24: 0.4731 - f1_25: 0.9684 - f1_26: 0.7140 - f1_27: 0.9951 - f1_28: 0.0000e+00 - f1_30: 0.0192 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.1932 - f1_35: 0.9618 - f1_36: 0.0000e+00 - f1_37: 0.8550 - f1_38: 0.8997 - f1_39: 0.8666 - f1_40: 0.9872 - f1_41: 0.7903 - f1_42: 0.0417 - f1_43: 0.9535 - f1_45: 0.9300 - val_loss: 0.0484 - val_accuracy: 0.9859 - val_f1: 0.4989 - val_f1_1: 0.0000e+00 - val_f1_2: 0.8944 - val_f1_3: 0.8741 - val_f1_4: 0.6496 - val_f1_5: 0.6317 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.7957 - val_f1_14: 0.9975 - val_f1_15: 0.9343 - val_f1_17: 0.5573 - val_f1_18: 0.9773 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8144 - val_f1_22: 0.0000e+00 - val_f1_23: 0.8068 - val_f1_24: 0.1735 - val_f1_25: 0.9752 - val_f1_26: 0.5863 - val_f1_27: 0.9936 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0364 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.3045 - val_f1_35: 0.9588 - val_f1_36: 0.0000e+00 - val_f1_37: 0.7995 - val_f1_38: 0.8988 - val_f1_39: 0.8256 - val_f1_40: 0.9763 - val_f1_41: 0.6351 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9445 - val_f1_45: 0.9152\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - 230s 14s/step - loss: 0.0365 - accuracy: 0.9901 - f1: 0.5381 - f1_1: 0.0000e+00 - f1_2: 0.9269 - f1_3: 0.9086 - f1_4: 0.7254 - f1_5: 0.7439 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0865 - f1_10: 0.9985 - f1_12: 0.0000e+00 - f1_13: 0.8192 - f1_14: 0.9950 - f1_15: 0.9440 - f1_17: 0.6375 - f1_18: 0.9836 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.8678 - f1_22: 0.0000e+00 - f1_23: 0.8768 - f1_24: 0.4077 - f1_25: 0.9714 - f1_26: 0.7146 - f1_27: 0.9964 - f1_28: 0.0000e+00 - f1_30: 0.0810 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.3677 - f1_35: 0.9677 - f1_36: 0.0295 - f1_37: 0.8584 - f1_38: 0.9226 - f1_39: 0.8767 - f1_40: 0.9857 - f1_41: 0.7894 - f1_42: 0.1479 - f1_43: 0.9594 - f1_45: 0.9341 - val_loss: 0.0474 - val_accuracy: 0.9861 - val_f1: 0.5013 - val_f1_1: 0.0000e+00 - val_f1_2: 0.8895 - val_f1_3: 0.8635 - val_f1_4: 0.7057 - val_f1_5: 0.6901 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0303 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.8008 - val_f1_14: 0.9975 - val_f1_15: 0.9427 - val_f1_17: 0.5268 - val_f1_18: 0.9763 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8146 - val_f1_22: 0.0000e+00 - val_f1_23: 0.8052 - val_f1_24: 0.1818 - val_f1_25: 0.9753 - val_f1_26: 0.6629 - val_f1_27: 0.9936 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0364 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.1467 - val_f1_35: 0.9623 - val_f1_36: 0.0000e+00 - val_f1_37: 0.7877 - val_f1_38: 0.8569 - val_f1_39: 0.8220 - val_f1_40: 0.9757 - val_f1_41: 0.6386 - val_f1_42: 0.0970 - val_f1_43: 0.9499 - val_f1_45: 0.9206\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - 230s 15s/step - loss: 0.0349 - accuracy: 0.9905 - f1: 0.5554 - f1_1: 0.0000e+00 - f1_2: 0.9292 - f1_3: 0.9127 - f1_4: 0.7521 - f1_5: 0.7744 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0954 - f1_10: 0.9987 - f1_12: 0.0000e+00 - f1_13: 0.8318 - f1_14: 0.9957 - f1_15: 0.9524 - f1_17: 0.6848 - f1_18: 0.9842 - f1_19: 0.0083 - f1_20: 0.0000e+00 - f1_21: 0.8660 - f1_22: 0.0000e+00 - f1_23: 0.8832 - f1_24: 0.5582 - f1_25: 0.9737 - f1_26: 0.7351 - f1_27: 0.9965 - f1_28: 0.0000e+00 - f1_30: 0.1637 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.5289 - f1_35: 0.9681 - f1_36: 0.0781 - f1_37: 0.8619 - f1_38: 0.9191 - f1_39: 0.8878 - f1_40: 0.9835 - f1_41: 0.8155 - f1_42: 0.1812 - f1_43: 0.9585 - f1_45: 0.9381 - val_loss: 0.0449 - val_accuracy: 0.9869 - val_f1: 0.5078 - val_f1_1: 0.0000e+00 - val_f1_2: 0.9039 - val_f1_3: 0.8757 - val_f1_4: 0.6707 - val_f1_5: 0.6476 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.1364 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.7951 - val_f1_14: 0.9975 - val_f1_15: 0.9600 - val_f1_17: 0.5100 - val_f1_18: 0.9790 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8293 - val_f1_22: 0.0000e+00 - val_f1_23: 0.8181 - val_f1_24: 0.2162 - val_f1_25: 0.9775 - val_f1_26: 0.6794 - val_f1_27: 0.9936 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0364 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2025 - val_f1_35: 0.9642 - val_f1_36: 0.0000e+00 - val_f1_37: 0.8337 - val_f1_38: 0.8631 - val_f1_39: 0.8260 - val_f1_40: 0.9761 - val_f1_41: 0.6508 - val_f1_42: 0.0970 - val_f1_43: 0.9510 - val_f1_45: 0.9231\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - 229s 14s/step - loss: 0.0339 - accuracy: 0.9907 - f1: 0.5615 - f1_1: 0.0000e+00 - f1_2: 0.9336 - f1_3: 0.9184 - f1_4: 0.7441 - f1_5: 0.7718 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.1226 - f1_10: 0.9986 - f1_12: 0.0000e+00 - f1_13: 0.8397 - f1_14: 0.9949 - f1_15: 0.9567 - f1_17: 0.6683 - f1_18: 0.9855 - f1_19: 0.0270 - f1_20: 0.0000e+00 - f1_21: 0.8809 - f1_22: 0.0000e+00 - f1_23: 0.8827 - f1_24: 0.5332 - f1_25: 0.9754 - f1_26: 0.7468 - f1_27: 0.9970 - f1_28: 0.0000e+00 - f1_30: 0.1158 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.4874 - f1_35: 0.9721 - f1_36: 0.1519 - f1_37: 0.8687 - f1_38: 0.9165 - f1_39: 0.8890 - f1_40: 0.9890 - f1_41: 0.7577 - f1_42: 0.4354 - f1_43: 0.9606 - f1_45: 0.9379 - val_loss: 0.0442 - val_accuracy: 0.9873 - val_f1: 0.5215 - val_f1_1: 0.0000e+00 - val_f1_2: 0.9046 - val_f1_3: 0.8818 - val_f1_4: 0.7378 - val_f1_5: 0.6749 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.1364 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.7624 - val_f1_14: 0.9975 - val_f1_15: 0.9699 - val_f1_17: 0.5187 - val_f1_18: 0.9817 - val_f1_19: 0.0557 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8296 - val_f1_22: 0.0000e+00 - val_f1_23: 0.8212 - val_f1_24: 0.1798 - val_f1_25: 0.9765 - val_f1_26: 0.6830 - val_f1_27: 0.9936 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0364 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.4776 - val_f1_35: 0.9699 - val_f1_36: 0.0000e+00 - val_f1_37: 0.8339 - val_f1_38: 0.9167 - val_f1_39: 0.8295 - val_f1_40: 0.9858 - val_f1_41: 0.6508 - val_f1_42: 0.1840 - val_f1_43: 0.9480 - val_f1_45: 0.9243\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - 228s 14s/step - loss: 0.0332 - accuracy: 0.9908 - f1: 0.5788 - f1_1: 0.0000e+00 - f1_2: 0.9357 - f1_3: 0.9220 - f1_4: 0.7731 - f1_5: 0.7842 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.1716 - f1_10: 0.9987 - f1_12: 0.0000e+00 - f1_13: 0.8426 - f1_14: 0.9932 - f1_15: 0.9682 - f1_17: 0.6847 - f1_18: 0.9882 - f1_19: 0.1173 - f1_20: 0.0000e+00 - f1_21: 0.8792 - f1_22: 0.0000e+00 - f1_23: 0.8818 - f1_24: 0.6058 - f1_25: 0.9762 - f1_26: 0.7506 - f1_27: 0.9974 - f1_28: 0.0000e+00 - f1_30: 0.2354 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.6878 - f1_35: 0.9741 - f1_36: 0.1304 - f1_37: 0.8609 - f1_38: 0.9252 - f1_39: 0.8872 - f1_40: 0.9907 - f1_41: 0.8325 - f1_42: 0.4536 - f1_43: 0.9619 - f1_45: 0.9407 - val_loss: 0.0454 - val_accuracy: 0.9867 - val_f1: 0.5314 - val_f1_1: 0.0000e+00 - val_f1_2: 0.9096 - val_f1_3: 0.8696 - val_f1_4: 0.7143 - val_f1_5: 0.6869 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.2379 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.7898 - val_f1_14: 0.9975 - val_f1_15: 0.9539 - val_f1_17: 0.5432 - val_f1_18: 0.9847 - val_f1_19: 0.0928 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8335 - val_f1_22: 0.0000e+00 - val_f1_23: 0.8238 - val_f1_24: 0.2182 - val_f1_25: 0.9768 - val_f1_26: 0.6278 - val_f1_27: 0.9917 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0364 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.5399 - val_f1_35: 0.9683 - val_f1_36: 0.0833 - val_f1_37: 0.7653 - val_f1_38: 0.9050 - val_f1_39: 0.8349 - val_f1_40: 0.9878 - val_f1_41: 0.6386 - val_f1_42: 0.3710 - val_f1_43: 0.9533 - val_f1_45: 0.9217\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - 229s 14s/step - loss: 0.0330 - accuracy: 0.9908 - f1: 0.5807 - f1_1: 0.0000e+00 - f1_2: 0.9394 - f1_3: 0.9231 - f1_4: 0.7789 - f1_5: 0.7776 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.1960 - f1_10: 0.9986 - f1_12: 0.0000e+00 - f1_13: 0.8497 - f1_14: 0.9920 - f1_15: 0.9659 - f1_17: 0.7175 - f1_18: 0.9814 - f1_19: 0.1110 - f1_20: 0.0000e+00 - f1_21: 0.8797 - f1_22: 0.0000e+00 - f1_23: 0.8755 - f1_24: 0.5700 - f1_25: 0.9787 - f1_26: 0.7510 - f1_27: 0.9978 - f1_28: 0.0250 - f1_30: 0.2815 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.6748 - f1_35: 0.9722 - f1_36: 0.1679 - f1_37: 0.8584 - f1_38: 0.9368 - f1_39: 0.8767 - f1_40: 0.9910 - f1_41: 0.7649 - f1_42: 0.4859 - f1_43: 0.9637 - f1_45: 0.9443 - val_loss: 0.0430 - val_accuracy: 0.9875 - val_f1: 0.5323 - val_f1_1: 0.0000e+00 - val_f1_2: 0.9138 - val_f1_3: 0.8793 - val_f1_4: 0.7347 - val_f1_5: 0.6675 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.2171 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.8003 - val_f1_14: 0.9975 - val_f1_15: 0.9695 - val_f1_17: 0.5778 - val_f1_18: 0.9845 - val_f1_19: 0.1123 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8246 - val_f1_22: 0.0000e+00 - val_f1_23: 0.8298 - val_f1_24: 0.2939 - val_f1_25: 0.9813 - val_f1_26: 0.6188 - val_f1_27: 0.9914 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0364 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.4270 - val_f1_35: 0.9683 - val_f1_36: 0.0000e+00 - val_f1_37: 0.8347 - val_f1_38: 0.8930 - val_f1_39: 0.8443 - val_f1_40: 0.9864 - val_f1_41: 0.6508 - val_f1_42: 0.3710 - val_f1_43: 0.9559 - val_f1_45: 0.9293\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - 229s 14s/step - loss: 0.0308 - accuracy: 0.9915 - f1: 0.5940 - f1_1: 0.0000e+00 - f1_2: 0.9472 - f1_3: 0.9333 - f1_4: 0.7803 - f1_5: 0.7957 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.1847 - f1_10: 0.9986 - f1_12: 0.0000e+00 - f1_13: 0.8497 - f1_14: 0.9951 - f1_15: 0.9701 - f1_17: 0.7175 - f1_18: 0.9873 - f1_19: 0.1900 - f1_20: 0.0000e+00 - f1_21: 0.8834 - f1_22: 0.0000e+00 - f1_23: 0.8938 - f1_24: 0.5966 - f1_25: 0.9798 - f1_26: 0.7502 - f1_27: 0.9979 - f1_28: 0.0458 - f1_30: 0.2506 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.7562 - f1_35: 0.9753 - f1_36: 0.2908 - f1_37: 0.8775 - f1_38: 0.9407 - f1_39: 0.9020 - f1_40: 0.9913 - f1_41: 0.8111 - f1_42: 0.5574 - f1_43: 0.9640 - f1_45: 0.9456 - val_loss: 0.0422 - val_accuracy: 0.9875 - val_f1: 0.5458 - val_f1_1: 0.0000e+00 - val_f1_2: 0.9121 - val_f1_3: 0.8756 - val_f1_4: 0.7353 - val_f1_5: 0.7000 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.2688 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.8095 - val_f1_14: 0.9975 - val_f1_15: 0.9645 - val_f1_17: 0.6111 - val_f1_18: 0.9871 - val_f1_19: 0.1422 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8372 - val_f1_22: 0.0000e+00 - val_f1_23: 0.7943 - val_f1_24: 0.3878 - val_f1_25: 0.9791 - val_f1_26: 0.6566 - val_f1_27: 0.9936 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0623 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.5851 - val_f1_35: 0.9705 - val_f1_36: 0.0606 - val_f1_37: 0.8420 - val_f1_38: 0.8840 - val_f1_39: 0.8386 - val_f1_40: 0.9916 - val_f1_41: 0.6508 - val_f1_42: 0.4114 - val_f1_43: 0.9581 - val_f1_45: 0.9249\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - 227s 14s/step - loss: 0.0298 - accuracy: 0.9917 - f1: 0.6054 - f1_1: 0.0000e+00 - f1_2: 0.9542 - f1_3: 0.9248 - f1_4: 0.7868 - f1_5: 0.8031 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.3405 - f1_10: 0.9987 - f1_12: 0.0000e+00 - f1_13: 0.8479 - f1_14: 0.9936 - f1_15: 0.9692 - f1_17: 0.7055 - f1_18: 0.9867 - f1_19: 0.1667 - f1_20: 0.0000e+00 - f1_21: 0.8903 - f1_22: 0.0000e+00 - f1_23: 0.8898 - f1_24: 0.6735 - f1_25: 0.9807 - f1_26: 0.7731 - f1_27: 0.9966 - f1_28: 0.0500 - f1_30: 0.3552 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.7838 - f1_35: 0.9785 - f1_36: 0.3217 - f1_37: 0.8794 - f1_38: 0.9336 - f1_39: 0.9039 - f1_40: 0.9937 - f1_41: 0.8217 - f1_42: 0.5977 - f1_43: 0.9679 - f1_45: 0.9471 - val_loss: 0.0415 - val_accuracy: 0.9881 - val_f1: 0.5590 - val_f1_1: 0.0000e+00 - val_f1_2: 0.9192 - val_f1_3: 0.8831 - val_f1_4: 0.7449 - val_f1_5: 0.6776 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.3212 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.8153 - val_f1_14: 0.9975 - val_f1_15: 0.9651 - val_f1_17: 0.6294 - val_f1_18: 0.9834 - val_f1_19: 0.2045 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8337 - val_f1_22: 0.0000e+00 - val_f1_23: 0.8279 - val_f1_24: 0.3164 - val_f1_25: 0.9788 - val_f1_26: 0.6720 - val_f1_27: 0.9936 - val_f1_28: 0.0182 - val_f1_30: 0.1869 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.7409 - val_f1_35: 0.9684 - val_f1_36: 0.0833 - val_f1_37: 0.8338 - val_f1_38: 0.9107 - val_f1_39: 0.8571 - val_f1_40: 0.9913 - val_f1_41: 0.6599 - val_f1_42: 0.4671 - val_f1_43: 0.9564 - val_f1_45: 0.9222\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - 230s 15s/step - loss: 0.0290 - accuracy: 0.9920 - f1: 0.6054 - f1_1: 0.0000e+00 - f1_2: 0.9560 - f1_3: 0.9314 - f1_4: 0.7959 - f1_5: 0.8065 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.3356 - f1_10: 0.9986 - f1_12: 0.0000e+00 - f1_13: 0.8600 - f1_14: 0.9953 - f1_15: 0.9728 - f1_17: 0.7402 - f1_18: 0.9894 - f1_19: 0.2986 - f1_20: 0.0089 - f1_21: 0.8963 - f1_22: 0.0000e+00 - f1_23: 0.8939 - f1_24: 0.6490 - f1_25: 0.9809 - f1_26: 0.7897 - f1_27: 0.9972 - f1_28: 0.1162 - f1_30: 0.2150 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.7507 - f1_35: 0.9762 - f1_36: 0.1621 - f1_37: 0.8796 - f1_38: 0.9543 - f1_39: 0.9086 - f1_40: 0.9886 - f1_41: 0.8515 - f1_42: 0.6029 - f1_43: 0.9671 - f1_45: 0.9473 - val_loss: 0.0423 - val_accuracy: 0.9876 - val_f1: 0.5597 - val_f1_1: 0.0000e+00 - val_f1_2: 0.9233 - val_f1_3: 0.8908 - val_f1_4: 0.7618 - val_f1_5: 0.6936 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.3690 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.7844 - val_f1_14: 0.9975 - val_f1_15: 0.9683 - val_f1_17: 0.6225 - val_f1_18: 0.9854 - val_f1_19: 0.0876 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8388 - val_f1_22: 0.0000e+00 - val_f1_23: 0.7894 - val_f1_24: 0.3284 - val_f1_25: 0.9764 - val_f1_26: 0.6826 - val_f1_27: 0.9936 - val_f1_28: 0.0000e+00 - val_f1_30: 0.2273 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.7547 - val_f1_35: 0.9727 - val_f1_36: 0.0833 - val_f1_37: 0.8374 - val_f1_38: 0.9070 - val_f1_39: 0.8351 - val_f1_40: 0.9956 - val_f1_41: 0.6567 - val_f1_42: 0.5514 - val_f1_43: 0.9483 - val_f1_45: 0.9233\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - 227s 14s/step - loss: 0.0283 - accuracy: 0.9922 - f1: 0.6189 - f1_1: 0.0000e+00 - f1_2: 0.9552 - f1_3: 0.9267 - f1_4: 0.7885 - f1_5: 0.8106 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.4315 - f1_10: 0.9986 - f1_12: 0.0000e+00 - f1_13: 0.8646 - f1_14: 0.9923 - f1_15: 0.9765 - f1_17: 0.7578 - f1_18: 0.9888 - f1_19: 0.1502 - f1_20: 0.0512 - f1_21: 0.9024 - f1_22: 0.0000e+00 - f1_23: 0.8983 - f1_24: 0.6415 - f1_25: 0.9826 - f1_26: 0.7891 - f1_27: 0.9947 - f1_28: 0.1355 - f1_30: 0.3262 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.8725 - f1_35: 0.9793 - f1_36: 0.2639 - f1_37: 0.8836 - f1_38: 0.9581 - f1_39: 0.9062 - f1_40: 0.9952 - f1_41: 0.8344 - f1_42: 0.7802 - f1_43: 0.9676 - f1_45: 0.9538 - val_loss: 0.0407 - val_accuracy: 0.9880 - val_f1: 0.5621 - val_f1_1: 0.0000e+00 - val_f1_2: 0.9189 - val_f1_3: 0.8904 - val_f1_4: 0.6994 - val_f1_5: 0.7089 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.2608 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.8176 - val_f1_14: 0.9975 - val_f1_15: 0.9694 - val_f1_17: 0.5858 - val_f1_18: 0.9846 - val_f1_19: 0.2758 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8394 - val_f1_22: 0.0000e+00 - val_f1_23: 0.8214 - val_f1_24: 0.4282 - val_f1_25: 0.9796 - val_f1_26: 0.6912 - val_f1_27: 0.9936 - val_f1_28: 0.0000e+00 - val_f1_30: 0.1472 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.7019 - val_f1_35: 0.9728 - val_f1_36: 0.2193 - val_f1_37: 0.8385 - val_f1_38: 0.8995 - val_f1_39: 0.8518 - val_f1_40: 0.9889 - val_f1_41: 0.6658 - val_f1_42: 0.4498 - val_f1_43: 0.9576 - val_f1_45: 0.9274\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - 229s 14s/step - loss: 0.0274 - accuracy: 0.9924 - f1: 0.6227 - f1_1: 0.0000e+00 - f1_2: 0.9584 - f1_3: 0.9358 - f1_4: 0.8023 - f1_5: 0.8112 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.3228 - f1_10: 0.9986 - f1_12: 0.0000e+00 - f1_13: 0.8814 - f1_14: 0.9951 - f1_15: 0.9781 - f1_17: 0.7416 - f1_18: 0.9886 - f1_19: 0.3139 - f1_20: 0.0625 - f1_21: 0.9103 - f1_22: 0.0000e+00 - f1_23: 0.8994 - f1_24: 0.7079 - f1_25: 0.9822 - f1_26: 0.8102 - f1_27: 0.9913 - f1_28: 0.1187 - f1_30: 0.3045 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.8238 - f1_35: 0.9805 - f1_36: 0.3338 - f1_37: 0.8894 - f1_38: 0.9585 - f1_39: 0.9119 - f1_40: 0.9926 - f1_41: 0.8426 - f1_42: 0.7481 - f1_43: 0.9691 - f1_45: 0.9431 - val_loss: 0.0431 - val_accuracy: 0.9872 - val_f1: 0.5536 - val_f1_1: 0.0000e+00 - val_f1_2: 0.9170 - val_f1_3: 0.8850 - val_f1_4: 0.7669 - val_f1_5: 0.6726 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.2833 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.8169 - val_f1_14: 0.9975 - val_f1_15: 0.9719 - val_f1_17: 0.5923 - val_f1_18: 0.9801 - val_f1_19: 0.1157 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8333 - val_f1_22: 0.0000e+00 - val_f1_23: 0.8099 - val_f1_24: 0.3078 - val_f1_25: 0.9813 - val_f1_26: 0.6615 - val_f1_27: 0.9936 - val_f1_28: 0.0696 - val_f1_30: 0.0825 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.7337 - val_f1_35: 0.9714 - val_f1_36: 0.1035 - val_f1_37: 0.8327 - val_f1_38: 0.9109 - val_f1_39: 0.7754 - val_f1_40: 0.9913 - val_f1_41: 0.6915 - val_f1_42: 0.5126 - val_f1_43: 0.9563 - val_f1_45: 0.9241\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - 227s 14s/step - loss: 0.0259 - accuracy: 0.9930 - f1: 0.6315 - f1_1: 0.0000e+00 - f1_2: 0.9632 - f1_3: 0.9399 - f1_4: 0.8193 - f1_5: 0.8279 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.4025 - f1_10: 0.9988 - f1_12: 0.0000e+00 - f1_13: 0.8828 - f1_14: 0.9936 - f1_15: 0.9794 - f1_17: 0.7676 - f1_18: 0.9884 - f1_19: 0.3133 - f1_20: 0.0658 - f1_21: 0.9194 - f1_22: 0.0000e+00 - f1_23: 0.9118 - f1_24: 0.7058 - f1_25: 0.9852 - f1_26: 0.8086 - f1_27: 0.9972 - f1_28: 0.2371 - f1_30: 0.4065 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.8022 - f1_35: 0.9803 - f1_36: 0.2882 - f1_37: 0.8937 - f1_38: 0.9640 - f1_39: 0.9141 - f1_40: 0.9925 - f1_41: 0.8530 - f1_42: 0.7313 - f1_43: 0.9712 - f1_45: 0.9550 - val_loss: 0.0401 - val_accuracy: 0.9881 - val_f1: 0.5642 - val_f1_1: 0.0000e+00 - val_f1_2: 0.9245 - val_f1_3: 0.8883 - val_f1_4: 0.7499 - val_f1_5: 0.6997 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.3630 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.8240 - val_f1_14: 0.9975 - val_f1_15: 0.9789 - val_f1_17: 0.5862 - val_f1_18: 0.9841 - val_f1_19: 0.2447 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8333 - val_f1_22: 0.0000e+00 - val_f1_23: 0.8323 - val_f1_24: 0.2388 - val_f1_25: 0.9811 - val_f1_26: 0.6737 - val_f1_27: 0.9973 - val_f1_28: 0.0712 - val_f1_30: 0.1869 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.6851 - val_f1_35: 0.9721 - val_f1_36: 0.1641 - val_f1_37: 0.8420 - val_f1_38: 0.9149 - val_f1_39: 0.8523 - val_f1_40: 0.9973 - val_f1_41: 0.6817 - val_f1_42: 0.5257 - val_f1_43: 0.9537 - val_f1_45: 0.9235\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - 227s 14s/step - loss: 0.0251 - accuracy: 0.9933 - f1: 0.6417 - f1_1: 0.0000e+00 - f1_2: 0.9667 - f1_3: 0.9437 - f1_4: 0.8221 - f1_5: 0.8290 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.4269 - f1_10: 0.9986 - f1_12: 0.0000e+00 - f1_13: 0.8660 - f1_14: 0.9947 - f1_15: 0.9827 - f1_17: 0.7566 - f1_18: 0.9898 - f1_19: 0.4300 - f1_20: 0.0994 - f1_21: 0.9155 - f1_22: 0.0000e+00 - f1_23: 0.9150 - f1_24: 0.6628 - f1_25: 0.9833 - f1_26: 0.8058 - f1_27: 0.9973 - f1_28: 0.2136 - f1_30: 0.4830 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.7972 - f1_35: 0.9796 - f1_36: 0.4323 - f1_37: 0.9014 - f1_38: 0.9583 - f1_39: 0.9193 - f1_40: 0.9863 - f1_41: 0.8506 - f1_42: 0.8368 - f1_43: 0.9711 - f1_45: 0.9508 - val_loss: 0.0404 - val_accuracy: 0.9883 - val_f1: 0.5689 - val_f1_1: 0.0000e+00 - val_f1_2: 0.9249 - val_f1_3: 0.8859 - val_f1_4: 0.7678 - val_f1_5: 0.6718 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.3528 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.8207 - val_f1_14: 0.9975 - val_f1_15: 0.9744 - val_f1_17: 0.5859 - val_f1_18: 0.9833 - val_f1_19: 0.3181 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8447 - val_f1_22: 0.0000e+00 - val_f1_23: 0.8246 - val_f1_24: 0.3552 - val_f1_25: 0.9835 - val_f1_26: 0.7049 - val_f1_27: 0.9936 - val_f1_28: 0.0817 - val_f1_30: 0.1472 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.7157 - val_f1_35: 0.9705 - val_f1_36: 0.0833 - val_f1_37: 0.8414 - val_f1_38: 0.9138 - val_f1_39: 0.8554 - val_f1_40: 0.9945 - val_f1_41: 0.6837 - val_f1_42: 0.5922 - val_f1_43: 0.9596 - val_f1_45: 0.9256\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - 226s 14s/step - loss: 0.0244 - accuracy: 0.9933 - f1: 0.6439 - f1_1: 0.0000e+00 - f1_2: 0.9683 - f1_3: 0.9455 - f1_4: 0.8218 - f1_5: 0.8374 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.4735 - f1_10: 0.9987 - f1_12: 0.0000e+00 - f1_13: 0.8819 - f1_14: 0.9918 - f1_15: 0.9777 - f1_17: 0.7697 - f1_18: 0.9889 - f1_19: 0.4056 - f1_20: 0.1505 - f1_21: 0.9187 - f1_22: 0.0000e+00 - f1_23: 0.9152 - f1_24: 0.7265 - f1_25: 0.9865 - f1_26: 0.8234 - f1_27: 0.9980 - f1_28: 0.2966 - f1_30: 0.4174 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.8481 - f1_35: 0.9810 - f1_36: 0.4041 - f1_37: 0.9010 - f1_38: 0.9687 - f1_39: 0.9239 - f1_40: 0.9917 - f1_41: 0.8658 - f1_42: 0.6518 - f1_43: 0.9728 - f1_45: 0.9513 - val_loss: 0.0399 - val_accuracy: 0.9884 - val_f1: 0.5767 - val_f1_1: 0.0000e+00 - val_f1_2: 0.9264 - val_f1_3: 0.8939 - val_f1_4: 0.7480 - val_f1_5: 0.7112 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.3247 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.7987 - val_f1_14: 0.9975 - val_f1_15: 0.9713 - val_f1_17: 0.6386 - val_f1_18: 0.9742 - val_f1_19: 0.3407 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8336 - val_f1_22: 0.0000e+00 - val_f1_23: 0.8218 - val_f1_24: 0.4082 - val_f1_25: 0.9791 - val_f1_26: 0.7280 - val_f1_27: 0.9936 - val_f1_28: 0.0923 - val_f1_30: 0.2204 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.7157 - val_f1_35: 0.9704 - val_f1_36: 0.1399 - val_f1_37: 0.8361 - val_f1_38: 0.9138 - val_f1_39: 0.8633 - val_f1_40: 0.9937 - val_f1_41: 0.6960 - val_f1_42: 0.6528 - val_f1_43: 0.9560 - val_f1_45: 0.9259\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - 228s 14s/step - loss: 0.0237 - accuracy: 0.9935 - f1: 0.6555 - f1_1: 0.0000e+00 - f1_2: 0.9691 - f1_3: 0.9436 - f1_4: 0.8210 - f1_5: 0.8381 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.4739 - f1_10: 0.9986 - f1_12: 0.0000e+00 - f1_13: 0.8842 - f1_14: 0.9944 - f1_15: 0.9861 - f1_17: 0.7656 - f1_18: 0.9888 - f1_19: 0.4760 - f1_20: 0.1272 - f1_21: 0.9186 - f1_22: 0.0000e+00 - f1_23: 0.9200 - f1_24: 0.7482 - f1_25: 0.9857 - f1_26: 0.8247 - f1_27: 0.9953 - f1_28: 0.2620 - f1_30: 0.4646 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.8559 - f1_35: 0.9811 - f1_36: 0.4262 - f1_37: 0.9051 - f1_38: 0.9752 - f1_39: 0.9258 - f1_40: 0.9957 - f1_41: 0.8972 - f1_42: 0.9425 - f1_43: 0.9722 - f1_45: 0.9581 - val_loss: 0.0396 - val_accuracy: 0.9884 - val_f1: 0.5799 - val_f1_1: 0.0000e+00 - val_f1_2: 0.9276 - val_f1_3: 0.8931 - val_f1_4: 0.7643 - val_f1_5: 0.6966 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.3814 - val_f1_10: 1.0000 - val_f1_12: 0.0000e+00 - val_f1_13: 0.8183 - val_f1_14: 1.0000 - val_f1_15: 0.9702 - val_f1_17: 0.6363 - val_f1_18: 0.9814 - val_f1_19: 0.2445 - val_f1_20: 0.0000e+00 - val_f1_21: 0.8388 - val_f1_22: 0.0000e+00 - val_f1_23: 0.8309 - val_f1_24: 0.3738 - val_f1_25: 0.9846 - val_f1_26: 0.6524 - val_f1_27: 0.9936 - val_f1_28: 0.0999 - val_f1_30: 0.2345 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.7640 - val_f1_35: 0.9718 - val_f1_36: 0.2561 - val_f1_37: 0.8401 - val_f1_38: 0.8919 - val_f1_39: 0.8579 - val_f1_40: 0.9956 - val_f1_41: 0.6987 - val_f1_42: 0.7113 - val_f1_43: 0.9591 - val_f1_45: 0.9268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#History f1 for class "
      ],
      "metadata": {
        "id": "xodj_G1VtJ8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_val_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_val_list[i] = history.history['val_f1_{}'.format(i)][-1]\n",
        "f1_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_list[i] = history.history['f1_{}'.format(i)][-1]"
      ],
      "metadata": {
        "id": "IDdWDpwQtP9i"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index2tag = {value : key for (key, value) in tag2index.items()}\n",
        "index2tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEBd5tE6tRdH",
        "outputId": "c0690866-460e-49b9-b3f5-5b2fc9f53922"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '-PAD-',\n",
              " 1: 'UH',\n",
              " 2: 'CD',\n",
              " 3: 'VB',\n",
              " 4: 'RB',\n",
              " 5: 'JJ',\n",
              " 6: 'PDT',\n",
              " 7: 'WP$',\n",
              " 8: '#',\n",
              " 9: 'RBR',\n",
              " 10: 'TO',\n",
              " 11: ':',\n",
              " 12: 'LS',\n",
              " 13: 'VBP',\n",
              " 14: '$',\n",
              " 15: 'MD',\n",
              " 16: '.',\n",
              " 17: 'VBG',\n",
              " 18: 'CC',\n",
              " 19: 'JJR',\n",
              " 20: 'NNPS',\n",
              " 21: 'VBD',\n",
              " 22: 'FW',\n",
              " 23: 'NNP',\n",
              " 24: 'RP',\n",
              " 25: 'DT',\n",
              " 26: 'VBN',\n",
              " 27: 'POS',\n",
              " 28: 'JJS',\n",
              " 29: \"''\",\n",
              " 30: '-RRB-',\n",
              " 31: 'RBS',\n",
              " 32: 'SYM',\n",
              " 33: 'WRB',\n",
              " 34: ',',\n",
              " 35: 'PRP',\n",
              " 36: '-LRB-',\n",
              " 37: 'NN',\n",
              " 38: 'WDT',\n",
              " 39: 'NNS',\n",
              " 40: 'PRP$',\n",
              " 41: 'WP',\n",
              " 42: 'EX',\n",
              " 43: 'IN',\n",
              " 44: '``',\n",
              " 45: 'VBZ'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- F1: {}'.format(index2tag[i], f1_list[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPT352DptUGS",
        "outputId": "0a53d17e-37b1-486f-8a8a-6a4d663f4756"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: UH --- F1: 0.0\n",
            "Tag: CD --- F1: 0.9691113829612732\n",
            "Tag: VB --- F1: 0.9436362981796265\n",
            "Tag: RB --- F1: 0.820997953414917\n",
            "Tag: JJ --- F1: 0.8381267189979553\n",
            "Tag: PDT --- F1: 0.0\n",
            "Tag: WP$ --- F1: 0.0\n",
            "Tag: # --- F1: 0.0\n",
            "Tag: RBR --- F1: 0.47392016649246216\n",
            "Tag: TO --- F1: 0.9986009001731873\n",
            "Tag: LS --- F1: 0.0\n",
            "Tag: VBP --- F1: 0.8841896057128906\n",
            "Tag: $ --- F1: 0.9943693280220032\n",
            "Tag: MD --- F1: 0.9861377477645874\n",
            "Tag: VBG --- F1: 0.7655966877937317\n",
            "Tag: CC --- F1: 0.9887946844100952\n",
            "Tag: JJR --- F1: 0.4759712517261505\n",
            "Tag: NNPS --- F1: 0.1272321343421936\n",
            "Tag: VBD --- F1: 0.9185523390769958\n",
            "Tag: FW --- F1: 0.0\n",
            "Tag: NNP --- F1: 0.9200152158737183\n",
            "Tag: RP --- F1: 0.7481581568717957\n",
            "Tag: DT --- F1: 0.9856942296028137\n",
            "Tag: VBN --- F1: 0.8247246742248535\n",
            "Tag: POS --- F1: 0.9952570199966431\n",
            "Tag: JJS --- F1: 0.26200395822525024\n",
            "Tag: -RRB- --- F1: 0.46458327770233154\n",
            "Tag: RBS --- F1: 0.0\n",
            "Tag: SYM --- F1: 0.0\n",
            "Tag: WRB --- F1: 0.8558622002601624\n",
            "Tag: PRP --- F1: 0.9811492562294006\n",
            "Tag: -LRB- --- F1: 0.42619043588638306\n",
            "Tag: NN --- F1: 0.9051370620727539\n",
            "Tag: WDT --- F1: 0.9751948118209839\n",
            "Tag: NNS --- F1: 0.92575603723526\n",
            "Tag: PRP$ --- F1: 0.9957131147384644\n",
            "Tag: WP --- F1: 0.8971786499023438\n",
            "Tag: EX --- F1: 0.9425324201583862\n",
            "Tag: IN --- F1: 0.9721957445144653\n",
            "Tag: VBZ --- F1: 0.9580621123313904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- Val_F1: {}'.format(index2tag[i], f1_val_list[i]))"
      ],
      "metadata": {
        "id": "RvuwBoaHtV6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c807bb3-5724-4d1b-e06f-a43e91e9a8ca"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: UH --- Val_F1: 0.0\n",
            "Tag: CD --- Val_F1: 0.927638828754425\n",
            "Tag: VB --- Val_F1: 0.8931081295013428\n",
            "Tag: RB --- Val_F1: 0.7643499970436096\n",
            "Tag: JJ --- Val_F1: 0.696648359298706\n",
            "Tag: PDT --- Val_F1: 0.0\n",
            "Tag: WP$ --- Val_F1: 0.0\n",
            "Tag: # --- Val_F1: 0.0\n",
            "Tag: RBR --- Val_F1: 0.3813852369785309\n",
            "Tag: TO --- Val_F1: 1.0\n",
            "Tag: LS --- Val_F1: 0.0\n",
            "Tag: VBP --- Val_F1: 0.8183384537696838\n",
            "Tag: $ --- Val_F1: 1.0\n",
            "Tag: MD --- Val_F1: 0.9702140688896179\n",
            "Tag: VBG --- Val_F1: 0.6363258957862854\n",
            "Tag: CC --- Val_F1: 0.9814064502716064\n",
            "Tag: JJR --- Val_F1: 0.2444600909948349\n",
            "Tag: NNPS --- Val_F1: 0.0\n",
            "Tag: VBD --- Val_F1: 0.8388074636459351\n",
            "Tag: FW --- Val_F1: 0.0\n",
            "Tag: NNP --- Val_F1: 0.830858051776886\n",
            "Tag: RP --- Val_F1: 0.3737761676311493\n",
            "Tag: DT --- Val_F1: 0.984567403793335\n",
            "Tag: VBN --- Val_F1: 0.652423620223999\n",
            "Tag: POS --- Val_F1: 0.9936438202857971\n",
            "Tag: JJS --- Val_F1: 0.0998622477054596\n",
            "Tag: -RRB- --- Val_F1: 0.23448771238327026\n",
            "Tag: RBS --- Val_F1: 0.0\n",
            "Tag: SYM --- Val_F1: 0.0\n",
            "Tag: WRB --- Val_F1: 0.7640314698219299\n",
            "Tag: PRP --- Val_F1: 0.9717600345611572\n",
            "Tag: -LRB- --- Val_F1: 0.2560606002807617\n",
            "Tag: NN --- Val_F1: 0.8401126861572266\n",
            "Tag: WDT --- Val_F1: 0.8919130563735962\n",
            "Tag: NNS --- Val_F1: 0.8579208254814148\n",
            "Tag: PRP$ --- Val_F1: 0.9955922961235046\n",
            "Tag: WP --- Val_F1: 0.6986796855926514\n",
            "Tag: EX --- Val_F1: 0.7112554311752319\n",
            "Tag: IN --- Val_F1: 0.9590938091278076\n",
            "Tag: VBZ --- Val_F1: 0.9268388152122498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Infos"
      ],
      "metadata": {
        "id": "eWilJpbetXIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#40 epochs, batch 128"
      ],
      "metadata": {
        "id": "f41IGmkWtZz2"
      }
    }
  ]
}