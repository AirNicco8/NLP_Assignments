{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_DENSE_biLSTM256_DENSE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Due to**: 23/12/2021 (dd/mm/yyyy)\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Part-of Speech (POS) tagging as Sequence Labelling using Recurrent Neural Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4_wqPdlBcKS"
      },
      "source": [
        "# Intro\n",
        "\n",
        "In this assignment  we will ask you to perform POS tagging using neural architectures\n",
        "\n",
        "You are asked to follow these steps:\n",
        "*   Download the corpora and split it in training and test sets, structuring a dataframe.\n",
        "*   Embed the words using GloVe embeddings\n",
        "*   Create a baseline model, using a simple neural architecture\n",
        "*   Experiment doing small modifications to the baseline model, choose hyperparameters using the validation set\n",
        "*   Evaluate your two best model\n",
        "*   Analyze the errors of your model\n",
        "\n",
        "\n",
        "**Task**: given a corpus of documents, predict the POS tag for each word\n",
        "\n",
        "**Corpus**:\n",
        "Ignore the numeric value in the third column, use only the words/symbols and its label. \n",
        "The corpus is available at:\n",
        "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
        "\n",
        "**Splits**: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
        "\n",
        "\n",
        "**Features**: you MUST use GloVe embeddings as the only input features to the model.\n",
        "\n",
        "**Splitting**: you can decide to split documents into sentences or not, the choice is yours.\n",
        "\n",
        "**I/O structure**: The input data will have three dimensions: 1-documents/sentences, 2-token, 3-features; for the output there are 2 possibilities: if you use one-hot encoding it will be 1-documents/sentences, 2-token labels, 3-classes, if you use a single integer that indicates the number of the class it will be 1-documents/sentences, 2-token labels.\n",
        "\n",
        "**Baseline**: two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top; the choice of hyper-parameters is yours.\n",
        "\n",
        "**Architectures**: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and adding an additional dense layer; do not mix these variantions.\n",
        "\n",
        "\n",
        "**Training and Experiments**: all the experiments must involve only the training and validation sets.\n",
        "\n",
        "**Evaluation**: in the end, only the two best models of your choice (according to the validation set) must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech. DO NOT CONSIDER THE PUNCTUATION CLASSES.\n",
        "\n",
        "**Metrics**: the metric you must use to evaluate your final model is the F1-macro, WITHOUT considering punctuation/symbols classes; during the training process you can use accuracy because you can't use the F1 metric unless you use a single (gigantic) batch because there is no way to aggregate \"partial\" F1 scores computed on mini-batches.\n",
        "\n",
        "**Discussion and Error Analysis** : verify and discuss if the results on the test sets are coherent with those on the validation set; analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
        "\n",
        "**Report**: you are asked to deliver the code of your experiments and a small pdf report of about 2 pages; the pdf must begin with the names of the people of your team and a small abstract (4-5 lines) that sums up your findings.\n",
        "\n",
        "# Out Of Vocabulary (OOV) terms\n",
        "\n",
        "How to handle words that are not in GloVe vocabulary?\n",
        "You can handle them as you want (random embedding, placeholder, whatever!), but they must be STATIC embeddings (you cannot train them).\n",
        "\n",
        "But there is a very important caveat! As usual, the element of the test set must not influence the elements of the other splits!\n",
        "\n",
        "So, when you compute new embeddings for train+validation, you must forget about test documents.\n",
        "The motivation is to emulate a real-world scenario, where you select and train a model in the first stage, without knowing nothing about the testing environment.\n",
        "\n",
        "For implementation convenience, you CAN use a single vocabulary file/matrix/whatever. The principle of the previous point is that the embeddings inside that file/matrix must be generated independently for train and test splits.\n",
        "\n",
        "Basically in a real-world scenario, this is what would happen:\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Training of the model(s)\n",
        "5. Compute embeddings for terms OOV2 of the validation split \n",
        "6. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "7. Validation of the model(s)\n",
        "8. Compute embeddings for terms OOV3 of the test split \n",
        "9. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2+OOV3\n",
        "10. Testing of the final model\n",
        "\n",
        "In this case, where we already have all the documents, we can simplify the process a bit, but the procedure must remain rigorous.\n",
        "\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Compute embeddings for terms OOV2 of the validation split \n",
        "5. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "6. Compute embeddings for terms OOV3 of the test split \n",
        "7. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2\n",
        "8. Training of the model(s)\n",
        "9. Validation of the model(s)\n",
        "10. Testing of the final model\n",
        "\n",
        "Step 2 and step 6 must be completely independent of each other, for what concerns the method and the documents. But they can rely on the previous vocabulary (V1 for step 2 and V3 for step 6)\n",
        "THEREFORE if a word is present both in the training set and the test split and not in the starting vocabulary, its embedding is computed in step 2) and it is not considered OOV anymore in step 6).\n",
        "\n",
        "# Report\n",
        "The report must not be just a copy and paste of graphs and tables!\n",
        "\n",
        "The report must not be longer than 2 pages and must contain:\n",
        "* The names of the member of your team\n",
        "* A short abstract (4-5 lines) that sum ups everything\n",
        "* A general description of the task you have addressed and how you have addressed it\n",
        "* A short description of the models you have used\n",
        "* Some tables that sum up your findings in validation and test and a discussion of those results\n",
        "* The most relevant findings of your error analysis\n",
        "\n",
        "# Evaluation Criterion\n",
        "\n",
        "The goal of this assignment is not to prove you can find best model ever, but to face a common task, structure it correctly, and follow a correct and rigorous experimental procedure.\n",
        "In other words, we don't care if you final models are awful as long as you have followed the correct procedure and wrote a decent report.\n",
        "\n",
        "The score of the assignment will be computed roughly as follows\n",
        "* 1 point for the general setting of the problem\n",
        "* 1 point for the handling of OOV terms\n",
        "* 1 point for the models\n",
        "* 1 point for train-validation-test procedure\n",
        "* 2 point for the discussion of the results, error analysis, and report\n",
        "\n",
        "This distribution of scores is tentative and we may decide to alter it at any moment.\n",
        "We also reserve the right to assign a small bonus (0.5 points) to any assignment that is particularly worthy. Similarly, in case of grave errors, we may decide to assign an equivalent malus (-0.5 points).\n",
        "\n",
        "# Contacts\n",
        "\n",
        "In case of any doubt, question, issue, or help we highly recommend you to check the [course useful material](https://virtuale.unibo.it/pluginfile.php/1036039/mod_resource/content/2/NLP_Course_Useful_Material.pdf) for additional information, and to use the Virtuale forums to discuss with other students.\n",
        "\n",
        "You can always contact us at the following email addresses. To increase the probability of a prompt response, we reccomend you to write to both the teaching assistants.\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Andrea Galassi -> a.galassi@unibo.it\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it\n",
        "\n",
        "\n",
        "# FAQ\n",
        "* You can use a non-trainable Embedding layer to load the glove embeddings\n",
        "* You can use any library of your choice to implement the networks. Two options are tensorflow/keras or pythorch. Both these libraries have all the classes you need to implement these simple architectures and there are plenty of tutorials around, where you can learn how to use them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emdubW66fnlD"
      },
      "source": [
        "import os, shutil  #  file management\n",
        "import sys \n",
        "import pandas as pd  #  dataframe management\n",
        "import numpy as np  #  data manipulation\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request  #  download files\n",
        "import zipfile  #  unzip files\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B-w2pqyfpF1"
      },
      "source": [
        "from keras import metrics \n",
        "from tensorflow.python.keras.metrics import Metric\n",
        "from sklearn.metrics import f1_score\n",
        "import keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb5zPs1Obr80",
        "outputId": "e0864e73-0874-483c-f72e-1ba8b0cae893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3xn1MZPgA_v",
        "outputId": "8f0fc9f1-8151-49e5-86ae-f0d3b67ad3ee"
      },
      "source": [
        "dataset_folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\")\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "    os.makedirs(dataset_folder)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
        "\n",
        "dataset_path = os.path.join(dataset_folder, \"treebank.zip\")\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    urllib.request.urlretrieve(url, dataset_path)\n",
        "    print(\"Successful download\")\n",
        "\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_folder)\n",
        "print(\"Successful extraction\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful download\n",
            "Successful extraction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pre Processing\n",
        "-splitting"
      ],
      "metadata": {
        "id": "FGmh5a8wbuNA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKbUsrVHhGAG"
      },
      "source": [
        "dataset_name = \"dependency_treebank\"\n",
        "\n",
        "\n",
        "folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\", dataset_name)\n",
        "\n",
        "pre_train = []\n",
        "pre_valid = []\n",
        "pre_test = []\n",
        "i = 1\n",
        "\n",
        "file_list = sorted(os.listdir(folder))\n",
        "\n",
        "for filename in file_list:\n",
        "  file_path = os.path.join(folder, filename)\n",
        "  if os.path.isfile(file_path):\n",
        "    # open the file\n",
        "      text = []\n",
        "      with open(file_path, mode='r', encoding='utf-8') as text_file:\n",
        "        text = text_file.read()\n",
        "        if i <= 100:\n",
        "          pre_train.append(text)\n",
        "        elif i <= 150:\n",
        "          pre_valid.append(text)\n",
        "        else:\n",
        "          pre_test.append(text)  \n",
        "  i+=1\n",
        "\n",
        "tr = []\n",
        "val = []\n",
        "tes = []\n",
        "\n",
        "for paragraph in pre_train:\n",
        "   tr.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_valid:\n",
        "   val.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_test:\n",
        "   tes.append(paragraph.split('\\n'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUgbTXi-F7Sa"
      },
      "source": [
        "train = []\n",
        "valid = []\n",
        "test = []\n",
        "\n",
        "for i in tr:\n",
        "  for j in i:\n",
        "    train.append(j.split('\\t'))\n",
        "\n",
        "for i in val:\n",
        "  for j in i:\n",
        "    valid.append(j.split('\\t'))\n",
        "\n",
        "for i in tes:\n",
        "  for j in i:\n",
        "    test.append(j.split('\\t'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmzVaRuQFp0M"
      },
      "source": [
        "train_sentences = []\n",
        "valid_sentences = []\n",
        "test_sentences = []\n",
        "train_tags = []\n",
        "valid_tags = []\n",
        "test_tags = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in train:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    train_sentences.append(s)\n",
        "    train_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in valid:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    valid_sentences.append(s)\n",
        "    valid_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in test:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    test_sentences.append(s)\n",
        "    test_tags.append(t)\n",
        "    s = []\n",
        "    t = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB65cUFLIeEr"
      },
      "source": [
        "flat_train = [item for sublist in train_sentences for item in sublist]\n",
        "flat_valid = [item for sublist in valid_sentences for item in sublist]\n",
        "flat_test = [item for sublist in test_sentences for item in sublist]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "- vocabularies\n",
        "- GloVe download"
      ],
      "metadata": {
        "id": "xRBn8mYlb9Gg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZO6ga1EGHdh"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "train_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "train_tokenizer.fit_on_texts(train_sentences)                    # fit tokeniser on data\n",
        "train_encoded = train_tokenizer.texts_to_sequences(train_sentences)\n",
        "\n",
        "valid_tokenizer = Tokenizer()       \n",
        "valid_tokenizer.fit_on_texts(valid_sentences)                  # instantiate tokeniser\n",
        "valid_encoded = valid_tokenizer.texts_to_sequences(valid_sentences)\n",
        "\n",
        "test_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "test_tokenizer.fit_on_texts(test_sentences)                    # fit tokeniser on data\n",
        "test_encoded = test_tokenizer.texts_to_sequences(test_sentences)  # use the tokeniser to encode input sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(train_tags)\n",
        "tags_encoded = tag_tokenizer.texts_to_sequences(train_tags)"
      ],
      "metadata": {
        "id": "H0DhjtKRcE-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMsG0Yw1l50L",
        "outputId": "c6dd82aa-4ea4-4399-b4cf-4128cfd4d8c1"
      },
      "source": [
        "voc = list(set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(valid_tokenizer.word_index.keys()) - set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(test_tokenizer.word_index.keys()) - set(voc))\n",
        "print(len(voc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7404\n",
            "9901\n",
            "10947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS9RwnOtJuas"
      },
      "source": [
        "word_index = dict(zip(voc, range(2, len(voc)+3)))\n",
        "word_index['-PAD-'] = 0\n",
        "word_index['-OOV-'] = 1\n",
        "\n",
        "tags = set([item for sublist in train_tags for item in sublist])\n",
        "\n",
        "tag2index = {t: i + 1 for i, t in enumerate(tags)}\n",
        "tag2index['-PAD-'] = 0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoj4Zx51J2MU",
        "outputId": "7f75deb0-5f1c-4238-db91-fbfd84c793aa"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-14 10:50:48--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-12-14 10:50:48--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-12-14 10:50:49--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 40s  \n",
            "\n",
            "2021-12-14 10:53:29 (5.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc-T6FcLKtS9",
        "outputId": "e87c649a-eb41-46c7-c5fc-a4a442528501"
      },
      "source": [
        "path_to_glove_file = os.path.join(os.getcwd(), \"glove.6B.100d.txt\")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding\n",
        "- matrix creation\n",
        "- padding\n",
        "- one hot encoding of tags\n",
        "- punctuation identification"
      ],
      "metadata": {
        "id": "dGZI_bM2cMg5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0mOStEPMeXt",
        "outputId": "c5ce5119-01a7-47ea-e5d8-be9553883ab8"
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        embedding_matrix += np.random.uniform(low=-0.05, high=0.05, size=embedding_dim)\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 10271 words (678 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpYrHs0RMtQJ"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8EZM31Js06-",
        "outputId": "abdf3795-6972-4a17-f10d-5328f82dcdb7"
      },
      "source": [
        "MAX_LENGTH = len(max(train_sentences, key=len))\n",
        "print(MAX_LENGTH)\n",
        "MAX_LENGTH2 = len(max(valid_sentences, key=len))\n",
        "print(MAX_LENGTH2)\n",
        "MAX_LENGTH3 = len(max(test_sentences, key=len))\n",
        "print(MAX_LENGTH3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "249\n",
            "81\n",
            "58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSRa9SwTuNrX"
      },
      "source": [
        "train_sentences_X, valid_sentences_X, test_sentences_X, train_tags_y, valid_tags_y, test_tags_y = [], [], [], [], [], []\n",
        " \n",
        "for s in train_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        "\n",
        "for s in valid_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    valid_sentences_X.append(s_int)\n",
        "\n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        " \n",
        "for s in train_tags:\n",
        "    train_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "for s in valid_tags:\n",
        "    valid_tags_y.append([tag2index[t] for t in s])\n",
        " \n",
        "for s in test_tags:\n",
        "    test_tags_y.append([tag2index[t] for t in s])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf-XlSrStQx3"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_sentences_X = pad_sequences(valid_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_tags_y = pad_sequences(valid_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)"
      ],
      "metadata": {
        "id": "jcA7Ejz0ilPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "point = [tag2index['.']]\n",
        "virg = [tag2index[',']]\n",
        "weird_apex = [tag2index['``']]\n",
        "single_apex = [tag2index[\"''\"]]\n",
        "two_dots = [tag2index[':']]\n",
        "pad = [tag2index['-PAD-']]\n",
        "\n",
        "punct_cat_classes = to_categorical([point, virg, weird_apex, single_apex, two_dots, pad], len(tag2index))\n",
        "punct_cat_classes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CuCZIWMintJ",
        "outputId": "72b22b1a-6c3a-44c8-98cd-3197368f7cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 1, 46)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cum_tags = np.zeros(46)\n",
        "for i in punct_cat_classes:\n",
        "  print(i[0])\n",
        "  cum_tags += i[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Yc4Ux9tiphn",
        "outputId": "57cc9387-5128-4205-e7f7-e471790130af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "where_tags = np.where(np.logical_not(cum_tags))\n",
        "where_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqBBdCtkirGS",
        "outputId": "88e84d48-54b0-4a14-9543-00a7746d9ba7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 1,  2,  3,  4,  6,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
              "        21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
              "        39, 40, 41, 42, 43, 44]),)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_punct_indexes = where_tags[0]\n",
        "n_classes = len(no_punct_indexes)"
      ],
      "metadata": {
        "id": "yENX6U7liseF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "cat_val_tags_y = to_categorical(valid_tags_y, len(tag2index))\n",
        "print(len(cat_train_tags_y), len(cat_val_tags_y))\n",
        "print(len(train_sentences_X), len(valid_sentences_X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNiP8hAyitxi",
        "outputId": "c2237ef2-af05-4ea8-cac4-da6d824ef531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1963 1299\n",
            "1963 1299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mini Data Vizualization Inset"
      ],
      "metadata": {
        "id": "K5lzRel5ivPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_tags = np.zeros(46)\n",
        "for i in cat_train_tags_y:\n",
        "  for t in i:\n",
        "    distribution_tags += t"
      ],
      "metadata": {
        "id": "58RfD-doi0Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = sum(distribution_tags[1:])\n",
        "norm = [float(i)/s for i in distribution_tags[1:]]"
      ],
      "metadata": {
        "id": "UnbSgY5Ci12w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt "
      ],
      "metadata": {
        "id": "2XCdkzLvi5XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(np.arange(len(norm)),norm)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "Gsox3mLci6hS",
        "outputId": "10772023-109c-4012-d191-68a51262f9a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#F1 Metric"
      ],
      "metadata": {
        "id": "YhqvaAf2i76D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric.\n",
        "\n",
        "    Only computes a batch-wise average of precision.\n",
        "\n",
        "    Computes the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric.\n",
        "\n",
        "    Only computes a batch-wise average of recall.\n",
        "\n",
        "    Computes the recall, a metric for multi-label classification of\n",
        "    how many relevant items are selected.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def f1_binary(y_true, y_pred):\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
        "\n",
        "def metric_wrapper(index):\n",
        "  def f1_for_class(y_true, y_pred):\n",
        "    #get only the desired class\n",
        "    true = y_true[:,:,index]\n",
        "    pred = y_pred[:,:,index]\n",
        "    #return dice per class\n",
        "    tmp = f1_binary(true,pred)\n",
        "    return tmp\n",
        "  f1_for_class.__name__ = 'f1_' + str(index)\n",
        "  return f1_for_class\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    result = 0.0\n",
        "    for class_id in no_punct_indexes:\n",
        "        y_true_single_class = y_true[:,:,class_id]\n",
        "        y_pred_single_class = y_pred[:,:,class_id]\n",
        "        f1_single = f1_binary(y_true_single_class, y_pred_single_class)\n",
        "        result += f1_single / float(n_classes)\n",
        "    return result"
      ],
      "metadata": {
        "id": "CMEBX3OFi_SB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model compile and fit"
      ],
      "metadata": {
        "id": "Nd-5r3b1jEvg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsVGByW_sdec",
        "outputId": "3e409b80-b642-4cf0-9a33-7548be236fa5"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(embedding_layer)\n",
        "model.add(Dense(len(tag2index)))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(Dense(len(tag2index)))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=Adam(0.001),\n",
        "             metrics=['accuracy', f1, [metric_wrapper(i) for i in no_punct_indexes]])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 249, 100)          1094900   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 249, 46)           4646      \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 249, 512)         620544    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 249, 46)           23598     \n",
            "                                                                 \n",
            " activation (Activation)     (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,743,688\n",
            "Trainable params: 648,788\n",
            "Non-trainable params: 1,094,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "history = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=40, validation_data=(valid_sentences_X, cat_val_tags_y), callbacks=[callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqWLhASkjQEc",
        "outputId": "7c19f0f1-a492-4bb6-f75b-59864a685a24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "16/16 [==============================] - 103s 6s/step - loss: 0.8040 - accuracy: 0.8457 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - val_loss: 0.3482 - val_accuracy: 0.9159 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - 81s 5s/step - loss: 0.3145 - accuracy: 0.9169 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - val_loss: 0.2930 - val_accuracy: 0.9198 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.2850 - accuracy: 0.9231 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - val_loss: 0.2762 - val_accuracy: 0.9277 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.2693 - accuracy: 0.9299 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - val_loss: 0.2630 - val_accuracy: 0.9319 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.2549 - accuracy: 0.9364 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - val_loss: 0.2485 - val_accuracy: 0.9386 - val_f1: 1.9397e-04 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0078 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.2383 - accuracy: 0.9415 - f1: 0.0019 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0776 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - val_loss: 0.2306 - val_accuracy: 0.9428 - val_f1: 0.0020 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0791 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - 82s 5s/step - loss: 0.2190 - accuracy: 0.9447 - f1: 0.0085 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0666 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.2742 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - val_loss: 0.2102 - val_accuracy: 0.9464 - val_f1: 0.0129 - val_f1_1: 6.7092e-04 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.2111 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.3035 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - 78s 5s/step - loss: 0.1978 - accuracy: 0.9489 - f1: 0.0239 - f1_1: 0.1791 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.3342 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.4359 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0055 - f1_44: 0.0000e+00 - val_loss: 0.1892 - val_accuracy: 0.9512 - val_f1: 0.0367 - val_f1_1: 0.5070 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.4574 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0080 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.4793 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0144 - val_f1_44: 0.0000e+00\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.1773 - accuracy: 0.9534 - f1: 0.0473 - f1_1: 0.7033 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.5652 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0095 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0032 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.5257 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0868 - f1_44: 0.0000e+00 - val_loss: 0.1695 - val_accuracy: 0.9551 - val_f1: 0.0585 - val_f1_1: 0.8483 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.6440 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.1138 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0054 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0364 - val_f1_39: 0.0000e+00 - val_f1_40: 0.4994 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.1924 - val_f1_44: 0.0000e+00\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - 82s 5s/step - loss: 0.1586 - accuracy: 0.9579 - f1: 0.0767 - f1_1: 0.8593 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_6: 0.0223 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.7009 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.2075 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0254 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.3824 - f1_39: 0.0000e+00 - f1_40: 0.5855 - f1_41: 0.0055 - f1_42: 0.0000e+00 - f1_43: 0.2777 - f1_44: 0.0000e+00 - val_loss: 0.1537 - val_accuracy: 0.9592 - val_f1: 0.0985 - val_f1_1: 0.8775 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0235 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.7373 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.4662 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0856 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.8412 - val_f1_39: 0.0000e+00 - val_f1_40: 0.5989 - val_f1_41: 0.0017 - val_f1_42: 0.0000e+00 - val_f1_43: 0.3076 - val_f1_44: 0.0000e+00\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.1434 - accuracy: 0.9620 - f1: 0.1129 - f1_1: 0.9017 - f1_2: 0.0000e+00 - f1_3: 0.1138 - f1_4: 0.0000e+00 - f1_6: 0.2374 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.7815 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.4195 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.1140 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.8882 - f1_39: 0.0000e+00 - f1_40: 0.6322 - f1_41: 0.0268 - f1_42: 0.0000e+00 - f1_43: 0.4027 - f1_44: 0.0000e+00 - val_loss: 0.1405 - val_accuracy: 0.9633 - val_f1: 0.1301 - val_f1_1: 0.9164 - val_f1_2: 0.0000e+00 - val_f1_3: 0.3814 - val_f1_4: 0.0000e+00 - val_f1_6: 0.4018 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.7585 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.5620 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.1822 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0022 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.9892 - val_f1_39: 0.0000e+00 - val_f1_40: 0.6221 - val_f1_41: 0.0435 - val_f1_42: 0.0000e+00 - val_f1_43: 0.3459 - val_f1_44: 0.0000e+00\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.1306 - accuracy: 0.9656 - f1: 0.1511 - f1_1: 0.9213 - f1_2: 0.0000e+00 - f1_3: 0.7671 - f1_4: 0.0000e+00 - f1_6: 0.5280 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.8161 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.5233 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.2462 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0124 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.9870 - f1_39: 0.0000e+00 - f1_40: 0.6555 - f1_41: 0.1135 - f1_42: 0.0000e+00 - f1_43: 0.4743 - f1_44: 0.0000e+00 - val_loss: 0.1285 - val_accuracy: 0.9659 - val_f1: 0.1580 - val_f1_1: 0.9268 - val_f1_2: 0.0000e+00 - val_f1_3: 0.8751 - val_f1_4: 0.0000e+00 - val_f1_6: 0.6005 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.8046 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.5856 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.2485 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0427 - val_f1_30: 0.0076 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.9972 - val_f1_39: 0.0000e+00 - val_f1_40: 0.6334 - val_f1_41: 0.1014 - val_f1_42: 0.0000e+00 - val_f1_43: 0.4965 - val_f1_44: 0.0000e+00\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.1198 - accuracy: 0.9683 - f1: 0.1761 - f1_1: 0.9283 - f1_2: 0.0000e+00 - f1_3: 0.9144 - f1_4: 0.0000e+00 - f1_6: 0.7298 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.8403 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.1421 - f1_17: 0.5804 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0156 - f1_23: 0.3378 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.1180 - f1_30: 0.0420 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.9972 - f1_39: 0.0000e+00 - f1_40: 0.6789 - f1_41: 0.1807 - f1_42: 0.0000e+00 - f1_43: 0.5374 - f1_44: 0.0000e+00 - val_loss: 0.1189 - val_accuracy: 0.9680 - val_f1: 0.1979 - val_f1_1: 0.9312 - val_f1_2: 0.0000e+00 - val_f1_3: 0.9653 - val_f1_4: 0.0000e+00 - val_f1_6: 0.7194 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.8352 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.6528 - val_f1_17: 0.6489 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.1168 - val_f1_23: 0.3670 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.2332 - val_f1_30: 0.1174 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.9995 - val_f1_39: 0.0000e+00 - val_f1_40: 0.6334 - val_f1_41: 0.1251 - val_f1_42: 0.0000e+00 - val_f1_43: 0.5701 - val_f1_44: 0.0000e+00\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.1110 - accuracy: 0.9705 - f1: 0.2145 - f1_1: 0.9331 - f1_2: 0.0000e+00 - f1_3: 0.9765 - f1_4: 0.0000e+00 - f1_6: 0.7843 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.8541 - f1_13: 0.0037 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.7407 - f1_17: 0.6332 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.1839 - f1_23: 0.4404 - f1_24: 0.0044 - f1_25: 0.0000e+00 - f1_26: 0.0098 - f1_27: 0.0000e+00 - f1_28: 0.3076 - f1_30: 0.2085 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.9971 - f1_39: 0.0000e+00 - f1_40: 0.6946 - f1_41: 0.2400 - f1_42: 0.0000e+00 - f1_43: 0.5674 - f1_44: 0.0000e+00 - val_loss: 0.1111 - val_accuracy: 0.9700 - val_f1: 0.2244 - val_f1_1: 0.9364 - val_f1_2: 0.0000e+00 - val_f1_3: 0.9880 - val_f1_4: 0.0000e+00 - val_f1_6: 0.7649 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.8437 - val_f1_13: 0.0034 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.8435 - val_f1_17: 0.7100 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.4130 - val_f1_23: 0.4077 - val_f1_24: 0.0057 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0151 - val_f1_27: 0.0000e+00 - val_f1_28: 0.4252 - val_f1_30: 0.2085 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.0000e+00 - val_f1_40: 0.6529 - val_f1_41: 0.1723 - val_f1_42: 0.0000e+00 - val_f1_43: 0.5860 - val_f1_44: 0.0000e+00\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.1035 - accuracy: 0.9724 - f1: 0.2424 - f1_1: 0.9350 - f1_2: 0.0000e+00 - f1_3: 0.9851 - f1_4: 0.0000e+00 - f1_6: 0.8176 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.8645 - f1_13: 0.0100 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.8845 - f1_17: 0.6725 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.4826 - f1_23: 0.4858 - f1_24: 0.0712 - f1_25: 0.0000e+00 - f1_26: 0.0923 - f1_27: 0.0000e+00 - f1_28: 0.4598 - f1_30: 0.3367 - f1_31: 0.0037 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.9985 - f1_39: 0.0000e+00 - f1_40: 0.7028 - f1_41: 0.3009 - f1_42: 0.0000e+00 - f1_43: 0.5920 - f1_44: 0.0000e+00 - val_loss: 0.1043 - val_accuracy: 0.9714 - val_f1: 0.2465 - val_f1_1: 0.9394 - val_f1_2: 0.0000e+00 - val_f1_3: 0.9952 - val_f1_4: 0.0000e+00 - val_f1_6: 0.7748 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.8589 - val_f1_13: 0.0120 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.9146 - val_f1_17: 0.7276 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.5163 - val_f1_23: 0.4733 - val_f1_24: 0.1250 - val_f1_25: 0.0000e+00 - val_f1_26: 0.1314 - val_f1_27: 0.0000e+00 - val_f1_28: 0.4524 - val_f1_30: 0.3832 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.0000e+00 - val_f1_40: 0.6563 - val_f1_41: 0.2787 - val_f1_42: 0.0000e+00 - val_f1_43: 0.6200 - val_f1_44: 0.0000e+00\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - 74s 5s/step - loss: 0.0970 - accuracy: 0.9740 - f1: 0.2666 - f1_1: 0.9373 - f1_2: 0.0000e+00 - f1_3: 0.9925 - f1_4: 0.0000e+00 - f1_6: 0.8269 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.8748 - f1_13: 0.0600 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.9206 - f1_17: 0.7078 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.6038 - f1_23: 0.5302 - f1_24: 0.2328 - f1_25: 0.0000e+00 - f1_26: 0.2124 - f1_27: 0.0000e+00 - f1_28: 0.5626 - f1_30: 0.4955 - f1_31: 0.0185 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.9984 - f1_39: 0.0000e+00 - f1_40: 0.7174 - f1_41: 0.3476 - f1_42: 0.0000e+00 - f1_43: 0.6232 - f1_44: 0.0000e+00 - val_loss: 0.0983 - val_accuracy: 0.9731 - val_f1: 0.2665 - val_f1_1: 0.9397 - val_f1_2: 0.0000e+00 - val_f1_3: 0.9975 - val_f1_4: 0.0000e+00 - val_f1_6: 0.7759 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.8674 - val_f1_13: 0.0581 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.9211 - val_f1_17: 0.7360 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.6806 - val_f1_23: 0.4863 - val_f1_24: 0.1806 - val_f1_25: 0.0000e+00 - val_f1_26: 0.2180 - val_f1_27: 0.0000e+00 - val_f1_28: 0.5487 - val_f1_30: 0.6327 - val_f1_31: 0.0242 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0038 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.0000e+00 - val_f1_40: 0.6774 - val_f1_41: 0.2799 - val_f1_42: 0.0043 - val_f1_43: 0.6291 - val_f1_44: 0.0000e+00\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0914 - accuracy: 0.9752 - f1: 0.2839 - f1_1: 0.9414 - f1_2: 0.0000e+00 - f1_3: 0.9937 - f1_4: 0.0000e+00 - f1_6: 0.8363 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.8837 - f1_13: 0.0685 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.9388 - f1_17: 0.7198 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.7011 - f1_23: 0.5706 - f1_24: 0.3237 - f1_25: 0.0000e+00 - f1_26: 0.3329 - f1_27: 0.0000e+00 - f1_28: 0.6119 - f1_30: 0.5759 - f1_31: 0.0497 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0345 - f1_37: 0.0000e+00 - f1_38: 0.9986 - f1_39: 0.0000e+00 - f1_40: 0.7342 - f1_41: 0.3733 - f1_42: 0.0300 - f1_43: 0.6364 - f1_44: 0.0000e+00 - val_loss: 0.0933 - val_accuracy: 0.9741 - val_f1: 0.2867 - val_f1_1: 0.9417 - val_f1_2: 0.0000e+00 - val_f1_3: 0.9975 - val_f1_4: 0.0000e+00 - val_f1_6: 0.7970 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.8751 - val_f1_13: 0.0382 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.9326 - val_f1_17: 0.7517 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.7419 - val_f1_23: 0.5647 - val_f1_24: 0.4546 - val_f1_25: 0.0000e+00 - val_f1_26: 0.2674 - val_f1_27: 0.0000e+00 - val_f1_28: 0.6293 - val_f1_30: 0.6398 - val_f1_31: 0.0735 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0207 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.0000e+00 - val_f1_40: 0.6938 - val_f1_41: 0.3469 - val_f1_42: 0.0453 - val_f1_43: 0.6557 - val_f1_44: 0.0000e+00\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - 82s 5s/step - loss: 0.0865 - accuracy: 0.9767 - f1: 0.3041 - f1_1: 0.9420 - f1_2: 0.0000e+00 - f1_3: 0.9953 - f1_4: 0.0000e+00 - f1_6: 0.8296 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.8883 - f1_13: 0.1029 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.9326 - f1_17: 0.7431 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.7567 - f1_23: 0.5912 - f1_24: 0.4130 - f1_25: 0.0000e+00 - f1_26: 0.4254 - f1_27: 0.0000e+00 - f1_28: 0.6592 - f1_30: 0.6487 - f1_31: 0.1120 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.1263 - f1_37: 0.0000e+00 - f1_38: 0.9985 - f1_39: 0.0000e+00 - f1_40: 0.7378 - f1_41: 0.3983 - f1_42: 0.2009 - f1_43: 0.6607 - f1_44: 0.0000e+00 - val_loss: 0.0891 - val_accuracy: 0.9753 - val_f1: 0.3094 - val_f1_1: 0.9415 - val_f1_2: 0.0000e+00 - val_f1_3: 0.9975 - val_f1_4: 0.0000e+00 - val_f1_6: 0.8164 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.8781 - val_f1_13: 0.0597 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.9317 - val_f1_17: 0.7697 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.7821 - val_f1_23: 0.6131 - val_f1_24: 0.4894 - val_f1_25: 0.0000e+00 - val_f1_26: 0.3521 - val_f1_27: 0.0000e+00 - val_f1_28: 0.7039 - val_f1_30: 0.6793 - val_f1_31: 0.1321 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.3237 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.0000e+00 - val_f1_40: 0.7031 - val_f1_41: 0.4065 - val_f1_42: 0.1655 - val_f1_43: 0.6304 - val_f1_44: 0.0000e+00\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0822 - accuracy: 0.9778 - f1: 0.3279 - f1_1: 0.9462 - f1_2: 0.0000e+00 - f1_3: 0.9942 - f1_4: 0.0000e+00 - f1_6: 0.8585 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.8945 - f1_13: 0.1218 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.9391 - f1_17: 0.7558 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.7892 - f1_23: 0.6367 - f1_24: 0.4618 - f1_25: 0.0000e+00 - f1_26: 0.5297 - f1_27: 0.0000e+00 - f1_28: 0.6848 - f1_30: 0.7067 - f1_31: 0.1939 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.3983 - f1_37: 0.0000e+00 - f1_38: 0.9970 - f1_39: 0.0000e+00 - f1_40: 0.7555 - f1_41: 0.4394 - f1_42: 0.3346 - f1_43: 0.6790 - f1_44: 0.0000e+00 - val_loss: 0.0855 - val_accuracy: 0.9764 - val_f1: 0.3366 - val_f1_1: 0.9490 - val_f1_2: 0.0000e+00 - val_f1_3: 0.9975 - val_f1_4: 0.0000e+00 - val_f1_6: 0.8043 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.8822 - val_f1_13: 0.0979 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.9367 - val_f1_17: 0.7726 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.8146 - val_f1_23: 0.6336 - val_f1_24: 0.5180 - val_f1_25: 0.0000e+00 - val_f1_26: 0.5753 - val_f1_27: 0.0000e+00 - val_f1_28: 0.7035 - val_f1_30: 0.7664 - val_f1_31: 0.1815 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.4652 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.0000e+00 - val_f1_40: 0.7231 - val_f1_41: 0.4100 - val_f1_42: 0.6021 - val_f1_43: 0.6301 - val_f1_44: 0.0000e+00\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - 74s 5s/step - loss: 0.0784 - accuracy: 0.9787 - f1: 0.3501 - f1_1: 0.9452 - f1_2: 0.0000e+00 - f1_3: 0.9936 - f1_4: 0.0000e+00 - f1_6: 0.8664 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.8998 - f1_13: 0.1788 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.9356 - f1_17: 0.7687 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0211 - f1_22: 0.8134 - f1_23: 0.6593 - f1_24: 0.4692 - f1_25: 0.0000e+00 - f1_26: 0.6322 - f1_27: 0.0000e+00 - f1_28: 0.7192 - f1_30: 0.7539 - f1_31: 0.2513 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.5475 - f1_37: 0.0000e+00 - f1_38: 0.9986 - f1_39: 0.0000e+00 - f1_40: 0.7599 - f1_41: 0.4570 - f1_42: 0.6413 - f1_43: 0.6928 - f1_44: 0.0000e+00 - val_loss: 0.0819 - val_accuracy: 0.9774 - val_f1: 0.3490 - val_f1_1: 0.9497 - val_f1_2: 0.0000e+00 - val_f1_3: 0.9975 - val_f1_4: 0.0000e+00 - val_f1_6: 0.8310 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.8870 - val_f1_13: 0.0931 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.9365 - val_f1_17: 0.7855 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0273 - val_f1_22: 0.8323 - val_f1_23: 0.6428 - val_f1_24: 0.5955 - val_f1_25: 0.0000e+00 - val_f1_26: 0.5943 - val_f1_27: 0.0000e+00 - val_f1_28: 0.7340 - val_f1_30: 0.7983 - val_f1_31: 0.2432 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.5326 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.0000e+00 - val_f1_40: 0.7277 - val_f1_41: 0.3960 - val_f1_42: 0.6838 - val_f1_43: 0.6721 - val_f1_44: 0.0000e+00\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0751 - accuracy: 0.9795 - f1: 0.3638 - f1_1: 0.9492 - f1_2: 0.0000e+00 - f1_3: 0.9955 - f1_4: 0.0000e+00 - f1_6: 0.8789 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.9019 - f1_13: 0.2064 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.9318 - f1_17: 0.7840 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0189 - f1_22: 0.8358 - f1_23: 0.6829 - f1_24: 0.5185 - f1_25: 0.0000e+00 - f1_26: 0.6628 - f1_27: 0.0000e+00 - f1_28: 0.7338 - f1_30: 0.7909 - f1_31: 0.3194 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.6193 - f1_37: 0.0000e+00 - f1_38: 0.9979 - f1_39: 0.0000e+00 - f1_40: 0.7633 - f1_41: 0.4899 - f1_42: 0.7667 - f1_43: 0.7032 - f1_44: 0.0000e+00 - val_loss: 0.0792 - val_accuracy: 0.9779 - val_f1: 0.3596 - val_f1_1: 0.9511 - val_f1_2: 0.0000e+00 - val_f1_3: 0.9975 - val_f1_4: 0.0000e+00 - val_f1_6: 0.8633 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.9009 - val_f1_13: 0.1426 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.9365 - val_f1_17: 0.7919 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0696 - val_f1_22: 0.8452 - val_f1_23: 0.6510 - val_f1_24: 0.5763 - val_f1_25: 0.0000e+00 - val_f1_26: 0.6512 - val_f1_27: 0.0000e+00 - val_f1_28: 0.7467 - val_f1_30: 0.8269 - val_f1_31: 0.2911 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.5910 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.0000e+00 - val_f1_40: 0.7339 - val_f1_41: 0.3769 - val_f1_42: 0.7598 - val_f1_43: 0.6823 - val_f1_44: 0.0000e+00\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0720 - accuracy: 0.9804 - f1: 0.3753 - f1_1: 0.9504 - f1_2: 0.0000e+00 - f1_3: 0.9950 - f1_4: 0.0000e+00 - f1_6: 0.8891 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.9101 - f1_13: 0.2696 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.9338 - f1_17: 0.7969 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0515 - f1_22: 0.8531 - f1_23: 0.6997 - f1_24: 0.5434 - f1_25: 0.0000e+00 - f1_26: 0.7067 - f1_27: 0.0000e+00 - f1_28: 0.7385 - f1_30: 0.8230 - f1_31: 0.3755 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.6889 - f1_37: 0.0000e+00 - f1_38: 0.9985 - f1_39: 0.0000e+00 - f1_40: 0.7784 - f1_41: 0.4722 - f1_42: 0.8139 - f1_43: 0.7218 - f1_44: 0.0000e+00 - val_loss: 0.0760 - val_accuracy: 0.9785 - val_f1: 0.3693 - val_f1_1: 0.9513 - val_f1_2: 0.0000e+00 - val_f1_3: 0.9975 - val_f1_4: 0.0000e+00 - val_f1_6: 0.8557 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.8975 - val_f1_13: 0.1921 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.9365 - val_f1_17: 0.7994 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0593 - val_f1_22: 0.8581 - val_f1_23: 0.6122 - val_f1_24: 0.5896 - val_f1_25: 0.0000e+00 - val_f1_26: 0.6499 - val_f1_27: 0.0000e+00 - val_f1_28: 0.7396 - val_f1_30: 0.8205 - val_f1_31: 0.4099 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.6817 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.0000e+00 - val_f1_40: 0.7366 - val_f1_41: 0.4336 - val_f1_42: 0.8203 - val_f1_43: 0.7307 - val_f1_44: 0.0000e+00\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - 74s 5s/step - loss: 0.0694 - accuracy: 0.9810 - f1: 0.3858 - f1_1: 0.9525 - f1_2: 0.0000e+00 - f1_3: 0.9909 - f1_4: 0.0000e+00 - f1_6: 0.8974 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.9125 - f1_13: 0.2986 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.9309 - f1_17: 0.8046 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.1090 - f1_22: 0.8603 - f1_23: 0.6996 - f1_24: 0.5416 - f1_25: 0.0000e+00 - f1_26: 0.7270 - f1_27: 0.0000e+00 - f1_28: 0.7480 - f1_30: 0.8597 - f1_31: 0.4429 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.7547 - f1_37: 0.0000e+00 - f1_38: 0.9977 - f1_39: 0.0000e+00 - f1_40: 0.7838 - f1_41: 0.5147 - f1_42: 0.8769 - f1_43: 0.7268 - f1_44: 0.0000e+00 - val_loss: 0.0732 - val_accuracy: 0.9797 - val_f1: 0.3792 - val_f1_1: 0.9542 - val_f1_2: 0.0000e+00 - val_f1_3: 0.9975 - val_f1_4: 0.0000e+00 - val_f1_6: 0.8773 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.9026 - val_f1_13: 0.2909 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.9381 - val_f1_17: 0.8312 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0592 - val_f1_22: 0.8582 - val_f1_23: 0.7013 - val_f1_24: 0.6240 - val_f1_25: 0.0000e+00 - val_f1_26: 0.6956 - val_f1_27: 0.0000e+00 - val_f1_28: 0.7512 - val_f1_30: 0.8188 - val_f1_31: 0.3767 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.7269 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.0000e+00 - val_f1_40: 0.7332 - val_f1_41: 0.4811 - val_f1_42: 0.8208 - val_f1_43: 0.7280 - val_f1_44: 0.0000e+00\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - 78s 5s/step - loss: 0.0667 - accuracy: 0.9816 - f1: 0.3935 - f1_1: 0.9536 - f1_2: 0.0000e+00 - f1_3: 0.9934 - f1_4: 0.0000e+00 - f1_6: 0.9057 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.9176 - f1_13: 0.3377 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.9396 - f1_17: 0.8192 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0980 - f1_22: 0.8673 - f1_23: 0.7286 - f1_24: 0.5767 - f1_25: 0.0000e+00 - f1_26: 0.7552 - f1_27: 0.0000e+00 - f1_28: 0.7747 - f1_30: 0.8631 - f1_31: 0.4721 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.7820 - f1_37: 0.0000e+00 - f1_38: 0.9987 - f1_39: 0.0000e+00 - f1_40: 0.7983 - f1_41: 0.5184 - f1_42: 0.9034 - f1_43: 0.7370 - f1_44: 0.0000e+00 - val_loss: 0.0710 - val_accuracy: 0.9804 - val_f1: 0.3878 - val_f1_1: 0.9547 - val_f1_2: 0.0000e+00 - val_f1_3: 0.9975 - val_f1_4: 0.0000e+00 - val_f1_6: 0.8901 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.9087 - val_f1_13: 0.3598 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.9410 - val_f1_17: 0.8335 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.1090 - val_f1_22: 0.8640 - val_f1_23: 0.7334 - val_f1_24: 0.6118 - val_f1_25: 0.0000e+00 - val_f1_26: 0.6976 - val_f1_27: 0.0000e+00 - val_f1_28: 0.7531 - val_f1_30: 0.8465 - val_f1_31: 0.4512 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.7367 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.0000e+00 - val_f1_40: 0.7395 - val_f1_41: 0.4777 - val_f1_42: 0.8748 - val_f1_43: 0.7311 - val_f1_44: 0.0000e+00\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - 81s 5s/step - loss: 0.0643 - accuracy: 0.9824 - f1: 0.4022 - f1_1: 0.9527 - f1_2: 0.0000e+00 - f1_3: 0.9942 - f1_4: 0.0000e+00 - f1_6: 0.9148 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.9215 - f1_13: 0.3977 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.9269 - f1_17: 0.8348 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.1373 - f1_22: 0.8898 - f1_23: 0.7421 - f1_24: 0.5975 - f1_25: 0.0000e+00 - f1_26: 0.7691 - f1_27: 0.0000e+00 - f1_28: 0.7879 - f1_30: 0.8877 - f1_31: 0.5079 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.8134 - f1_37: 0.0000e+00 - f1_38: 0.9986 - f1_39: 0.0000e+00 - f1_40: 0.8016 - f1_41: 0.5353 - f1_42: 0.9250 - f1_43: 0.7525 - f1_44: 0.0000e+00 - val_loss: 0.0690 - val_accuracy: 0.9808 - val_f1: 0.3931 - val_f1_1: 0.9581 - val_f1_2: 0.0000e+00 - val_f1_3: 0.9975 - val_f1_4: 0.0096 - val_f1_6: 0.8954 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.9123 - val_f1_13: 0.2511 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.9410 - val_f1_17: 0.8226 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.1714 - val_f1_22: 0.8781 - val_f1_23: 0.7248 - val_f1_24: 0.6654 - val_f1_25: 0.0000e+00 - val_f1_26: 0.6946 - val_f1_27: 0.0000e+00 - val_f1_28: 0.7882 - val_f1_30: 0.8705 - val_f1_31: 0.4744 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.7751 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.0000e+00 - val_f1_40: 0.7495 - val_f1_41: 0.5156 - val_f1_42: 0.8871 - val_f1_43: 0.7398 - val_f1_44: 0.0000e+00\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - 83s 5s/step - loss: 0.0621 - accuracy: 0.9830 - f1: 0.4084 - f1_1: 0.9552 - f1_2: 0.0000e+00 - f1_3: 0.9943 - f1_4: 0.0474 - f1_6: 0.9247 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.9277 - f1_13: 0.3851 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.9400 - f1_17: 0.8315 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.1821 - f1_22: 0.8882 - f1_23: 0.7594 - f1_24: 0.6270 - f1_25: 0.0000e+00 - f1_26: 0.7788 - f1_27: 0.0000e+00 - f1_28: 0.8010 - f1_30: 0.8982 - f1_31: 0.5322 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.8126 - f1_37: 0.0000e+00 - f1_38: 0.9985 - f1_39: 0.0000e+00 - f1_40: 0.8076 - f1_41: 0.5509 - f1_42: 0.9362 - f1_43: 0.7576 - f1_44: 0.0000e+00 - val_loss: 0.0673 - val_accuracy: 0.9812 - val_f1: 0.4015 - val_f1_1: 0.9593 - val_f1_2: 0.0000e+00 - val_f1_3: 0.9975 - val_f1_4: 0.0882 - val_f1_6: 0.8780 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.9184 - val_f1_13: 0.3880 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.9453 - val_f1_17: 0.8421 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.1898 - val_f1_22: 0.8790 - val_f1_23: 0.7215 - val_f1_24: 0.6690 - val_f1_25: 0.0000e+00 - val_f1_26: 0.7487 - val_f1_27: 0.0000e+00 - val_f1_28: 0.7838 - val_f1_30: 0.8842 - val_f1_31: 0.5283 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.7770 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.0107 - val_f1_40: 0.7532 - val_f1_41: 0.4628 - val_f1_42: 0.8981 - val_f1_43: 0.7379 - val_f1_44: 0.0000e+00\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - 86s 5s/step - loss: 0.0601 - accuracy: 0.9836 - f1: 0.4151 - f1_1: 0.9553 - f1_2: 0.0000e+00 - f1_3: 0.9942 - f1_4: 0.1032 - f1_6: 0.9343 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.9294 - f1_13: 0.4259 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.9303 - f1_17: 0.8436 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.2147 - f1_22: 0.9026 - f1_23: 0.7646 - f1_24: 0.6426 - f1_25: 0.0000e+00 - f1_26: 0.7912 - f1_27: 0.0000e+00 - f1_28: 0.8126 - f1_30: 0.9120 - f1_31: 0.5363 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.8345 - f1_37: 0.0000e+00 - f1_38: 0.9986 - f1_39: 0.0000e+00 - f1_40: 0.8076 - f1_41: 0.5647 - f1_42: 0.9396 - f1_43: 0.7670 - f1_44: 0.0000e+00 - val_loss: 0.0652 - val_accuracy: 0.9817 - val_f1: 0.4070 - val_f1_1: 0.9596 - val_f1_2: 0.0000e+00 - val_f1_3: 0.9975 - val_f1_4: 0.1379 - val_f1_6: 0.8963 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.9223 - val_f1_13: 0.4557 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.9410 - val_f1_17: 0.8405 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.1868 - val_f1_22: 0.8847 - val_f1_23: 0.7222 - val_f1_24: 0.6781 - val_f1_25: 0.0000e+00 - val_f1_26: 0.7141 - val_f1_27: 0.0000e+00 - val_f1_28: 0.7684 - val_f1_30: 0.8854 - val_f1_31: 0.5138 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.7738 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.0107 - val_f1_40: 0.7586 - val_f1_41: 0.5504 - val_f1_42: 0.9192 - val_f1_43: 0.7624 - val_f1_44: 0.0000e+00\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - 86s 5s/step - loss: 0.0582 - accuracy: 0.9840 - f1: 0.4214 - f1_1: 0.9576 - f1_2: 0.0000e+00 - f1_3: 0.9945 - f1_4: 0.1171 - f1_6: 0.9377 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.9329 - f1_13: 0.4541 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.9351 - f1_17: 0.8475 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.2612 - f1_22: 0.8983 - f1_23: 0.7733 - f1_24: 0.6561 - f1_25: 0.0000e+00 - f1_26: 0.7878 - f1_27: 0.0000e+00 - f1_28: 0.8251 - f1_30: 0.9039 - f1_31: 0.5567 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.8531 - f1_37: 0.0000e+00 - f1_38: 0.9986 - f1_39: 0.0335 - f1_40: 0.8186 - f1_41: 0.5868 - f1_42: 0.9497 - f1_43: 0.7751 - f1_44: 0.0000e+00 - val_loss: 0.0644 - val_accuracy: 0.9820 - val_f1: 0.4144 - val_f1_1: 0.9605 - val_f1_2: 0.0000e+00 - val_f1_3: 0.9975 - val_f1_4: 0.2496 - val_f1_6: 0.9214 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.9253 - val_f1_13: 0.4951 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.9422 - val_f1_17: 0.8500 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.1729 - val_f1_22: 0.8959 - val_f1_23: 0.7455 - val_f1_24: 0.6505 - val_f1_25: 0.0000e+00 - val_f1_26: 0.7636 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8108 - val_f1_30: 0.8983 - val_f1_31: 0.5239 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8066 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.0202 - val_f1_40: 0.7665 - val_f1_41: 0.5149 - val_f1_42: 0.9241 - val_f1_43: 0.7402 - val_f1_44: 0.0000e+00\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - 79s 5s/step - loss: 0.0565 - accuracy: 0.9845 - f1: 0.4353 - f1_1: 0.9613 - f1_2: 0.0000e+00 - f1_3: 0.9948 - f1_4: 0.3644 - f1_6: 0.9458 - f1_8: 0.0505 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.9338 - f1_13: 0.4747 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.9391 - f1_17: 0.8617 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.2671 - f1_22: 0.9058 - f1_23: 0.7823 - f1_24: 0.6782 - f1_25: 0.0000e+00 - f1_26: 0.8016 - f1_27: 0.0000e+00 - f1_28: 0.8285 - f1_30: 0.9237 - f1_31: 0.5656 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.8630 - f1_37: 0.0000e+00 - f1_38: 0.9986 - f1_39: 0.1246 - f1_40: 0.8245 - f1_41: 0.5950 - f1_42: 0.9508 - f1_43: 0.7778 - f1_44: 0.0000e+00 - val_loss: 0.0619 - val_accuracy: 0.9826 - val_f1: 0.4253 - val_f1_1: 0.9646 - val_f1_2: 0.0000e+00 - val_f1_3: 0.9975 - val_f1_4: 0.2728 - val_f1_6: 0.9257 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.9313 - val_f1_13: 0.4139 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.9422 - val_f1_17: 0.8503 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.2055 - val_f1_22: 0.9001 - val_f1_23: 0.7515 - val_f1_24: 0.6856 - val_f1_25: 0.0000e+00 - val_f1_26: 0.7544 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8125 - val_f1_30: 0.9283 - val_f1_31: 0.5562 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8552 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.2783 - val_f1_40: 0.7622 - val_f1_41: 0.5167 - val_f1_42: 0.9321 - val_f1_43: 0.7742 - val_f1_44: 0.0000e+00\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - 81s 5s/step - loss: 0.0550 - accuracy: 0.9850 - f1: 0.4437 - f1_1: 0.9605 - f1_2: 0.0000e+00 - f1_3: 0.9927 - f1_4: 0.2836 - f1_6: 0.9521 - f1_8: 0.0574 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.9381 - f1_13: 0.5218 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.9384 - f1_17: 0.8608 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.2820 - f1_22: 0.9130 - f1_23: 0.7865 - f1_24: 0.6765 - f1_25: 0.0000e+00 - f1_26: 0.8065 - f1_27: 0.0000e+00 - f1_28: 0.8336 - f1_30: 0.9278 - f1_31: 0.5747 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.8602 - f1_37: 0.0000e+00 - f1_38: 0.9986 - f1_39: 0.4085 - f1_40: 0.8237 - f1_41: 0.6141 - f1_42: 0.9484 - f1_43: 0.7884 - f1_44: 0.0000e+00 - val_loss: 0.0612 - val_accuracy: 0.9828 - val_f1: 0.4360 - val_f1_1: 0.9627 - val_f1_2: 0.0000e+00 - val_f1_3: 0.9975 - val_f1_4: 0.4871 - val_f1_6: 0.9329 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.9383 - val_f1_13: 0.4670 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.9481 - val_f1_17: 0.8536 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.2071 - val_f1_22: 0.9033 - val_f1_23: 0.7757 - val_f1_24: 0.6965 - val_f1_25: 0.0000e+00 - val_f1_26: 0.7638 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8226 - val_f1_30: 0.9214 - val_f1_31: 0.5621 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8436 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.3470 - val_f1_40: 0.7722 - val_f1_41: 0.5525 - val_f1_42: 0.9386 - val_f1_43: 0.7446 - val_f1_44: 0.0000e+00\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0533 - accuracy: 0.9854 - f1: 0.4574 - f1_1: 0.9598 - f1_2: 0.0000e+00 - f1_3: 0.9941 - f1_4: 0.4967 - f1_6: 0.9518 - f1_8: 0.0813 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.9400 - f1_13: 0.5425 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.9476 - f1_17: 0.8717 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.3359 - f1_22: 0.9171 - f1_23: 0.7994 - f1_24: 0.6930 - f1_25: 0.0000e+00 - f1_26: 0.8189 - f1_27: 0.0000e+00 - f1_28: 0.8390 - f1_30: 0.9372 - f1_31: 0.5965 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.8726 - f1_37: 0.0000e+00 - f1_38: 0.9985 - f1_39: 0.4965 - f1_40: 0.8311 - f1_41: 0.6196 - f1_42: 0.9617 - f1_43: 0.7948 - f1_44: 0.0000e+00 - val_loss: 0.0592 - val_accuracy: 0.9835 - val_f1: 0.4447 - val_f1_1: 0.9617 - val_f1_2: 0.0000e+00 - val_f1_3: 0.9975 - val_f1_4: 0.5071 - val_f1_6: 0.9377 - val_f1_8: 0.0747 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.9370 - val_f1_13: 0.4642 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.9481 - val_f1_17: 0.8580 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.2685 - val_f1_22: 0.9002 - val_f1_23: 0.7790 - val_f1_24: 0.7003 - val_f1_25: 0.0000e+00 - val_f1_26: 0.7823 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8203 - val_f1_30: 0.9265 - val_f1_31: 0.5758 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8542 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.4235 - val_f1_40: 0.7705 - val_f1_41: 0.5825 - val_f1_42: 0.9465 - val_f1_43: 0.7716 - val_f1_44: 0.0000e+00\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0519 - accuracy: 0.9858 - f1: 0.4659 - f1_1: 0.9614 - f1_2: 0.0000e+00 - f1_3: 0.9946 - f1_4: 0.5168 - f1_6: 0.9569 - f1_8: 0.1384 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.9424 - f1_13: 0.5412 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.9475 - f1_17: 0.8698 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.3587 - f1_22: 0.9158 - f1_23: 0.8030 - f1_24: 0.7211 - f1_25: 0.0000e+00 - f1_26: 0.8291 - f1_27: 0.0000e+00 - f1_28: 0.8487 - f1_30: 0.9320 - f1_31: 0.6053 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.8862 - f1_37: 0.0000e+00 - f1_38: 0.9985 - f1_39: 0.6391 - f1_40: 0.8356 - f1_41: 0.6377 - f1_42: 0.9601 - f1_43: 0.7962 - f1_44: 0.0000e+00 - val_loss: 0.0578 - val_accuracy: 0.9839 - val_f1: 0.4472 - val_f1_1: 0.9668 - val_f1_2: 0.0000e+00 - val_f1_3: 0.9975 - val_f1_4: 0.5812 - val_f1_6: 0.9360 - val_f1_8: 0.0530 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.9389 - val_f1_13: 0.5059 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.9602 - val_f1_17: 0.8693 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.2223 - val_f1_22: 0.9034 - val_f1_23: 0.7804 - val_f1_24: 0.6767 - val_f1_25: 0.0000e+00 - val_f1_26: 0.7865 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8390 - val_f1_30: 0.9298 - val_f1_31: 0.5589 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8736 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.4141 - val_f1_40: 0.7534 - val_f1_41: 0.6033 - val_f1_42: 0.9489 - val_f1_43: 0.7901 - val_f1_44: 0.0000e+00\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0507 - accuracy: 0.9861 - f1: 0.4744 - f1_1: 0.9628 - f1_2: 0.0000e+00 - f1_3: 0.9950 - f1_4: 0.6268 - f1_6: 0.9566 - f1_8: 0.1718 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.9442 - f1_13: 0.5754 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.9692 - f1_17: 0.8799 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.3519 - f1_22: 0.9186 - f1_23: 0.8085 - f1_24: 0.6961 - f1_25: 0.0000e+00 - f1_26: 0.8223 - f1_27: 0.0000e+00 - f1_28: 0.8688 - f1_30: 0.9341 - f1_31: 0.6211 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.8999 - f1_37: 0.0000e+00 - f1_38: 0.9987 - f1_39: 0.7174 - f1_40: 0.8347 - f1_41: 0.6479 - f1_42: 0.9677 - f1_43: 0.8047 - f1_44: 0.0000e+00 - val_loss: 0.0580 - val_accuracy: 0.9836 - val_f1: 0.4543 - val_f1_1: 0.9658 - val_f1_2: 0.0000e+00 - val_f1_3: 0.9975 - val_f1_4: 0.5886 - val_f1_6: 0.9401 - val_f1_8: 0.1358 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.9419 - val_f1_13: 0.5483 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.9693 - val_f1_17: 0.8690 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.3076 - val_f1_22: 0.9092 - val_f1_23: 0.7829 - val_f1_24: 0.7095 - val_f1_25: 0.0000e+00 - val_f1_26: 0.7913 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8448 - val_f1_30: 0.9317 - val_f1_31: 0.5511 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8635 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.4924 - val_f1_40: 0.7783 - val_f1_41: 0.5660 - val_f1_42: 0.9453 - val_f1_43: 0.7433 - val_f1_44: 0.0000e+00\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - 80s 5s/step - loss: 0.0496 - accuracy: 0.9863 - f1: 0.4801 - f1_1: 0.9614 - f1_2: 0.0096 - f1_3: 0.9946 - f1_4: 0.6812 - f1_6: 0.9580 - f1_8: 0.2530 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.9452 - f1_13: 0.5952 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.9721 - f1_17: 0.8830 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.4029 - f1_22: 0.9169 - f1_23: 0.8068 - f1_24: 0.7284 - f1_25: 0.0000e+00 - f1_26: 0.8333 - f1_27: 0.0000e+00 - f1_28: 0.8648 - f1_30: 0.9375 - f1_31: 0.6185 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.8857 - f1_37: 0.0000e+00 - f1_38: 0.9987 - f1_39: 0.7077 - f1_40: 0.8377 - f1_41: 0.6502 - f1_42: 0.9639 - f1_43: 0.7978 - f1_44: 0.0000e+00 - val_loss: 0.0556 - val_accuracy: 0.9844 - val_f1: 0.4617 - val_f1_1: 0.9670 - val_f1_2: 0.0000e+00 - val_f1_3: 0.9975 - val_f1_4: 0.6498 - val_f1_6: 0.9419 - val_f1_8: 0.1668 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.9413 - val_f1_13: 0.5049 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.9613 - val_f1_17: 0.8706 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.3405 - val_f1_22: 0.9005 - val_f1_23: 0.8002 - val_f1_24: 0.7402 - val_f1_25: 0.0000e+00 - val_f1_26: 0.7845 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8351 - val_f1_30: 0.9305 - val_f1_31: 0.5167 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8746 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.6116 - val_f1_40: 0.7743 - val_f1_41: 0.6155 - val_f1_42: 0.9537 - val_f1_43: 0.7896 - val_f1_44: 0.0000e+00\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0484 - accuracy: 0.9867 - f1: 0.4871 - f1_1: 0.9624 - f1_2: 0.0156 - f1_3: 0.9877 - f1_4: 0.7050 - f1_6: 0.9606 - f1_8: 0.2806 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.9465 - f1_13: 0.6033 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.9796 - f1_17: 0.8867 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.4235 - f1_22: 0.9281 - f1_23: 0.8219 - f1_24: 0.7373 - f1_25: 0.0000e+00 - f1_26: 0.8460 - f1_27: 0.0000e+00 - f1_28: 0.8724 - f1_30: 0.9446 - f1_31: 0.6387 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.9292 - f1_37: 0.0000e+00 - f1_38: 0.9987 - f1_39: 0.7248 - f1_40: 0.8315 - f1_41: 0.6726 - f1_42: 0.9740 - f1_43: 0.8119 - f1_44: 0.0000e+00 - val_loss: 0.0562 - val_accuracy: 0.9841 - val_f1: 0.4666 - val_f1_1: 0.9685 - val_f1_2: 0.0076 - val_f1_3: 0.9975 - val_f1_4: 0.6844 - val_f1_6: 0.9460 - val_f1_8: 0.2182 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.9418 - val_f1_13: 0.5837 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.9889 - val_f1_17: 0.8712 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.3411 - val_f1_22: 0.9109 - val_f1_23: 0.7872 - val_f1_24: 0.7317 - val_f1_25: 0.0000e+00 - val_f1_26: 0.8039 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8513 - val_f1_30: 0.9328 - val_f1_31: 0.5505 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8727 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.6045 - val_f1_40: 0.7764 - val_f1_41: 0.5930 - val_f1_42: 0.9543 - val_f1_43: 0.7468 - val_f1_44: 0.0000e+00\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0474 - accuracy: 0.9868 - f1: 0.4914 - f1_1: 0.9642 - f1_2: 0.0000e+00 - f1_3: 0.9955 - f1_4: 0.7423 - f1_6: 0.9596 - f1_8: 0.3434 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.9491 - f1_13: 0.6280 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.9832 - f1_17: 0.8903 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.4582 - f1_22: 0.9288 - f1_23: 0.8213 - f1_24: 0.7439 - f1_25: 0.0000e+00 - f1_26: 0.8389 - f1_27: 0.0000e+00 - f1_28: 0.8765 - f1_30: 0.9497 - f1_31: 0.6414 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.9118 - f1_37: 0.0000e+00 - f1_38: 0.9988 - f1_39: 0.7496 - f1_40: 0.8371 - f1_41: 0.6690 - f1_42: 0.9716 - f1_43: 0.8027 - f1_44: 0.0000e+00 - val_loss: 0.0534 - val_accuracy: 0.9849 - val_f1: 0.4712 - val_f1_1: 0.9676 - val_f1_2: 0.0158 - val_f1_3: 0.9975 - val_f1_4: 0.7547 - val_f1_6: 0.9434 - val_f1_8: 0.1442 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.9480 - val_f1_13: 0.5187 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.9758 - val_f1_17: 0.8785 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.3882 - val_f1_22: 0.9144 - val_f1_23: 0.7980 - val_f1_24: 0.7521 - val_f1_25: 0.0000e+00 - val_f1_26: 0.8016 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8482 - val_f1_30: 0.9308 - val_f1_31: 0.5827 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.9030 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.6199 - val_f1_40: 0.7744 - val_f1_41: 0.6426 - val_f1_42: 0.9613 - val_f1_43: 0.7874 - val_f1_44: 0.0000e+00\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0460 - accuracy: 0.9873 - f1: 0.4949 - f1_1: 0.9658 - f1_2: 0.0066 - f1_3: 0.9952 - f1_4: 0.7245 - f1_6: 0.9632 - f1_8: 0.3498 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.9509 - f1_13: 0.6383 - f1_14: 0.0000e+00 - f1_15: 0.0491 - f1_16: 0.9822 - f1_17: 0.8925 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.4896 - f1_22: 0.9307 - f1_23: 0.8284 - f1_24: 0.7595 - f1_25: 0.0000e+00 - f1_26: 0.8517 - f1_27: 0.0000e+00 - f1_28: 0.8801 - f1_30: 0.9485 - f1_31: 0.6499 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.9144 - f1_37: 0.0000e+00 - f1_38: 0.9985 - f1_39: 0.6997 - f1_40: 0.8407 - f1_41: 0.6927 - f1_42: 0.9756 - f1_43: 0.8186 - f1_44: 0.0000e+00 - val_loss: 0.0526 - val_accuracy: 0.9851 - val_f1: 0.4760 - val_f1_1: 0.9686 - val_f1_2: 0.0076 - val_f1_3: 0.9975 - val_f1_4: 0.7837 - val_f1_6: 0.9455 - val_f1_8: 0.2182 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.9466 - val_f1_13: 0.5706 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.9753 - val_f1_17: 0.8755 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.4271 - val_f1_22: 0.9160 - val_f1_23: 0.7943 - val_f1_24: 0.7627 - val_f1_25: 0.0000e+00 - val_f1_26: 0.8069 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8390 - val_f1_30: 0.9375 - val_f1_31: 0.5853 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.8793 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.6264 - val_f1_40: 0.7749 - val_f1_41: 0.6348 - val_f1_42: 0.9602 - val_f1_43: 0.8047 - val_f1_44: 0.0000e+00\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0445 - accuracy: 0.9877 - f1: 0.5022 - f1_1: 0.9668 - f1_2: 0.0451 - f1_3: 0.9950 - f1_4: 0.7525 - f1_6: 0.9613 - f1_8: 0.3822 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.9529 - f1_13: 0.6395 - f1_14: 0.0000e+00 - f1_15: 0.0522 - f1_16: 0.9853 - f1_17: 0.9002 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.4923 - f1_22: 0.9309 - f1_23: 0.8308 - f1_24: 0.7577 - f1_25: 0.0000e+00 - f1_26: 0.8577 - f1_27: 0.0000e+00 - f1_28: 0.8892 - f1_30: 0.9448 - f1_31: 0.6681 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.9424 - f1_37: 0.0000e+00 - f1_38: 0.9984 - f1_39: 0.7870 - f1_40: 0.8538 - f1_41: 0.6981 - f1_42: 0.9792 - f1_43: 0.8238 - f1_44: 0.0000e+00 - val_loss: 0.0522 - val_accuracy: 0.9852 - val_f1: 0.4789 - val_f1_1: 0.9710 - val_f1_2: 0.0406 - val_f1_3: 0.9975 - val_f1_4: 0.7082 - val_f1_6: 0.9462 - val_f1_8: 0.2701 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.9495 - val_f1_13: 0.6382 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.9908 - val_f1_17: 0.8787 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.4354 - val_f1_22: 0.9127 - val_f1_23: 0.8224 - val_f1_24: 0.7457 - val_f1_25: 0.0000e+00 - val_f1_26: 0.8062 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8577 - val_f1_30: 0.9404 - val_f1_31: 0.5855 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.9129 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.6264 - val_f1_40: 0.7809 - val_f1_41: 0.5858 - val_f1_42: 0.9712 - val_f1_43: 0.7836 - val_f1_44: 0.0000e+00\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0436 - accuracy: 0.9879 - f1: 0.5065 - f1_1: 0.9700 - f1_2: 0.0314 - f1_3: 0.9933 - f1_4: 0.8140 - f1_6: 0.9669 - f1_8: 0.4130 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.9542 - f1_13: 0.6574 - f1_14: 0.0000e+00 - f1_15: 0.0458 - f1_16: 0.9890 - f1_17: 0.9032 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.5308 - f1_22: 0.9306 - f1_23: 0.8392 - f1_24: 0.7762 - f1_25: 0.0000e+00 - f1_26: 0.8532 - f1_27: 0.0000e+00 - f1_28: 0.8865 - f1_30: 0.9534 - f1_31: 0.6657 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.9448 - f1_37: 0.0000e+00 - f1_38: 0.9986 - f1_39: 0.7746 - f1_40: 0.8542 - f1_41: 0.7058 - f1_42: 0.9793 - f1_43: 0.8272 - f1_44: 0.0000e+00 - val_loss: 0.0508 - val_accuracy: 0.9856 - val_f1: 0.4850 - val_f1_1: 0.9702 - val_f1_2: 0.0330 - val_f1_3: 0.9975 - val_f1_4: 0.7562 - val_f1_6: 0.9453 - val_f1_8: 0.3019 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.9457 - val_f1_13: 0.6228 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0563 - val_f1_16: 0.9908 - val_f1_17: 0.8918 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.4356 - val_f1_22: 0.9166 - val_f1_23: 0.8001 - val_f1_24: 0.7337 - val_f1_25: 0.0000e+00 - val_f1_26: 0.8017 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8567 - val_f1_30: 0.9419 - val_f1_31: 0.6094 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.9088 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.6541 - val_f1_40: 0.7779 - val_f1_41: 0.6728 - val_f1_42: 0.9733 - val_f1_43: 0.8070 - val_f1_44: 0.0000e+00\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0426 - accuracy: 0.9883 - f1: 0.5106 - f1_1: 0.9694 - f1_2: 0.0401 - f1_3: 0.9954 - f1_4: 0.8083 - f1_6: 0.9675 - f1_8: 0.4535 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.9539 - f1_13: 0.6640 - f1_14: 0.0000e+00 - f1_15: 0.1053 - f1_16: 0.9869 - f1_17: 0.9068 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.5452 - f1_22: 0.9296 - f1_23: 0.8428 - f1_24: 0.7746 - f1_25: 0.0000e+00 - f1_26: 0.8609 - f1_27: 0.0000e+00 - f1_28: 0.8986 - f1_30: 0.9556 - f1_31: 0.6835 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.9486 - f1_37: 0.0000e+00 - f1_38: 0.9986 - f1_39: 0.7494 - f1_40: 0.8591 - f1_41: 0.7122 - f1_42: 0.9810 - f1_43: 0.8323 - f1_44: 0.0000e+00 - val_loss: 0.0502 - val_accuracy: 0.9857 - val_f1: 0.4877 - val_f1_1: 0.9708 - val_f1_2: 0.0621 - val_f1_3: 0.9975 - val_f1_4: 0.8617 - val_f1_6: 0.9469 - val_f1_8: 0.2941 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.9518 - val_f1_13: 0.5848 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0303 - val_f1_16: 0.9851 - val_f1_17: 0.8888 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.4049 - val_f1_22: 0.9169 - val_f1_23: 0.8202 - val_f1_24: 0.7608 - val_f1_25: 0.0000e+00 - val_f1_26: 0.8107 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8564 - val_f1_30: 0.9396 - val_f1_31: 0.6041 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.9309 - val_f1_37: 0.0000e+00 - val_f1_38: 1.0000 - val_f1_39: 0.6541 - val_f1_40: 0.7887 - val_f1_41: 0.6756 - val_f1_42: 0.9756 - val_f1_43: 0.7939 - val_f1_44: 0.0000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# History f1 for class"
      ],
      "metadata": {
        "id": "KycSCuMUjU8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_val_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_val_list[i] = history.history['val_f1_{}'.format(i)][-1]\n",
        "f1_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_list[i] = history.history['f1_{}'.format(i)][-1]"
      ],
      "metadata": {
        "id": "lVJIeRimjW7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index2tag = {value : key for (key, value) in tag2index.items()}\n",
        "index2tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hI-5oeIjX6-",
        "outputId": "c1c792a5-2c63-467f-8286-01ea5170e754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '-PAD-',\n",
              " 1: 'DT',\n",
              " 2: 'JJR',\n",
              " 3: '$',\n",
              " 4: 'WDT',\n",
              " 5: ',',\n",
              " 6: 'CC',\n",
              " 7: ':',\n",
              " 8: 'RP',\n",
              " 9: 'UH',\n",
              " 10: 'SYM',\n",
              " 11: 'LS',\n",
              " 12: 'IN',\n",
              " 13: 'RB',\n",
              " 14: '-RRB-',\n",
              " 15: 'RBR',\n",
              " 16: 'POS',\n",
              " 17: 'CD',\n",
              " 18: 'WP$',\n",
              " 19: '#',\n",
              " 20: '.',\n",
              " 21: 'VBG',\n",
              " 22: 'VBZ',\n",
              " 23: 'NNS',\n",
              " 24: 'VBP',\n",
              " 25: '-LRB-',\n",
              " 26: 'VBD',\n",
              " 27: 'JJS',\n",
              " 28: 'VB',\n",
              " 29: \"''\",\n",
              " 30: 'PRP',\n",
              " 31: 'VBN',\n",
              " 32: 'EX',\n",
              " 33: 'WRB',\n",
              " 34: 'RBS',\n",
              " 35: 'PDT',\n",
              " 36: 'MD',\n",
              " 37: 'FW',\n",
              " 38: 'TO',\n",
              " 39: 'WP',\n",
              " 40: 'NNP',\n",
              " 41: 'JJ',\n",
              " 42: 'PRP$',\n",
              " 43: 'NN',\n",
              " 44: 'NNPS',\n",
              " 45: '``'}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- F1: {}'.format(index2tag[i], f1_list[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjNcJNswjaH-",
        "outputId": "7a4a8c57-aaa2-479a-c459-819d0ddf70ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: DT --- F1: 0.9694027304649353\n",
            "Tag: JJR --- F1: 0.040064096450805664\n",
            "Tag: $ --- F1: 0.9953662157058716\n",
            "Tag: WDT --- F1: 0.8082517385482788\n",
            "Tag: CC --- F1: 0.9674540758132935\n",
            "Tag: RP --- F1: 0.4534687399864197\n",
            "Tag: UH --- F1: 0.0\n",
            "Tag: SYM --- F1: 0.0\n",
            "Tag: LS --- F1: 0.0\n",
            "Tag: IN --- F1: 0.9538957476615906\n",
            "Tag: RB --- F1: 0.6639585494995117\n",
            "Tag: -RRB- --- F1: 0.0\n",
            "Tag: RBR --- F1: 0.10527145117521286\n",
            "Tag: POS --- F1: 0.9869294166564941\n",
            "Tag: CD --- F1: 0.9067718386650085\n",
            "Tag: WP$ --- F1: 0.0\n",
            "Tag: # --- F1: 0.0\n",
            "Tag: VBG --- F1: 0.5451856255531311\n",
            "Tag: VBZ --- F1: 0.9296085834503174\n",
            "Tag: NNS --- F1: 0.8427517414093018\n",
            "Tag: VBP --- F1: 0.7746413946151733\n",
            "Tag: -LRB- --- F1: 0.0\n",
            "Tag: VBD --- F1: 0.8609495162963867\n",
            "Tag: JJS --- F1: 0.0\n",
            "Tag: VB --- F1: 0.8985973596572876\n",
            "Tag: PRP --- F1: 0.955596923828125\n",
            "Tag: VBN --- F1: 0.6834931373596191\n",
            "Tag: EX --- F1: 0.0\n",
            "Tag: WRB --- F1: 0.0\n",
            "Tag: RBS --- F1: 0.0\n",
            "Tag: PDT --- F1: 0.0\n",
            "Tag: MD --- F1: 0.9485870599746704\n",
            "Tag: FW --- F1: 0.0\n",
            "Tag: TO --- F1: 0.9986046552658081\n",
            "Tag: WP --- F1: 0.7494247555732727\n",
            "Tag: NNP --- F1: 0.8591487407684326\n",
            "Tag: JJ --- F1: 0.7121654748916626\n",
            "Tag: PRP$ --- F1: 0.9810415506362915\n",
            "Tag: NN --- F1: 0.8323107361793518\n",
            "Tag: NNPS --- F1: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- Val_F1: {}'.format(index2tag[i], f1_val_list[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WudhuDGajbgI",
        "outputId": "69c2ccce-3bc4-4b66-a6ec-197c3c50bef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: DT --- Val_F1: 0.9707609415054321\n",
            "Tag: JJR --- Val_F1: 0.06209293380379677\n",
            "Tag: $ --- Val_F1: 0.9975429773330688\n",
            "Tag: WDT --- Val_F1: 0.8616756796836853\n",
            "Tag: CC --- Val_F1: 0.9468650221824646\n",
            "Tag: RP --- Val_F1: 0.2940836548805237\n",
            "Tag: UH --- Val_F1: 0.0\n",
            "Tag: SYM --- Val_F1: 0.0\n",
            "Tag: LS --- Val_F1: 0.0\n",
            "Tag: IN --- Val_F1: 0.9517644643783569\n",
            "Tag: RB --- Val_F1: 0.5847631692886353\n",
            "Tag: -RRB- --- Val_F1: 0.0\n",
            "Tag: RBR --- Val_F1: 0.030303025618195534\n",
            "Tag: POS --- Val_F1: 0.9850664138793945\n",
            "Tag: CD --- Val_F1: 0.8888280391693115\n",
            "Tag: WP$ --- Val_F1: 0.0\n",
            "Tag: # --- Val_F1: 0.0\n",
            "Tag: VBG --- Val_F1: 0.4048955738544464\n",
            "Tag: VBZ --- Val_F1: 0.9168899655342102\n",
            "Tag: NNS --- Val_F1: 0.8202285170555115\n",
            "Tag: VBP --- Val_F1: 0.7608014941215515\n",
            "Tag: -LRB- --- Val_F1: 0.0\n",
            "Tag: VBD --- Val_F1: 0.8107040524482727\n",
            "Tag: JJS --- Val_F1: 0.0\n",
            "Tag: VB --- Val_F1: 0.8564296960830688\n",
            "Tag: PRP --- Val_F1: 0.9396199584007263\n",
            "Tag: VBN --- Val_F1: 0.6041143536567688\n",
            "Tag: EX --- Val_F1: 0.0\n",
            "Tag: WRB --- Val_F1: 0.0\n",
            "Tag: RBS --- Val_F1: 0.0\n",
            "Tag: PDT --- Val_F1: 0.0\n",
            "Tag: MD --- Val_F1: 0.9308757185935974\n",
            "Tag: FW --- Val_F1: 0.0\n",
            "Tag: TO --- Val_F1: 1.0\n",
            "Tag: WP --- Val_F1: 0.6541125774383545\n",
            "Tag: NNP --- Val_F1: 0.7887400984764099\n",
            "Tag: JJ --- Val_F1: 0.6755973100662231\n",
            "Tag: PRP$ --- Val_F1: 0.9756343960762024\n",
            "Tag: NN --- Val_F1: 0.7938627004623413\n",
            "Tag: NNPS --- Val_F1: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Infos"
      ],
      "metadata": {
        "id": "pGvMYtaAjc5e"
      }
    }
  ]
}