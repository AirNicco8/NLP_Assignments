{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_biLSTM256_LSTM256.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Due to**: 23/12/2021 (dd/mm/yyyy)\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Part-of Speech (POS) tagging as Sequence Labelling using Recurrent Neural Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4_wqPdlBcKS"
      },
      "source": [
        "# Intro\n",
        "\n",
        "In this assignment  we will ask you to perform POS tagging using neural architectures\n",
        "\n",
        "You are asked to follow these steps:\n",
        "*   Download the corpora and split it in training and test sets, structuring a dataframe.\n",
        "*   Embed the words using GloVe embeddings\n",
        "*   Create a baseline model, using a simple neural architecture\n",
        "*   Experiment doing small modifications to the baseline model, choose hyperparameters using the validation set\n",
        "*   Evaluate your two best model\n",
        "*   Analyze the errors of your model\n",
        "\n",
        "\n",
        "**Task**: given a corpus of documents, predict the POS tag for each word\n",
        "\n",
        "**Corpus**:\n",
        "Ignore the numeric value in the third column, use only the words/symbols and its label. \n",
        "The corpus is available at:\n",
        "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
        "\n",
        "**Splits**: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
        "\n",
        "\n",
        "**Features**: you MUST use GloVe embeddings as the only input features to the model.\n",
        "\n",
        "**Splitting**: you can decide to split documents into sentences or not, the choice is yours.\n",
        "\n",
        "**I/O structure**: The input data will have three dimensions: 1-documents/sentences, 2-token, 3-features; for the output there are 2 possibilities: if you use one-hot encoding it will be 1-documents/sentences, 2-token labels, 3-classes, if you use a single integer that indicates the number of the class it will be 1-documents/sentences, 2-token labels.\n",
        "\n",
        "**Baseline**: two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top; the choice of hyper-parameters is yours.\n",
        "\n",
        "**Architectures**: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and adding an additional dense layer; do not mix these variantions.\n",
        "\n",
        "\n",
        "**Training and Experiments**: all the experiments must involve only the training and validation sets.\n",
        "\n",
        "**Evaluation**: in the end, only the two best models of your choice (according to the validation set) must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech. DO NOT CONSIDER THE PUNCTUATION CLASSES.\n",
        "\n",
        "**Metrics**: the metric you must use to evaluate your final model is the F1-macro, WITHOUT considering punctuation/symbols classes; during the training process you can use accuracy because you can't use the F1 metric unless you use a single (gigantic) batch because there is no way to aggregate \"partial\" F1 scores computed on mini-batches.\n",
        "\n",
        "**Discussion and Error Analysis** : verify and discuss if the results on the test sets are coherent with those on the validation set; analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
        "\n",
        "**Report**: you are asked to deliver the code of your experiments and a small pdf report of about 2 pages; the pdf must begin with the names of the people of your team and a small abstract (4-5 lines) that sums up your findings.\n",
        "\n",
        "# Out Of Vocabulary (OOV) terms\n",
        "\n",
        "How to handle words that are not in GloVe vocabulary?\n",
        "You can handle them as you want (random embedding, placeholder, whatever!), but they must be STATIC embeddings (you cannot train them).\n",
        "\n",
        "But there is a very important caveat! As usual, the element of the test set must not influence the elements of the other splits!\n",
        "\n",
        "So, when you compute new embeddings for train+validation, you must forget about test documents.\n",
        "The motivation is to emulate a real-world scenario, where you select and train a model in the first stage, without knowing nothing about the testing environment.\n",
        "\n",
        "For implementation convenience, you CAN use a single vocabulary file/matrix/whatever. The principle of the previous point is that the embeddings inside that file/matrix must be generated independently for train and test splits.\n",
        "\n",
        "Basically in a real-world scenario, this is what would happen:\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Training of the model(s)\n",
        "5. Compute embeddings for terms OOV2 of the validation split \n",
        "6. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "7. Validation of the model(s)\n",
        "8. Compute embeddings for terms OOV3 of the test split \n",
        "9. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2+OOV3\n",
        "10. Testing of the final model\n",
        "\n",
        "In this case, where we already have all the documents, we can simplify the process a bit, but the procedure must remain rigorous.\n",
        "\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Compute embeddings for terms OOV2 of the validation split \n",
        "5. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "6. Compute embeddings for terms OOV3 of the test split \n",
        "7. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2\n",
        "8. Training of the model(s)\n",
        "9. Validation of the model(s)\n",
        "10. Testing of the final model\n",
        "\n",
        "Step 2 and step 6 must be completely independent of each other, for what concerns the method and the documents. But they can rely on the previous vocabulary (V1 for step 2 and V3 for step 6)\n",
        "THEREFORE if a word is present both in the training set and the test split and not in the starting vocabulary, its embedding is computed in step 2) and it is not considered OOV anymore in step 6).\n",
        "\n",
        "# Report\n",
        "The report must not be just a copy and paste of graphs and tables!\n",
        "\n",
        "The report must not be longer than 2 pages and must contain:\n",
        "* The names of the member of your team\n",
        "* A short abstract (4-5 lines) that sum ups everything\n",
        "* A general description of the task you have addressed and how you have addressed it\n",
        "* A short description of the models you have used\n",
        "* Some tables that sum up your findings in validation and test and a discussion of those results\n",
        "* The most relevant findings of your error analysis\n",
        "\n",
        "# Evaluation Criterion\n",
        "\n",
        "The goal of this assignment is not to prove you can find best model ever, but to face a common task, structure it correctly, and follow a correct and rigorous experimental procedure.\n",
        "In other words, we don't care if you final models are awful as long as you have followed the correct procedure and wrote a decent report.\n",
        "\n",
        "The score of the assignment will be computed roughly as follows\n",
        "* 1 point for the general setting of the problem\n",
        "* 1 point for the handling of OOV terms\n",
        "* 1 point for the models\n",
        "* 1 point for train-validation-test procedure\n",
        "* 2 point for the discussion of the results, error analysis, and report\n",
        "\n",
        "This distribution of scores is tentative and we may decide to alter it at any moment.\n",
        "We also reserve the right to assign a small bonus (0.5 points) to any assignment that is particularly worthy. Similarly, in case of grave errors, we may decide to assign an equivalent malus (-0.5 points).\n",
        "\n",
        "# Contacts\n",
        "\n",
        "In case of any doubt, question, issue, or help we highly recommend you to check the [course useful material](https://virtuale.unibo.it/pluginfile.php/1036039/mod_resource/content/2/NLP_Course_Useful_Material.pdf) for additional information, and to use the Virtuale forums to discuss with other students.\n",
        "\n",
        "You can always contact us at the following email addresses. To increase the probability of a prompt response, we reccomend you to write to both the teaching assistants.\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Andrea Galassi -> a.galassi@unibo.it\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it\n",
        "\n",
        "\n",
        "# FAQ\n",
        "* You can use a non-trainable Embedding layer to load the glove embeddings\n",
        "* You can use any library of your choice to implement the networks. Two options are tensorflow/keras or pythorch. Both these libraries have all the classes you need to implement these simple architectures and there are plenty of tutorials around, where you can learn how to use them."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports & Dataset"
      ],
      "metadata": {
        "id": "Gh-i2KJetynG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emdubW66fnlD"
      },
      "source": [
        "import os, shutil  #  file management\n",
        "import sys \n",
        "import pandas as pd  #  dataframe management\n",
        "import numpy as np  #  data manipulation\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request  #  download files\n",
        "import zipfile  #  unzip files\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B-w2pqyfpF1"
      },
      "source": [
        "from keras import metrics \n",
        "from tensorflow.python.keras.metrics import Metric\n",
        "from sklearn.metrics import f1_score\n",
        "import keras.backend as K"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsV-0pj2t8yS",
        "outputId": "183392cc-aec3-4d2d-a735-bac9355f8454"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3xn1MZPgA_v",
        "outputId": "055d59e9-51d7-4da7-9279-63cf97c9f2c1"
      },
      "source": [
        "dataset_folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\")\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "    os.makedirs(dataset_folder)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
        "\n",
        "dataset_path = os.path.join(dataset_folder, \"treebank.zip\")\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    urllib.request.urlretrieve(url, dataset_path)\n",
        "    print(\"Successful download\")\n",
        "\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_folder)\n",
        "print(\"Successful extraction\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful extraction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre Processing\n",
        "- splitting"
      ],
      "metadata": {
        "id": "-UgZSC3ht_Xu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKbUsrVHhGAG"
      },
      "source": [
        "dataset_name = \"dependency_treebank\"\n",
        "\n",
        "\n",
        "folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\", dataset_name)\n",
        "\n",
        "pre_train = []\n",
        "pre_valid = []\n",
        "pre_test = []\n",
        "i = 1\n",
        "\n",
        "file_list = sorted(os.listdir(folder))\n",
        "\n",
        "for filename in file_list:\n",
        "  file_path = os.path.join(folder, filename)\n",
        "  if os.path.isfile(file_path):\n",
        "    # open the file\n",
        "      text = []\n",
        "      with open(file_path, mode='r', encoding='utf-8') as text_file:\n",
        "        text = text_file.read()\n",
        "        if i <= 100:\n",
        "          pre_train.append(text)\n",
        "        elif i <= 150:\n",
        "          pre_valid.append(text)\n",
        "        else:\n",
        "          pre_test.append(text)  \n",
        "  i+=1\n",
        "\n",
        "tr = []\n",
        "val = []\n",
        "tes = []\n",
        "\n",
        "for paragraph in pre_train:\n",
        "   tr.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_valid:\n",
        "   val.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_test:\n",
        "   tes.append(paragraph.split('\\n'))\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUgbTXi-F7Sa"
      },
      "source": [
        "train = []\n",
        "valid = []\n",
        "test = []\n",
        "\n",
        "for i in tr:\n",
        "  for j in i:\n",
        "    train.append(j.split('\\t'))\n",
        "\n",
        "for i in val:\n",
        "  for j in i:\n",
        "    valid.append(j.split('\\t'))\n",
        "\n",
        "for i in tes:\n",
        "  for j in i:\n",
        "    test.append(j.split('\\t'))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmzVaRuQFp0M"
      },
      "source": [
        "train_sentences = []\n",
        "valid_sentences = []\n",
        "test_sentences = []\n",
        "train_tags = []\n",
        "valid_tags = []\n",
        "test_tags = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in train:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    train_sentences.append(s)\n",
        "    train_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in valid:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    valid_sentences.append(s)\n",
        "    valid_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in test:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    test_sentences.append(s)\n",
        "    test_tags.append(t)\n",
        "    s = []\n",
        "    t = []"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB65cUFLIeEr"
      },
      "source": [
        "flat_train = [item for sublist in train_sentences for item in sublist]\n",
        "flat_valid = [item for sublist in valid_sentences for item in sublist]\n",
        "flat_test = [item for sublist in test_sentences for item in sublist]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "- vocabularies\n",
        "- GloVe download"
      ],
      "metadata": {
        "id": "FhY0Uaq1uDOx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZO6ga1EGHdh"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "train_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "train_tokenizer.fit_on_texts(train_sentences)                    # fit tokeniser on data\n",
        "train_encoded = train_tokenizer.texts_to_sequences(train_sentences)\n",
        "\n",
        "valid_tokenizer = Tokenizer()       \n",
        "valid_tokenizer.fit_on_texts(valid_sentences)                  # instantiate tokeniser\n",
        "valid_encoded = valid_tokenizer.texts_to_sequences(valid_sentences)\n",
        "\n",
        "test_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "test_tokenizer.fit_on_texts(test_sentences)                    # fit tokeniser on data\n",
        "test_encoded = test_tokenizer.texts_to_sequences(test_sentences)  # use the tokeniser to encode input sequence"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(train_tags)\n",
        "tags_encoded = tag_tokenizer.texts_to_sequences(train_tags)"
      ],
      "metadata": {
        "id": "VfDmU4-huKNK"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMsG0Yw1l50L",
        "outputId": "f47ec4dd-9c7f-464e-de90-d1a35d30bb8e"
      },
      "source": [
        "voc = list(set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(valid_tokenizer.word_index.keys()) - set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(test_tokenizer.word_index.keys()) - set(voc))\n",
        "print(len(voc))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7404\n",
            "9901\n",
            "10947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS9RwnOtJuas"
      },
      "source": [
        "word_index = dict(zip(voc, range(2, len(voc)+3)))\n",
        "word_index['-PAD-'] = 0\n",
        "word_index['-OOV-'] = 1\n",
        "\n",
        "tags = set([item for sublist in train_tags for item in sublist])\n",
        "\n",
        "tag2index = {t: i + 1 for i, t in enumerate(tags)}\n",
        "tag2index['-PAD-'] = 0 "
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoj4Zx51J2MU",
        "outputId": "ebf61b06-c26f-4475-ce1b-00de2eb05478"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-14 14:22:13--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-12-14 14:22:13--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-12-14 14:22:13--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.2’\n",
            "\n",
            "glove.6B.zip.2      100%[===================>] 822.24M  5.04MB/s    in 2m 40s  \n",
            "\n",
            "2021-12-14 14:24:54 (5.13 MB/s) - ‘glove.6B.zip.2’ saved [862182613/862182613]\n",
            "\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc-T6FcLKtS9",
        "outputId": "a6027a74-da7b-451c-eb9b-557772475102"
      },
      "source": [
        "path_to_glove_file = os.path.join(os.getcwd(), \"glove.6B.100d.txt\")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding\n",
        "- matrix creation\n",
        "- padding\n",
        "- one hot encoding of tags\n",
        "- punctuation identification"
      ],
      "metadata": {
        "id": "dZmhBiMWuR6B"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0mOStEPMeXt",
        "outputId": "badf8bca-8e84-45e1-df35-bd7cb86fa9a3"
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        embedding_matrix += np.random.uniform(low=-0.05, high=0.05, size=embedding_dim)\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 10271 words (678 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpYrHs0RMtQJ"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8EZM31Js06-",
        "outputId": "a0b0a64e-76b1-430e-cccf-c88cf63fa039"
      },
      "source": [
        "MAX_LENGTH = len(max(train_sentences, key=len))\n",
        "print(MAX_LENGTH)\n",
        "MAX_LENGTH2 = len(max(valid_sentences, key=len))\n",
        "print(MAX_LENGTH2)\n",
        "MAX_LENGTH3 = len(max(test_sentences, key=len))\n",
        "print(MAX_LENGTH3)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "249\n",
            "81\n",
            "58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSRa9SwTuNrX"
      },
      "source": [
        "train_sentences_X, valid_sentences_X, test_sentences_X, train_tags_y, valid_tags_y, test_tags_y = [], [], [], [], [], []\n",
        " \n",
        "for s in train_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        "\n",
        "for s in valid_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    valid_sentences_X.append(s_int)\n",
        "\n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        " \n",
        "for s in train_tags:\n",
        "    train_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "for s in valid_tags:\n",
        "    valid_tags_y.append([tag2index[t] for t in s])\n",
        " \n",
        "for s in test_tags:\n",
        "    test_tags_y.append([tag2index[t] for t in s])"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf-XlSrStQx3"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_sentences_X = pad_sequences(valid_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_tags_y = pad_sequences(valid_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)"
      ],
      "metadata": {
        "id": "oqIJE3bnue9T"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "point = [tag2index['.']]\n",
        "virg = [tag2index[',']]\n",
        "weird_apex = [tag2index['``']]\n",
        "single_apex = [tag2index[\"''\"]]\n",
        "two_dots = [tag2index[':']]\n",
        "pad = [tag2index['-PAD-']]\n",
        "\n",
        "punct_cat_classes = to_categorical([point, virg, weird_apex, single_apex, two_dots, pad], len(tag2index))\n",
        "punct_cat_classes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI0oto57ug6y",
        "outputId": "ab439a4e-baaa-47f3-e860-406e29ac9ad8"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 1, 46)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cum_tags = np.zeros(46)\n",
        "for i in punct_cat_classes:\n",
        "  print(i[0])\n",
        "  cum_tags += i[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FsWbMp0unVb",
        "outputId": "b48364f1-d413-4b2b-a305-7f1f39d42c29"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "where_tags = np.where(np.logical_not(cum_tags))\n",
        "where_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SC2loOTEun8c",
        "outputId": "653e9762-01c5-449c-d791-d7ecd1ad24f3"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 2,  3,  4,  5,  6,  8,  9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20,\n",
              "        23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39,\n",
              "        40, 41, 42, 43, 44, 45]),)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_punct_indexes = where_tags[0]\n",
        "n_classes = len(no_punct_indexes)"
      ],
      "metadata": {
        "id": "ekJTN308u44W"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "cat_val_tags_y = to_categorical(valid_tags_y, len(tag2index))\n",
        "print(len(cat_train_tags_y), len(cat_val_tags_y))\n",
        "print(len(train_sentences_X), len(valid_sentences_X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUuX0sJ4u5pj",
        "outputId": "63b7ad9b-a803-4631-e5ae-742cec9b089c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1963 1299\n",
            "1963 1299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mini Data Vizualization Inset"
      ],
      "metadata": {
        "id": "iCRjfjVeu7qR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_tags = np.zeros(46)\n",
        "for i in cat_train_tags_y:\n",
        "  for t in i:\n",
        "    distribution_tags += t"
      ],
      "metadata": {
        "id": "njocco_evBCH"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = sum(distribution_tags[1:])\n",
        "norm = [float(i)/s for i in distribution_tags[1:]]"
      ],
      "metadata": {
        "id": "cJyeVkVvvCnK"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt "
      ],
      "metadata": {
        "id": "b9nSczkfvGFk"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(np.arange(len(norm)),norm)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J5oiLOFsvHjW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "35a22663-3616-48bd-9ff0-447a881e4dd9"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#F1 Metric"
      ],
      "metadata": {
        "id": "EuwamfeBvI6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric.\n",
        "\n",
        "    Only computes a batch-wise average of precision.\n",
        "\n",
        "    Computes the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric.\n",
        "\n",
        "    Only computes a batch-wise average of recall.\n",
        "\n",
        "    Computes the recall, a metric for multi-label classification of\n",
        "    how many relevant items are selected.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def f1_binary(y_true, y_pred):\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
        "\n",
        "def metric_wrapper(index):\n",
        "  def f1_for_class(y_true, y_pred):\n",
        "    #get only the desired class\n",
        "    true = y_true[:,:,index]\n",
        "    pred = y_pred[:,:,index]\n",
        "    #return dice per class\n",
        "    tmp = f1_binary(true,pred)\n",
        "    return tmp\n",
        "  f1_for_class.__name__ = 'f1_' + str(index)\n",
        "  return f1_for_class\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    result = 0.0\n",
        "    for class_id in no_punct_indexes:\n",
        "        y_true_single_class = y_true[:,:,class_id]\n",
        "        y_pred_single_class = y_pred[:,:,class_id]\n",
        "        f1_single = f1_binary(y_true_single_class, y_pred_single_class)\n",
        "        result += f1_single / float(n_classes)\n",
        "    return result"
      ],
      "metadata": {
        "id": "kL2FW8K7DSdy"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model compile and fit"
      ],
      "metadata": {
        "id": "ew3TwrNJvOOR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsVGByW_sdec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "991f7934-7c67-4cec-fcdd-bae3ed3305a1"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(embedding_layer)\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(Dense(len(tag2index)))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=Adam(0.001),\n",
        "             metrics=['accuracy', f1, [metric_wrapper(i) for i in no_punct_indexes]])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 249, 100)          1094900   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 249, 512)         731136    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 249, 128)          328192    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 249, 46)           5934      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,160,162\n",
            "Trainable params: 1,065,262\n",
            "Non-trainable params: 1,094,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "history = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=40, validation_data=(valid_sentences_X, cat_val_tags_y), callbacks=[callback])"
      ],
      "metadata": {
        "id": "f4BHmKzMvUpH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0530ddb2-381b-4ea0-f509-7240e230b020"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "16/16 [==============================] - 125s 7s/step - loss: 0.8262 - accuracy: 0.8442 - f1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.3845 - val_accuracy: 0.9048 - val_f1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - 103s 6s/step - loss: 0.3406 - accuracy: 0.9115 - f1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.3139 - val_accuracy: 0.9164 - val_f1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - 105s 7s/step - loss: 0.3027 - accuracy: 0.9157 - f1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2937 - val_accuracy: 0.9199 - val_f1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - 104s 7s/step - loss: 0.2884 - accuracy: 0.9195 - f1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2843 - val_accuracy: 0.9216 - val_f1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - 102s 6s/step - loss: 0.2802 - accuracy: 0.9216 - f1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2782 - val_accuracy: 0.9226 - val_f1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - 106s 7s/step - loss: 0.2733 - accuracy: 0.9237 - f1: 2.0139e-04 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0081 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2708 - val_accuracy: 0.9246 - val_f1: 3.0102e-05 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0012 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - 102s 6s/step - loss: 0.2649 - accuracy: 0.9262 - f1: 8.0848e-04 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0323 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2644 - val_accuracy: 0.9262 - val_f1: 0.0011 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0460 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - 101s 6s/step - loss: 0.2558 - accuracy: 0.9287 - f1: 0.0035 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.1388 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2512 - val_accuracy: 0.9308 - val_f1: 0.0022 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0889 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - 102s 6s/step - loss: 0.2425 - accuracy: 0.9338 - f1: 0.0069 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.2754 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2369 - val_accuracy: 0.9359 - val_f1: 0.0065 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.2584 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - 102s 6s/step - loss: 0.2266 - accuracy: 0.9397 - f1: 0.0104 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.4159 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2204 - val_accuracy: 0.9423 - val_f1: 0.0108 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.4305 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - 101s 6s/step - loss: 0.2086 - accuracy: 0.9455 - f1: 0.0130 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0028 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0075 - f1_36: 0.0000e+00 - f1_37: 0.5053 - f1_38: 0.0033 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2011 - val_accuracy: 0.9475 - val_f1: 0.0142 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0104 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0448 - val_f1_36: 0.0000e+00 - val_f1_37: 0.4807 - val_f1_38: 0.0323 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - 102s 6s/step - loss: 0.1893 - accuracy: 0.9508 - f1: 0.0234 - f1_2: 0.0354 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0494 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.1914 - f1_36: 0.0000e+00 - f1_37: 0.5783 - f1_38: 0.0811 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1836 - val_accuracy: 0.9531 - val_f1: 0.0323 - val_f1_2: 0.0986 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0722 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.4741 - val_f1_36: 0.0000e+00 - val_f1_37: 0.4471 - val_f1_38: 0.1984 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - 102s 6s/step - loss: 0.1715 - accuracy: 0.9551 - f1: 0.0507 - f1_2: 0.3954 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.1761 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0051 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.5467 - f1_36: 0.0000e+00 - f1_37: 0.6223 - f1_38: 0.2835 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1658 - val_accuracy: 0.9562 - val_f1: 0.0663 - val_f1_2: 0.7305 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.2391 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0102 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.6408 - val_f1_36: 0.0000e+00 - val_f1_37: 0.6058 - val_f1_38: 0.4258 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - 102s 6s/step - loss: 0.1555 - accuracy: 0.9589 - f1: 0.0732 - f1_2: 0.8172 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.2923 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0339 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.7192 - f1_36: 0.0000e+00 - f1_37: 0.6628 - f1_38: 0.4013 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1516 - val_accuracy: 0.9606 - val_f1: 0.0778 - val_f1_2: 0.8703 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0014 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.4257 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0377 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.7590 - val_f1_36: 0.0000e+00 - val_f1_37: 0.5689 - val_f1_38: 0.4499 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - 102s 6s/step - loss: 0.1420 - accuracy: 0.9629 - f1: 0.0860 - f1_2: 0.8964 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0160 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.4103 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.1390 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0251 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.7827 - f1_36: 0.0000e+00 - f1_37: 0.6895 - f1_38: 0.4803 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1397 - val_accuracy: 0.9639 - val_f1: 0.0906 - val_f1_2: 0.9060 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0566 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.3902 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0876 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.1308 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.7928 - val_f1_36: 0.0000e+00 - val_f1_37: 0.6834 - val_f1_38: 0.5755 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 9.6200e-04\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - 102s 6s/step - loss: 0.1299 - accuracy: 0.9666 - f1: 0.1084 - f1_2: 0.9165 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.1585 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.4951 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.2353 - f1_14: 0.0000e+00 - f1_15: 0.0073 - f1_17: 0.0000e+00 - f1_18: 0.3941 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.8308 - f1_36: 0.0000e+00 - f1_37: 0.7179 - f1_38: 0.5682 - f1_39: 0.0024 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0089 - val_loss: 0.1290 - val_accuracy: 0.9660 - val_f1: 0.1154 - val_f1_2: 0.9233 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.1659 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.5550 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.1861 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0301 - val_f1_17: 0.0000e+00 - val_f1_18: 0.5035 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8213 - val_f1_36: 0.0000e+00 - val_f1_37: 0.6974 - val_f1_38: 0.6308 - val_f1_39: 0.0032 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0929 - val_f1_45: 0.0048\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - 102s 6s/step - loss: 0.1195 - accuracy: 0.9695 - f1: 0.1416 - f1_2: 0.9211 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.3014 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.5569 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.3557 - f1_14: 0.0000e+00 - f1_15: 0.2604 - f1_17: 0.0000e+00 - f1_18: 0.7695 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.8516 - f1_36: 0.0000e+00 - f1_37: 0.7410 - f1_38: 0.6021 - f1_39: 0.0203 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.2389 - f1_45: 0.0445 - val_loss: 0.1195 - val_accuracy: 0.9684 - val_f1: 0.1556 - val_f1_2: 0.9295 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.3725 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0045 - val_f1_9: 0.0000e+00 - val_f1_10: 0.6343 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.2984 - val_f1_14: 0.0000e+00 - val_f1_15: 0.4649 - val_f1_17: 0.0000e+00 - val_f1_18: 0.8535 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8509 - val_f1_36: 0.0000e+00 - val_f1_37: 0.7060 - val_f1_38: 0.6574 - val_f1_39: 0.0781 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.3505 - val_f1_45: 0.0229\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - 102s 6s/step - loss: 0.1107 - accuracy: 0.9719 - f1: 0.1786 - f1_2: 0.9278 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.4671 - f1_6: 0.0046 - f1_8: 0.0155 - f1_9: 0.0000e+00 - f1_10: 0.6152 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.4613 - f1_14: 0.0000e+00 - f1_15: 0.6258 - f1_17: 0.0000e+00 - f1_18: 0.9217 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.0471 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.8631 - f1_36: 0.0000e+00 - f1_37: 0.7647 - f1_38: 0.6415 - f1_39: 0.1163 - f1_40: 0.0061 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.5427 - f1_45: 0.1229 - val_loss: 0.1113 - val_accuracy: 0.9713 - val_f1: 0.1917 - val_f1_2: 0.9306 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.5128 - val_f1_6: 0.0180 - val_f1_8: 0.0534 - val_f1_9: 0.0000e+00 - val_f1_10: 0.6277 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.4367 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6741 - val_f1_17: 0.0000e+00 - val_f1_18: 0.9519 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.0718 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8633 - val_f1_36: 0.0000e+00 - val_f1_37: 0.6886 - val_f1_38: 0.6926 - val_f1_39: 0.1958 - val_f1_40: 0.0174 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.8008 - val_f1_45: 0.1326\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - 102s 6s/step - loss: 0.1676 - accuracy: 0.9550 - f1: 0.1404 - f1_2: 0.8806 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.3147 - f1_6: 0.0449 - f1_8: 0.0457 - f1_9: 0.0000e+00 - f1_10: 0.4518 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.3119 - f1_14: 0.0000e+00 - f1_15: 0.4252 - f1_17: 0.0000e+00 - f1_18: 0.6373 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.1308 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.7933 - f1_36: 0.0000e+00 - f1_37: 0.4338 - f1_38: 0.4241 - f1_39: 0.1481 - f1_40: 0.0219 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.4487 - f1_45: 0.1051 - val_loss: 0.1873 - val_accuracy: 0.9484 - val_f1: 0.0477 - val_f1_2: 0.8695 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.3875 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.6368 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0116 - val_f1_38: 0.0031 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - 102s 6s/step - loss: 0.1550 - accuracy: 0.9612 - f1: 0.0685 - f1_2: 0.8081 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0044 - f1_6: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.2957 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0875 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.2392 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.7032 - f1_36: 0.0000e+00 - f1_37: 0.4543 - f1_38: 0.1458 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 5.7604e-04 - val_loss: 0.1345 - val_accuracy: 0.9659 - val_f1: 0.0981 - val_f1_2: 0.9172 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0452 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.4769 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.3309 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0024 - val_f1_17: 0.0000e+00 - val_f1_18: 0.3200 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8311 - val_f1_36: 0.0000e+00 - val_f1_37: 0.5569 - val_f1_38: 0.4422 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - 101s 6s/step - loss: 0.1220 - accuracy: 0.9691 - f1: 0.1243 - f1_2: 0.9165 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.2580 - f1_6: 0.0000e+00 - f1_8: 0.0019 - f1_9: 0.0000e+00 - f1_10: 0.5563 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.3481 - f1_14: 0.0000e+00 - f1_15: 0.0115 - f1_17: 0.0000e+00 - f1_18: 0.7321 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.8443 - f1_36: 0.0000e+00 - f1_37: 0.6692 - f1_38: 0.5658 - f1_39: 0.0115 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0560 - val_loss: 0.1169 - val_accuracy: 0.9696 - val_f1: 0.1489 - val_f1_2: 0.9220 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.4627 - val_f1_6: 0.0000e+00 - val_f1_8: 0.0079 - val_f1_9: 0.0000e+00 - val_f1_10: 0.6168 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.3245 - val_f1_14: 0.0000e+00 - val_f1_15: 0.2686 - val_f1_17: 0.0000e+00 - val_f1_18: 0.9749 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8481 - val_f1_36: 0.0000e+00 - val_f1_37: 0.6554 - val_f1_38: 0.6761 - val_f1_39: 0.0760 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0254 - val_f1_45: 0.0964\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - 101s 6s/step - loss: 0.1087 - accuracy: 0.9721 - f1: 0.1795 - f1_2: 0.9181 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.5305 - f1_6: 0.0179 - f1_8: 0.0758 - f1_9: 0.0000e+00 - f1_10: 0.6211 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.4707 - f1_14: 0.0000e+00 - f1_15: 0.6847 - f1_17: 0.0000e+00 - f1_18: 0.9844 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.8656 - f1_36: 0.0000e+00 - f1_37: 0.7350 - f1_38: 0.6729 - f1_39: 0.1833 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.2870 - f1_45: 0.1328 - val_loss: 0.1083 - val_accuracy: 0.9715 - val_f1: 0.1931 - val_f1_2: 0.9262 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.4838 - val_f1_6: 0.1057 - val_f1_8: 0.0424 - val_f1_9: 0.0000e+00 - val_f1_10: 0.6369 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.4478 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7398 - val_f1_17: 0.0000e+00 - val_f1_18: 0.9816 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.0537 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8684 - val_f1_36: 0.0000e+00 - val_f1_37: 0.6975 - val_f1_38: 0.7245 - val_f1_39: 0.2176 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.5855 - val_f1_45: 0.2128\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - 101s 6s/step - loss: 0.1007 - accuracy: 0.9742 - f1: 0.2305 - f1_2: 0.9208 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.5860 - f1_6: 0.2404 - f1_8: 0.2047 - f1_9: 0.0000e+00 - f1_10: 0.6688 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.5397 - f1_14: 0.0000e+00 - f1_15: 0.8052 - f1_17: 0.0000e+00 - f1_18: 0.9916 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.5456 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.8770 - f1_36: 0.0000e+00 - f1_37: 0.7498 - f1_38: 0.7116 - f1_39: 0.2820 - f1_40: 0.0127 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.8318 - f1_45: 0.2526 - val_loss: 0.1021 - val_accuracy: 0.9732 - val_f1: 0.2473 - val_f1_2: 0.9287 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.5926 - val_f1_6: 0.4879 - val_f1_8: 0.4334 - val_f1_9: 0.0000e+00 - val_f1_10: 0.6618 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.5026 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7507 - val_f1_17: 0.0000e+00 - val_f1_18: 0.9880 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.7448 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8701 - val_f1_36: 0.0000e+00 - val_f1_37: 0.7052 - val_f1_38: 0.7439 - val_f1_39: 0.2920 - val_f1_40: 0.0243 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.8784 - val_f1_45: 0.2884\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - 102s 6s/step - loss: 0.0946 - accuracy: 0.9757 - f1: 0.2604 - f1_2: 0.9253 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.6288 - f1_6: 0.5032 - f1_8: 0.4025 - f1_9: 0.0000e+00 - f1_10: 0.6899 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.5994 - f1_14: 0.0000e+00 - f1_15: 0.8067 - f1_17: 0.0000e+00 - f1_18: 0.9942 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.8295 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.8848 - f1_36: 0.0000e+00 - f1_37: 0.7759 - f1_38: 0.7376 - f1_39: 0.3602 - f1_40: 0.0372 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.9452 - f1_45: 0.2965 - val_loss: 0.0968 - val_accuracy: 0.9747 - val_f1: 0.2670 - val_f1_2: 0.9325 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.6736 - val_f1_6: 0.5512 - val_f1_8: 0.4750 - val_f1_9: 0.0000e+00 - val_f1_10: 0.6699 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.5880 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7559 - val_f1_17: 0.0000e+00 - val_f1_18: 0.9945 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.9066 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8794 - val_f1_36: 0.0000e+00 - val_f1_37: 0.7267 - val_f1_38: 0.7673 - val_f1_39: 0.4049 - val_f1_40: 0.0548 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9449 - val_f1_45: 0.3531\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - 104s 7s/step - loss: 0.0898 - accuracy: 0.9772 - f1: 0.2784 - f1_2: 0.9319 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.7110 - f1_6: 0.5948 - f1_8: 0.4871 - f1_9: 0.0000e+00 - f1_10: 0.7135 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.6461 - f1_14: 0.0000e+00 - f1_15: 0.8006 - f1_17: 0.0000e+00 - f1_18: 0.9973 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.9114 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.8887 - f1_36: 0.0000e+00 - f1_37: 0.7819 - f1_38: 0.7501 - f1_39: 0.4504 - f1_40: 0.1032 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.9866 - f1_45: 0.3805 - val_loss: 0.0924 - val_accuracy: 0.9757 - val_f1: 0.2762 - val_f1_2: 0.9369 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.6480 - val_f1_6: 0.6202 - val_f1_8: 0.5675 - val_f1_9: 0.0000e+00 - val_f1_10: 0.7105 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6033 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7606 - val_f1_17: 0.0000e+00 - val_f1_18: 0.9967 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.9211 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8900 - val_f1_36: 0.0000e+00 - val_f1_37: 0.7441 - val_f1_38: 0.7809 - val_f1_39: 0.4310 - val_f1_40: 0.1039 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9578 - val_f1_45: 0.3761\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - 104s 7s/step - loss: 0.0854 - accuracy: 0.9784 - f1: 0.2913 - f1_2: 0.9348 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.7174 - f1_6: 0.6474 - f1_8: 0.5779 - f1_9: 0.0000e+00 - f1_10: 0.7331 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.6853 - f1_14: 0.0000e+00 - f1_15: 0.8161 - f1_17: 0.0000e+00 - f1_18: 0.9985 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.9303 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0266 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.8960 - f1_36: 0.0304 - f1_37: 0.7916 - f1_38: 0.7664 - f1_39: 0.5463 - f1_40: 0.1433 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.9901 - f1_45: 0.4222 - val_loss: 0.0887 - val_accuracy: 0.9767 - val_f1: 0.2918 - val_f1_2: 0.9391 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.7199 - val_f1_6: 0.6669 - val_f1_8: 0.6923 - val_f1_9: 0.0000e+00 - val_f1_10: 0.7134 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6191 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7674 - val_f1_17: 0.0000e+00 - val_f1_18: 0.9994 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.9424 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0412 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8924 - val_f1_36: 0.0187 - val_f1_37: 0.7535 - val_f1_38: 0.7953 - val_f1_39: 0.5221 - val_f1_40: 0.1980 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9768 - val_f1_45: 0.4140\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - 104s 7s/step - loss: 0.0817 - accuracy: 0.9794 - f1: 0.3055 - f1_2: 0.9354 - f1_3: 0.0011 - f1_4: 0.0000e+00 - f1_5: 0.7450 - f1_6: 0.7033 - f1_8: 0.6683 - f1_9: 0.0000e+00 - f1_10: 0.7400 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.7075 - f1_14: 0.0000e+00 - f1_15: 0.8185 - f1_17: 0.0000e+00 - f1_18: 0.9987 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.9351 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0046 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.1231 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.9021 - f1_36: 0.0655 - f1_37: 0.8016 - f1_38: 0.7837 - f1_39: 0.5784 - f1_40: 0.2533 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.9905 - f1_45: 0.4629 - val_loss: 0.0853 - val_accuracy: 0.9775 - val_f1: 0.3013 - val_f1_2: 0.9398 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.7252 - val_f1_6: 0.6755 - val_f1_8: 0.7434 - val_f1_9: 0.0000e+00 - val_f1_10: 0.7375 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6429 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7665 - val_f1_17: 0.0000e+00 - val_f1_18: 1.0000 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.9445 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.1406 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8989 - val_f1_36: 0.0623 - val_f1_37: 0.7545 - val_f1_38: 0.8152 - val_f1_39: 0.5598 - val_f1_40: 0.2346 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9768 - val_f1_45: 0.4341\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - 103s 6s/step - loss: 0.0784 - accuracy: 0.9801 - f1: 0.3190 - f1_2: 0.9373 - f1_3: 0.0036 - f1_4: 0.0000e+00 - f1_5: 0.7505 - f1_6: 0.7273 - f1_8: 0.7439 - f1_9: 0.0000e+00 - f1_10: 0.7550 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.7298 - f1_14: 0.0000e+00 - f1_15: 0.8269 - f1_17: 0.0000e+00 - f1_18: 0.9986 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.9321 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0111 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.3093 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.9061 - f1_36: 0.1436 - f1_37: 0.8048 - f1_38: 0.8035 - f1_39: 0.6175 - f1_40: 0.2826 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.9948 - f1_45: 0.4809 - val_loss: 0.0821 - val_accuracy: 0.9786 - val_f1: 0.3142 - val_f1_2: 0.9395 - val_f1_3: 0.0057 - val_f1_4: 0.0000e+00 - val_f1_5: 0.7450 - val_f1_6: 0.7153 - val_f1_8: 0.7698 - val_f1_9: 0.0000e+00 - val_f1_10: 0.7411 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6659 - val_f1_14: 0.0087 - val_f1_15: 0.7645 - val_f1_17: 0.0000e+00 - val_f1_18: 1.0000 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.9445 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.2991 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.9012 - val_f1_36: 0.1634 - val_f1_37: 0.7561 - val_f1_38: 0.8165 - val_f1_39: 0.5857 - val_f1_40: 0.2695 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9923 - val_f1_45: 0.4825\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - 102s 6s/step - loss: 0.0756 - accuracy: 0.9808 - f1: 0.3319 - f1_2: 0.9374 - f1_3: 0.0194 - f1_4: 0.0000e+00 - f1_5: 0.7791 - f1_6: 0.7507 - f1_8: 0.7821 - f1_9: 0.0000e+00 - f1_10: 0.7614 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.7379 - f1_14: 0.0521 - f1_15: 0.8207 - f1_17: 0.0000e+00 - f1_18: 0.9987 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.9458 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0102 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.4136 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.9107 - f1_36: 0.2658 - f1_37: 0.8083 - f1_38: 0.8118 - f1_39: 0.6570 - f1_40: 0.3117 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.9941 - f1_45: 0.5092 - val_loss: 0.0796 - val_accuracy: 0.9791 - val_f1: 0.3295 - val_f1_2: 0.9396 - val_f1_3: 0.0160 - val_f1_4: 0.0000e+00 - val_f1_5: 0.7676 - val_f1_6: 0.7356 - val_f1_8: 0.7960 - val_f1_9: 0.0000e+00 - val_f1_10: 0.7643 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6805 - val_f1_14: 0.0542 - val_f1_15: 0.7670 - val_f1_17: 0.0000e+00 - val_f1_18: 1.0000 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.9477 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0312 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.4253 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.9052 - val_f1_36: 0.3492 - val_f1_37: 0.7364 - val_f1_38: 0.8310 - val_f1_39: 0.5826 - val_f1_40: 0.3589 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9975 - val_f1_45: 0.4923\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - 101s 6s/step - loss: 0.0728 - accuracy: 0.9815 - f1: 0.3466 - f1_2: 0.9379 - f1_3: 0.0323 - f1_4: 0.0000e+00 - f1_5: 0.7917 - f1_6: 0.7772 - f1_8: 0.8142 - f1_9: 0.0000e+00 - f1_10: 0.7728 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.7549 - f1_14: 0.2111 - f1_15: 0.8261 - f1_17: 0.0000e+00 - f1_18: 0.9986 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.9435 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0228 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.5202 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.9120 - f1_36: 0.3703 - f1_37: 0.8111 - f1_38: 0.8232 - f1_39: 0.6458 - f1_40: 0.3783 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.9934 - f1_45: 0.5269 - val_loss: 0.0772 - val_accuracy: 0.9800 - val_f1: 0.3437 - val_f1_2: 0.9412 - val_f1_3: 0.0259 - val_f1_4: 0.0000e+00 - val_f1_5: 0.7846 - val_f1_6: 0.7568 - val_f1_8: 0.7977 - val_f1_9: 0.0000e+00 - val_f1_10: 0.7486 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6877 - val_f1_14: 0.2300 - val_f1_15: 0.7821 - val_f1_17: 0.0000e+00 - val_f1_18: 1.0000 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.9477 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0609 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.5044 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.9050 - val_f1_36: 0.3923 - val_f1_37: 0.7597 - val_f1_38: 0.8328 - val_f1_39: 0.6471 - val_f1_40: 0.4190 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9975 - val_f1_45: 0.5251\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - 102s 6s/step - loss: 0.0703 - accuracy: 0.9822 - f1: 0.3591 - f1_2: 0.9410 - f1_3: 0.0409 - f1_4: 0.0000e+00 - f1_5: 0.8043 - f1_6: 0.7998 - f1_8: 0.8316 - f1_9: 0.0000e+00 - f1_10: 0.7751 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.7616 - f1_14: 0.3652 - f1_15: 0.8394 - f1_17: 0.0000e+00 - f1_18: 0.9987 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.9466 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0556 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.5951 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.9156 - f1_36: 0.4148 - f1_37: 0.8198 - f1_38: 0.8304 - f1_39: 0.6809 - f1_40: 0.3969 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.9935 - f1_45: 0.5579 - val_loss: 0.0747 - val_accuracy: 0.9805 - val_f1: 0.3549 - val_f1_2: 0.9447 - val_f1_3: 0.0679 - val_f1_4: 0.0000e+00 - val_f1_5: 0.7890 - val_f1_6: 0.7877 - val_f1_8: 0.8024 - val_f1_9: 0.0000e+00 - val_f1_10: 0.7607 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.7244 - val_f1_14: 0.3997 - val_f1_15: 0.8019 - val_f1_17: 0.0000e+00 - val_f1_18: 1.0000 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.9495 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0818 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.5402 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.9115 - val_f1_36: 0.4490 - val_f1_37: 0.7727 - val_f1_38: 0.8315 - val_f1_39: 0.6468 - val_f1_40: 0.3947 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9975 - val_f1_45: 0.5428\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - 102s 6s/step - loss: 0.0679 - accuracy: 0.9828 - f1: 0.3689 - f1_2: 0.9429 - f1_3: 0.0692 - f1_4: 0.0000e+00 - f1_5: 0.8151 - f1_6: 0.8233 - f1_8: 0.8444 - f1_9: 0.0000e+00 - f1_10: 0.7869 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.7688 - f1_14: 0.4709 - f1_15: 0.8496 - f1_17: 0.0000e+00 - f1_18: 0.9975 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.9409 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0825 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.6553 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.9182 - f1_36: 0.4230 - f1_37: 0.8262 - f1_38: 0.8322 - f1_39: 0.7092 - f1_40: 0.4288 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.9929 - f1_45: 0.5779 - val_loss: 0.0724 - val_accuracy: 0.9810 - val_f1: 0.3661 - val_f1_2: 0.9445 - val_f1_3: 0.0787 - val_f1_4: 0.0000e+00 - val_f1_5: 0.8033 - val_f1_6: 0.8264 - val_f1_8: 0.8438 - val_f1_9: 0.0000e+00 - val_f1_10: 0.7685 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.7131 - val_f1_14: 0.5689 - val_f1_15: 0.8132 - val_f1_17: 0.0000e+00 - val_f1_18: 1.0000 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.9495 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0890 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.6111 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.9145 - val_f1_36: 0.4507 - val_f1_37: 0.7751 - val_f1_38: 0.8485 - val_f1_39: 0.6590 - val_f1_40: 0.4272 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9975 - val_f1_45: 0.5596\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - 104s 7s/step - loss: 0.0658 - accuracy: 0.9832 - f1: 0.3805 - f1_2: 0.9444 - f1_3: 0.0871 - f1_4: 0.0000e+00 - f1_5: 0.8193 - f1_6: 0.8395 - f1_8: 0.8646 - f1_9: 0.0000e+00 - f1_10: 0.7908 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.7862 - f1_14: 0.6698 - f1_15: 0.8522 - f1_17: 0.0000e+00 - f1_18: 0.9987 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.9414 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.1201 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.6809 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.9214 - f1_36: 0.4697 - f1_37: 0.8279 - f1_38: 0.8419 - f1_39: 0.7267 - f1_40: 0.4520 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.9933 - f1_45: 0.5922 - val_loss: 0.0706 - val_accuracy: 0.9814 - val_f1: 0.3737 - val_f1_2: 0.9509 - val_f1_3: 0.0984 - val_f1_4: 0.0000e+00 - val_f1_5: 0.8022 - val_f1_6: 0.8503 - val_f1_8: 0.8555 - val_f1_9: 0.0000e+00 - val_f1_10: 0.7807 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.7038 - val_f1_14: 0.6427 - val_f1_15: 0.8274 - val_f1_17: 0.0000e+00 - val_f1_18: 1.0000 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.9634 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.1170 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.6787 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.9158 - val_f1_36: 0.4453 - val_f1_37: 0.7854 - val_f1_38: 0.8451 - val_f1_39: 0.7057 - val_f1_40: 0.4808 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9975 - val_f1_45: 0.5005\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - 106s 7s/step - loss: 0.0638 - accuracy: 0.9838 - f1: 0.3900 - f1_2: 0.9438 - f1_3: 0.1407 - f1_4: 0.0000e+00 - f1_5: 0.8243 - f1_6: 0.8508 - f1_8: 0.8670 - f1_9: 0.0000e+00 - f1_10: 0.7978 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.7909 - f1_14: 0.7203 - f1_15: 0.8676 - f1_17: 0.0000e+00 - f1_18: 0.9987 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.9447 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.1640 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.7436 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.9231 - f1_36: 0.4896 - f1_37: 0.8361 - f1_38: 0.8601 - f1_39: 0.7444 - f1_40: 0.4907 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.9919 - f1_45: 0.6118 - val_loss: 0.0695 - val_accuracy: 0.9814 - val_f1: 0.3784 - val_f1_2: 0.9514 - val_f1_3: 0.1221 - val_f1_4: 0.0000e+00 - val_f1_5: 0.8185 - val_f1_6: 0.8710 - val_f1_8: 0.8785 - val_f1_9: 0.0000e+00 - val_f1_10: 0.7696 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6998 - val_f1_14: 0.6780 - val_f1_15: 0.8393 - val_f1_17: 0.0000e+00 - val_f1_18: 1.0000 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.9670 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.1185 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.7134 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.9171 - val_f1_36: 0.4598 - val_f1_37: 0.7880 - val_f1_38: 0.8469 - val_f1_39: 0.7285 - val_f1_40: 0.4538 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9975 - val_f1_45: 0.5175\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - 104s 7s/step - loss: 0.0620 - accuracy: 0.9840 - f1: 0.3957 - f1_2: 0.9464 - f1_3: 0.1620 - f1_4: 0.0000e+00 - f1_5: 0.8369 - f1_6: 0.8671 - f1_8: 0.8828 - f1_9: 0.0000e+00 - f1_10: 0.8017 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.7985 - f1_14: 0.7801 - f1_15: 0.8714 - f1_17: 0.0000e+00 - f1_18: 0.9984 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.9481 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.1648 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.7896 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.9278 - f1_36: 0.5081 - f1_37: 0.8285 - f1_38: 0.8641 - f1_39: 0.7544 - f1_40: 0.4847 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.9944 - f1_45: 0.6179 - val_loss: 0.0673 - val_accuracy: 0.9823 - val_f1: 0.3885 - val_f1_2: 0.9523 - val_f1_3: 0.2291 - val_f1_4: 0.0000e+00 - val_f1_5: 0.8061 - val_f1_6: 0.8777 - val_f1_8: 0.9017 - val_f1_9: 0.0000e+00 - val_f1_10: 0.7947 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.7011 - val_f1_14: 0.7250 - val_f1_15: 0.8409 - val_f1_17: 0.0000e+00 - val_f1_18: 1.0000 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.9682 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.1704 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.7519 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.9194 - val_f1_36: 0.5205 - val_f1_37: 0.7880 - val_f1_38: 0.8406 - val_f1_39: 0.7267 - val_f1_40: 0.4688 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9975 - val_f1_45: 0.5601\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - 102s 6s/step - loss: 0.0600 - accuracy: 0.9846 - f1: 0.4036 - f1_2: 0.9485 - f1_3: 0.2139 - f1_4: 0.0000e+00 - f1_5: 0.8437 - f1_6: 0.8702 - f1_8: 0.8982 - f1_9: 0.0000e+00 - f1_10: 0.8080 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.8075 - f1_14: 0.7864 - f1_15: 0.8784 - f1_17: 0.0000e+00 - f1_18: 0.9986 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.9450 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.2272 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.8039 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.9274 - f1_36: 0.5545 - f1_37: 0.8392 - f1_38: 0.8701 - f1_39: 0.7663 - f1_40: 0.5258 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.9944 - f1_45: 0.6350 - val_loss: 0.0654 - val_accuracy: 0.9827 - val_f1: 0.3957 - val_f1_2: 0.9530 - val_f1_3: 0.2269 - val_f1_4: 0.0000e+00 - val_f1_5: 0.8224 - val_f1_6: 0.8809 - val_f1_8: 0.8899 - val_f1_9: 0.0000e+00 - val_f1_10: 0.7926 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.7226 - val_f1_14: 0.7639 - val_f1_15: 0.8472 - val_f1_17: 0.0000e+00 - val_f1_18: 1.0000 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.9682 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.2225 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.7688 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.9225 - val_f1_36: 0.5583 - val_f1_37: 0.7914 - val_f1_38: 0.8627 - val_f1_39: 0.7506 - val_f1_40: 0.5202 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9975 - val_f1_45: 0.5678\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - 102s 6s/step - loss: 0.0583 - accuracy: 0.9850 - f1: 0.4087 - f1_2: 0.9489 - f1_3: 0.2521 - f1_4: 0.0000e+00 - f1_5: 0.8455 - f1_6: 0.8761 - f1_8: 0.9113 - f1_9: 0.0000e+00 - f1_10: 0.8113 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.8111 - f1_14: 0.8082 - f1_15: 0.8817 - f1_17: 0.0000e+00 - f1_18: 0.9986 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.9455 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.2591 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.8338 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.9303 - f1_36: 0.5715 - f1_37: 0.8404 - f1_38: 0.8740 - f1_39: 0.7711 - f1_40: 0.5356 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.9947 - f1_45: 0.6467 - val_loss: 0.0635 - val_accuracy: 0.9832 - val_f1: 0.4015 - val_f1_2: 0.9523 - val_f1_3: 0.2598 - val_f1_4: 0.0000e+00 - val_f1_5: 0.8320 - val_f1_6: 0.8779 - val_f1_8: 0.8849 - val_f1_9: 0.0000e+00 - val_f1_10: 0.7985 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.7443 - val_f1_14: 0.7723 - val_f1_15: 0.8582 - val_f1_17: 0.0000e+00 - val_f1_18: 1.0000 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.9682 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.2480 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.7939 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.9247 - val_f1_36: 0.6098 - val_f1_37: 0.7882 - val_f1_38: 0.8647 - val_f1_39: 0.7388 - val_f1_40: 0.5175 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9975 - val_f1_45: 0.6266\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - 102s 6s/step - loss: 0.0567 - accuracy: 0.9853 - f1: 0.4128 - f1_2: 0.9512 - f1_3: 0.2714 - f1_4: 0.0000e+00 - f1_5: 0.8424 - f1_6: 0.8802 - f1_8: 0.9190 - f1_9: 0.0000e+00 - f1_10: 0.8128 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.8158 - f1_14: 0.8154 - f1_15: 0.8982 - f1_17: 0.0000e+00 - f1_18: 0.9985 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.9458 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.3000 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.8246 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.9342 - f1_36: 0.5860 - f1_37: 0.8484 - f1_38: 0.8844 - f1_39: 0.7770 - f1_40: 0.5539 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.9940 - f1_45: 0.6607 - val_loss: 0.0626 - val_accuracy: 0.9833 - val_f1: 0.4068 - val_f1_2: 0.9540 - val_f1_3: 0.2773 - val_f1_4: 0.0000e+00 - val_f1_5: 0.8180 - val_f1_6: 0.8747 - val_f1_8: 0.8911 - val_f1_9: 0.0000e+00 - val_f1_10: 0.8114 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.7277 - val_f1_14: 0.7912 - val_f1_15: 0.8646 - val_f1_17: 0.0000e+00 - val_f1_18: 1.0000 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.9682 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.3301 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.8100 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.9308 - val_f1_36: 0.6286 - val_f1_37: 0.7787 - val_f1_38: 0.8644 - val_f1_39: 0.7541 - val_f1_40: 0.5560 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9975 - val_f1_45: 0.6433\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - 102s 6s/step - loss: 0.0554 - accuracy: 0.9855 - f1: 0.4190 - f1_2: 0.9522 - f1_3: 0.3131 - f1_4: 0.0052 - f1_5: 0.8501 - f1_6: 0.8822 - f1_8: 0.9264 - f1_9: 0.0000e+00 - f1_10: 0.8163 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.8238 - f1_14: 0.8322 - f1_15: 0.8993 - f1_17: 0.0000e+00 - f1_18: 0.9986 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.9566 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.3591 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.8608 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.9338 - f1_36: 0.6105 - f1_37: 0.8478 - f1_38: 0.8815 - f1_39: 0.7888 - f1_40: 0.5700 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.9940 - f1_45: 0.6588 - val_loss: 0.0610 - val_accuracy: 0.9836 - val_f1: 0.4115 - val_f1_2: 0.9549 - val_f1_3: 0.3055 - val_f1_4: 0.0000e+00 - val_f1_5: 0.8346 - val_f1_6: 0.8772 - val_f1_8: 0.8978 - val_f1_9: 0.0000e+00 - val_f1_10: 0.7860 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.7639 - val_f1_14: 0.8155 - val_f1_15: 0.8858 - val_f1_17: 0.0000e+00 - val_f1_18: 1.0000 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.9782 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.3350 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.8161 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.9333 - val_f1_36: 0.6296 - val_f1_37: 0.7980 - val_f1_38: 0.8795 - val_f1_39: 0.7654 - val_f1_40: 0.5549 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9975 - val_f1_45: 0.6530\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - 104s 7s/step - loss: 0.0541 - accuracy: 0.9859 - f1: 0.4262 - f1_2: 0.9534 - f1_3: 0.3649 - f1_4: 0.1039 - f1_5: 0.8528 - f1_6: 0.8858 - f1_8: 0.9220 - f1_9: 0.0000e+00 - f1_10: 0.8136 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.8243 - f1_14: 0.8462 - f1_15: 0.9178 - f1_17: 0.0000e+00 - f1_18: 0.9987 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_23: 0.9636 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.3797 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.8626 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.9363 - f1_36: 0.6214 - f1_37: 0.8477 - f1_38: 0.8901 - f1_39: 0.7935 - f1_40: 0.5949 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.9953 - f1_45: 0.6781 - val_loss: 0.0604 - val_accuracy: 0.9835 - val_f1: 0.4170 - val_f1_2: 0.9570 - val_f1_3: 0.3008 - val_f1_4: 0.2167 - val_f1_5: 0.8366 - val_f1_6: 0.8827 - val_f1_8: 0.9237 - val_f1_9: 0.0000e+00 - val_f1_10: 0.8188 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.7207 - val_f1_14: 0.8400 - val_f1_15: 0.8861 - val_f1_17: 0.0000e+00 - val_f1_18: 1.0000 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_23: 0.9831 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.3558 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.8425 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.9307 - val_f1_36: 0.6421 - val_f1_37: 0.7973 - val_f1_38: 0.8584 - val_f1_39: 0.7675 - val_f1_40: 0.5347 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9975 - val_f1_45: 0.5885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#History f1 for class "
      ],
      "metadata": {
        "id": "7d6dhJH1vWMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_val_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_val_list[i] = history.history['val_f1_{}'.format(i)][-1]\n",
        "f1_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_list[i] = history.history['f1_{}'.format(i)][-1]"
      ],
      "metadata": {
        "id": "Kcg3joq5vbN1"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index2tag = {value : key for (key, value) in tag2index.items()}\n",
        "index2tag"
      ],
      "metadata": {
        "id": "GTu3brWTvdZe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80079bd5-90cd-4eaa-f4a4-10082a17fe0d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '-PAD-',\n",
              " 1: ':',\n",
              " 2: 'DT',\n",
              " 3: 'RB',\n",
              " 4: 'WDT',\n",
              " 5: 'VB',\n",
              " 6: 'VBZ',\n",
              " 7: \"''\",\n",
              " 8: 'PRP',\n",
              " 9: 'FW',\n",
              " 10: 'NN',\n",
              " 11: 'EX',\n",
              " 12: 'RBS',\n",
              " 13: 'NNS',\n",
              " 14: 'PRP$',\n",
              " 15: 'CC',\n",
              " 16: '``',\n",
              " 17: 'NNPS',\n",
              " 18: 'TO',\n",
              " 19: 'UH',\n",
              " 20: 'RP',\n",
              " 21: ',',\n",
              " 22: '.',\n",
              " 23: 'POS',\n",
              " 24: 'WRB',\n",
              " 25: 'WP$',\n",
              " 26: 'WP',\n",
              " 27: '-LRB-',\n",
              " 28: 'VBG',\n",
              " 29: 'JJR',\n",
              " 30: 'SYM',\n",
              " 31: 'PDT',\n",
              " 32: 'MD',\n",
              " 33: 'JJS',\n",
              " 34: 'LS',\n",
              " 35: 'IN',\n",
              " 36: 'VBP',\n",
              " 37: 'NNP',\n",
              " 38: 'CD',\n",
              " 39: 'VBD',\n",
              " 40: 'VBN',\n",
              " 41: 'RBR',\n",
              " 42: '-RRB-',\n",
              " 43: '#',\n",
              " 44: '$',\n",
              " 45: 'JJ'}"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- F1: {}'.format(index2tag[i], f1_list[i]))"
      ],
      "metadata": {
        "id": "1gTQPWJkveDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e823fb48-11bf-4834-c3d4-90b23d64f37c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: DT --- F1: 0.9533611536026001\n",
            "Tag: RB --- F1: 0.364933580160141\n",
            "Tag: WDT --- F1: 0.10393527150154114\n",
            "Tag: VB --- F1: 0.8527644276618958\n",
            "Tag: VBZ --- F1: 0.8857626914978027\n",
            "Tag: PRP --- F1: 0.9220440983772278\n",
            "Tag: FW --- F1: 0.0\n",
            "Tag: NN --- F1: 0.8136210441589355\n",
            "Tag: EX --- F1: 0.0\n",
            "Tag: RBS --- F1: 0.0\n",
            "Tag: NNS --- F1: 0.8243310451507568\n",
            "Tag: PRP$ --- F1: 0.8461764454841614\n",
            "Tag: CC --- F1: 0.917809784412384\n",
            "Tag: NNPS --- F1: 0.0\n",
            "Tag: TO --- F1: 0.9986765384674072\n",
            "Tag: UH --- F1: 0.0\n",
            "Tag: RP --- F1: 0.0\n",
            "Tag: POS --- F1: 0.9635649919509888\n",
            "Tag: WRB --- F1: 0.0\n",
            "Tag: WP$ --- F1: 0.0\n",
            "Tag: WP --- F1: 0.0\n",
            "Tag: -LRB- --- F1: 0.0\n",
            "Tag: VBG --- F1: 0.3797420263290405\n",
            "Tag: JJR --- F1: 0.0\n",
            "Tag: SYM --- F1: 0.0\n",
            "Tag: PDT --- F1: 0.0\n",
            "Tag: MD --- F1: 0.8625609278678894\n",
            "Tag: JJS --- F1: 0.0\n",
            "Tag: LS --- F1: 0.0\n",
            "Tag: IN --- F1: 0.9363183379173279\n",
            "Tag: VBP --- F1: 0.6213510632514954\n",
            "Tag: NNP --- F1: 0.8476958274841309\n",
            "Tag: CD --- F1: 0.8901400566101074\n",
            "Tag: VBD --- F1: 0.7934531569480896\n",
            "Tag: VBN --- F1: 0.594856858253479\n",
            "Tag: RBR --- F1: 0.0\n",
            "Tag: -RRB- --- F1: 0.0\n",
            "Tag: # --- F1: 0.0\n",
            "Tag: $ --- F1: 0.9952611923217773\n",
            "Tag: JJ --- F1: 0.6781041622161865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- Val_F1: {}'.format(index2tag[i], f1_val_list[i]))"
      ],
      "metadata": {
        "id": "Y3S2R1GfvgUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "736c7b90-a010-4857-c93e-ca275898dac6"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: DT --- Val_F1: 0.9569797515869141\n",
            "Tag: RB --- Val_F1: 0.300798237323761\n",
            "Tag: WDT --- Val_F1: 0.21670140326023102\n",
            "Tag: VB --- Val_F1: 0.8365562558174133\n",
            "Tag: VBZ --- Val_F1: 0.8826829791069031\n",
            "Tag: PRP --- Val_F1: 0.9236684441566467\n",
            "Tag: FW --- Val_F1: 0.0\n",
            "Tag: NN --- Val_F1: 0.8188362717628479\n",
            "Tag: EX --- Val_F1: 0.0\n",
            "Tag: RBS --- Val_F1: 0.0\n",
            "Tag: NNS --- Val_F1: 0.7207023501396179\n",
            "Tag: PRP$ --- Val_F1: 0.840006411075592\n",
            "Tag: CC --- Val_F1: 0.8860945105552673\n",
            "Tag: NNPS --- Val_F1: 0.0\n",
            "Tag: TO --- Val_F1: 1.0\n",
            "Tag: UH --- Val_F1: 0.0\n",
            "Tag: RP --- Val_F1: 0.0\n",
            "Tag: POS --- Val_F1: 0.9830823540687561\n",
            "Tag: WRB --- Val_F1: 0.0\n",
            "Tag: WP$ --- Val_F1: 0.0\n",
            "Tag: WP --- Val_F1: 0.0\n",
            "Tag: -LRB- --- Val_F1: 0.0\n",
            "Tag: VBG --- Val_F1: 0.3557761311531067\n",
            "Tag: JJR --- Val_F1: 0.0\n",
            "Tag: SYM --- Val_F1: 0.0\n",
            "Tag: PDT --- Val_F1: 0.0\n",
            "Tag: MD --- Val_F1: 0.8424840569496155\n",
            "Tag: JJS --- Val_F1: 0.0\n",
            "Tag: LS --- Val_F1: 0.0\n",
            "Tag: IN --- Val_F1: 0.9306560754776001\n",
            "Tag: VBP --- Val_F1: 0.6420993208885193\n",
            "Tag: NNP --- Val_F1: 0.797328531742096\n",
            "Tag: CD --- Val_F1: 0.8584414124488831\n",
            "Tag: VBD --- Val_F1: 0.7674820423126221\n",
            "Tag: VBN --- Val_F1: 0.5346512198448181\n",
            "Tag: RBR --- Val_F1: 0.0\n",
            "Tag: -RRB- --- Val_F1: 0.0\n",
            "Tag: # --- Val_F1: 0.0\n",
            "Tag: $ --- Val_F1: 0.9975429773330688\n",
            "Tag: JJ --- Val_F1: 0.5885202884674072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Infos"
      ],
      "metadata": {
        "id": "q-_flfImviTg"
      }
    }
  ]
}