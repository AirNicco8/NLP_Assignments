{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_biLSTM256_2DENSE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Due to**: 23/12/2021 (dd/mm/yyyy)\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Part-of Speech (POS) tagging as Sequence Labelling using Recurrent Neural Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4_wqPdlBcKS"
      },
      "source": [
        "# Intro\n",
        "\n",
        "In this assignment  we will ask you to perform POS tagging using neural architectures\n",
        "\n",
        "You are asked to follow these steps:\n",
        "*   Download the corpora and split it in training and test sets, structuring a dataframe.\n",
        "*   Embed the words using GloVe embeddings\n",
        "*   Create a baseline model, using a simple neural architecture\n",
        "*   Experiment doing small modifications to the baseline model, choose hyperparameters using the validation set\n",
        "*   Evaluate your two best model\n",
        "*   Analyze the errors of your model\n",
        "\n",
        "\n",
        "**Task**: given a corpus of documents, predict the POS tag for each word\n",
        "\n",
        "**Corpus**:\n",
        "Ignore the numeric value in the third column, use only the words/symbols and its label. \n",
        "The corpus is available at:\n",
        "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
        "\n",
        "**Splits**: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
        "\n",
        "\n",
        "**Features**: you MUST use GloVe embeddings as the only input features to the model.\n",
        "\n",
        "**Splitting**: you can decide to split documents into sentences or not, the choice is yours.\n",
        "\n",
        "**I/O structure**: The input data will have three dimensions: 1-documents/sentences, 2-token, 3-features; for the output there are 2 possibilities: if you use one-hot encoding it will be 1-documents/sentences, 2-token labels, 3-classes, if you use a single integer that indicates the number of the class it will be 1-documents/sentences, 2-token labels.\n",
        "\n",
        "**Baseline**: two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top; the choice of hyper-parameters is yours.\n",
        "\n",
        "**Architectures**: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and adding an additional dense layer; do not mix these variantions.\n",
        "\n",
        "\n",
        "**Training and Experiments**: all the experiments must involve only the training and validation sets.\n",
        "\n",
        "**Evaluation**: in the end, only the two best models of your choice (according to the validation set) must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech. DO NOT CONSIDER THE PUNCTUATION CLASSES.\n",
        "\n",
        "**Metrics**: the metric you must use to evaluate your final model is the F1-macro, WITHOUT considering punctuation/symbols classes; during the training process you can use accuracy because you can't use the F1 metric unless you use a single (gigantic) batch because there is no way to aggregate \"partial\" F1 scores computed on mini-batches.\n",
        "\n",
        "**Discussion and Error Analysis** : verify and discuss if the results on the test sets are coherent with those on the validation set; analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
        "\n",
        "**Report**: you are asked to deliver the code of your experiments and a small pdf report of about 2 pages; the pdf must begin with the names of the people of your team and a small abstract (4-5 lines) that sums up your findings.\n",
        "\n",
        "# Out Of Vocabulary (OOV) terms\n",
        "\n",
        "How to handle words that are not in GloVe vocabulary?\n",
        "You can handle them as you want (random embedding, placeholder, whatever!), but they must be STATIC embeddings (you cannot train them).\n",
        "\n",
        "But there is a very important caveat! As usual, the element of the test set must not influence the elements of the other splits!\n",
        "\n",
        "So, when you compute new embeddings for train+validation, you must forget about test documents.\n",
        "The motivation is to emulate a real-world scenario, where you select and train a model in the first stage, without knowing nothing about the testing environment.\n",
        "\n",
        "For implementation convenience, you CAN use a single vocabulary file/matrix/whatever. The principle of the previous point is that the embeddings inside that file/matrix must be generated independently for train and test splits.\n",
        "\n",
        "Basically in a real-world scenario, this is what would happen:\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Training of the model(s)\n",
        "5. Compute embeddings for terms OOV2 of the validation split \n",
        "6. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "7. Validation of the model(s)\n",
        "8. Compute embeddings for terms OOV3 of the test split \n",
        "9. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2+OOV3\n",
        "10. Testing of the final model\n",
        "\n",
        "In this case, where we already have all the documents, we can simplify the process a bit, but the procedure must remain rigorous.\n",
        "\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Compute embeddings for terms OOV2 of the validation split \n",
        "5. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "6. Compute embeddings for terms OOV3 of the test split \n",
        "7. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2\n",
        "8. Training of the model(s)\n",
        "9. Validation of the model(s)\n",
        "10. Testing of the final model\n",
        "\n",
        "Step 2 and step 6 must be completely independent of each other, for what concerns the method and the documents. But they can rely on the previous vocabulary (V1 for step 2 and V3 for step 6)\n",
        "THEREFORE if a word is present both in the training set and the test split and not in the starting vocabulary, its embedding is computed in step 2) and it is not considered OOV anymore in step 6).\n",
        "\n",
        "# Report\n",
        "The report must not be just a copy and paste of graphs and tables!\n",
        "\n",
        "The report must not be longer than 2 pages and must contain:\n",
        "* The names of the member of your team\n",
        "* A short abstract (4-5 lines) that sum ups everything\n",
        "* A general description of the task you have addressed and how you have addressed it\n",
        "* A short description of the models you have used\n",
        "* Some tables that sum up your findings in validation and test and a discussion of those results\n",
        "* The most relevant findings of your error analysis\n",
        "\n",
        "# Evaluation Criterion\n",
        "\n",
        "The goal of this assignment is not to prove you can find best model ever, but to face a common task, structure it correctly, and follow a correct and rigorous experimental procedure.\n",
        "In other words, we don't care if you final models are awful as long as you have followed the correct procedure and wrote a decent report.\n",
        "\n",
        "The score of the assignment will be computed roughly as follows\n",
        "* 1 point for the general setting of the problem\n",
        "* 1 point for the handling of OOV terms\n",
        "* 1 point for the models\n",
        "* 1 point for train-validation-test procedure\n",
        "* 2 point for the discussion of the results, error analysis, and report\n",
        "\n",
        "This distribution of scores is tentative and we may decide to alter it at any moment.\n",
        "We also reserve the right to assign a small bonus (0.5 points) to any assignment that is particularly worthy. Similarly, in case of grave errors, we may decide to assign an equivalent malus (-0.5 points).\n",
        "\n",
        "# Contacts\n",
        "\n",
        "In case of any doubt, question, issue, or help we highly recommend you to check the [course useful material](https://virtuale.unibo.it/pluginfile.php/1036039/mod_resource/content/2/NLP_Course_Useful_Material.pdf) for additional information, and to use the Virtuale forums to discuss with other students.\n",
        "\n",
        "You can always contact us at the following email addresses. To increase the probability of a prompt response, we reccomend you to write to both the teaching assistants.\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Andrea Galassi -> a.galassi@unibo.it\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it\n",
        "\n",
        "\n",
        "# FAQ\n",
        "* You can use a non-trainable Embedding layer to load the glove embeddings\n",
        "* You can use any library of your choice to implement the networks. Two options are tensorflow/keras or pythorch. Both these libraries have all the classes you need to implement these simple architectures and there are plenty of tutorials around, where you can learn how to use them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emdubW66fnlD"
      },
      "source": [
        "import os, shutil  #  file management\n",
        "import sys \n",
        "import pandas as pd  #  dataframe management\n",
        "import numpy as np  #  data manipulation\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request  #  download files\n",
        "import zipfile  #  unzip files\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B-w2pqyfpF1"
      },
      "source": [
        "from keras import metrics \n",
        "from tensorflow.python.keras.metrics import Metric\n",
        "from sklearn.metrics import f1_score\n",
        "import keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFvajV2sni19",
        "outputId": "97b44679-bd70-4849-b6db-61a6fce300bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3xn1MZPgA_v",
        "outputId": "5f7c1cee-0880-42fa-c094-742583ecd3a4"
      },
      "source": [
        "dataset_folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\")\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "    os.makedirs(dataset_folder)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
        "\n",
        "dataset_path = os.path.join(dataset_folder, \"treebank.zip\")\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    urllib.request.urlretrieve(url, dataset_path)\n",
        "    print(\"Successful download\")\n",
        "\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_folder)\n",
        "print(\"Successful extraction\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful download\n",
            "Successful extraction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre Processing\n",
        "- splitting"
      ],
      "metadata": {
        "id": "FiO1v6SJmm37"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKbUsrVHhGAG"
      },
      "source": [
        "dataset_name = \"dependency_treebank\"\n",
        "\n",
        "\n",
        "folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\", dataset_name)\n",
        "\n",
        "pre_train = []\n",
        "pre_valid = []\n",
        "pre_test = []\n",
        "i = 1\n",
        "\n",
        "file_list = sorted(os.listdir(folder))\n",
        "\n",
        "for filename in file_list:\n",
        "  file_path = os.path.join(folder, filename)\n",
        "  if os.path.isfile(file_path):\n",
        "    # open the file\n",
        "      text = []\n",
        "      with open(file_path, mode='r', encoding='utf-8') as text_file:\n",
        "        text = text_file.read()\n",
        "        if i <= 100:\n",
        "          pre_train.append(text)\n",
        "        elif i <= 150:\n",
        "          pre_valid.append(text)\n",
        "        else:\n",
        "          pre_test.append(text)  \n",
        "  i+=1\n",
        "\n",
        "tr = []\n",
        "val = []\n",
        "tes = []\n",
        "\n",
        "for paragraph in pre_train:\n",
        "   tr.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_valid:\n",
        "   val.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_test:\n",
        "   tes.append(paragraph.split('\\n'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUgbTXi-F7Sa"
      },
      "source": [
        "train = []\n",
        "valid = []\n",
        "test = []\n",
        "\n",
        "for i in tr:\n",
        "  for j in i:\n",
        "    train.append(j.split('\\t'))\n",
        "\n",
        "for i in val:\n",
        "  for j in i:\n",
        "    valid.append(j.split('\\t'))\n",
        "\n",
        "for i in tes:\n",
        "  for j in i:\n",
        "    test.append(j.split('\\t'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmzVaRuQFp0M"
      },
      "source": [
        "train_sentences = []\n",
        "valid_sentences = []\n",
        "test_sentences = []\n",
        "train_tags = []\n",
        "valid_tags = []\n",
        "test_tags = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in train:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    train_sentences.append(s)\n",
        "    train_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in valid:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    valid_sentences.append(s)\n",
        "    valid_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in test:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    test_sentences.append(s)\n",
        "    test_tags.append(t)\n",
        "    s = []\n",
        "    t = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB65cUFLIeEr"
      },
      "source": [
        "flat_train = [item for sublist in train_sentences for item in sublist]\n",
        "flat_valid = [item for sublist in valid_sentences for item in sublist]\n",
        "flat_test = [item for sublist in test_sentences for item in sublist]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "- vocabularies\n",
        "- GloVe download"
      ],
      "metadata": {
        "id": "d4IenRMamtLg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZO6ga1EGHdh"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "train_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "train_tokenizer.fit_on_texts(train_sentences)                    # fit tokeniser on data\n",
        "train_encoded = train_tokenizer.texts_to_sequences(train_sentences)\n",
        "\n",
        "valid_tokenizer = Tokenizer()       \n",
        "valid_tokenizer.fit_on_texts(valid_sentences)                  # instantiate tokeniser\n",
        "valid_encoded = valid_tokenizer.texts_to_sequences(valid_sentences)\n",
        "\n",
        "test_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "test_tokenizer.fit_on_texts(test_sentences)                    # fit tokeniser on data\n",
        "test_encoded = test_tokenizer.texts_to_sequences(test_sentences)  # use the tokeniser to encode input sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(train_tags)\n",
        "tags_encoded = tag_tokenizer.texts_to_sequences(train_tags)"
      ],
      "metadata": {
        "id": "MSvhz4IvmziU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMsG0Yw1l50L",
        "outputId": "d3ab69c1-f71d-411b-b5e7-fb543e6cf20d"
      },
      "source": [
        "voc = list(set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(valid_tokenizer.word_index.keys()) - set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(test_tokenizer.word_index.keys()) - set(voc))\n",
        "print(len(voc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7404\n",
            "9901\n",
            "10947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS9RwnOtJuas"
      },
      "source": [
        "word_index = dict(zip(voc, range(2, len(voc)+3)))\n",
        "word_index['-PAD-'] = 0\n",
        "word_index['-OOV-'] = 1\n",
        "\n",
        "tags = set([item for sublist in train_tags for item in sublist])\n",
        "\n",
        "tag2index = {t: i + 1 for i, t in enumerate(tags)}\n",
        "tag2index['-PAD-'] = 0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoj4Zx51J2MU",
        "outputId": "0fad5e21-a205-4825-bb5f-90d06f6ee047"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-17 14:05:59--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-12-17 14:05:59--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-12-17 14:06:00--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.04MB/s    in 2m 40s  \n",
            "\n",
            "2021-12-17 14:08:40 (5.14 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc-T6FcLKtS9",
        "outputId": "91ace8a1-3ded-4e3d-865f-aad557c6d11f"
      },
      "source": [
        "path_to_glove_file = os.path.join(os.getcwd(), \"glove.6B.100d.txt\")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding\n",
        "- matrix creation\n",
        "- padding\n",
        "- one hot encoding of tags\n",
        "- punctuation identification"
      ],
      "metadata": {
        "id": "E2IN3Mh-m85T"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0mOStEPMeXt",
        "outputId": "4ba12fd4-dac3-4cd4-dc0c-fcfc25dd03b3"
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        embedding_matrix += np.random.uniform(low=-0.05, high=0.05, size=embedding_dim)\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 10271 words (678 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpYrHs0RMtQJ"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8EZM31Js06-",
        "outputId": "7801fcb1-6296-489d-a218-f564d9c61669"
      },
      "source": [
        "MAX_LENGTH = len(max(train_sentences, key=len))\n",
        "print(MAX_LENGTH)\n",
        "MAX_LENGTH2 = len(max(valid_sentences, key=len))\n",
        "print(MAX_LENGTH2)\n",
        "MAX_LENGTH3 = len(max(test_sentences, key=len))\n",
        "print(MAX_LENGTH3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "249\n",
            "81\n",
            "58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSRa9SwTuNrX"
      },
      "source": [
        "train_sentences_X, valid_sentences_X, train_tags_y, valid_tags_y, = [], [], [], []\n",
        " \n",
        "for s in train_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        "\n",
        "for s in valid_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    valid_sentences_X.append(s_int)\n",
        "\n",
        "\n",
        "for s in train_tags:\n",
        "    train_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "for s in valid_tags:\n",
        "    valid_tags_y.append([tag2index[t] for t in s])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf-XlSrStQx3"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_sentences_X = pad_sequences(valid_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_tags_y = pad_sequences(valid_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)"
      ],
      "metadata": {
        "id": "nUtevCQrnskt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "point = [tag2index['.']]\n",
        "virg = [tag2index[',']]\n",
        "weird_apex = [tag2index['``']]\n",
        "single_apex = [tag2index[\"''\"]]\n",
        "two_dots = [tag2index[':']]\n",
        "pad = [tag2index['-PAD-']]\n",
        "\n",
        "punct_cat_classes = to_categorical([point, virg, weird_apex, single_apex, two_dots, pad], len(tag2index))\n",
        "punct_cat_classes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT8PjDIynuHG",
        "outputId": "586182b5-d657-4916-9d3c-0fb00493cf0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 1, 46)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cum_tags = np.zeros(46)\n",
        "for i in punct_cat_classes:\n",
        "  print(i[0])\n",
        "  cum_tags += i[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAqZ0Xkrnw_d",
        "outputId": "fd3f0b0e-edca-4971-ce02-f5c10d9ad287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "where_tags = np.where(np.logical_not(cum_tags))\n",
        "where_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzJNfc1xn7jA",
        "outputId": "14370acf-a98f-480f-e32c-040591d719bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 1,  2,  3,  4,  5,  6,  7,  9, 10, 11, 13, 15, 16, 17, 18, 19, 21,\n",
              "        22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39,\n",
              "        40, 41, 42, 43, 44, 45]),)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_punct_indexes = where_tags[0]\n",
        "n_classes = len(no_punct_indexes)"
      ],
      "metadata": {
        "id": "LrzwDP5Wn9TK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "cat_val_tags_y = to_categorical(valid_tags_y, len(tag2index))\n",
        "print(len(cat_train_tags_y), len(cat_val_tags_y))\n",
        "print(len(train_sentences_X), len(valid_sentences_X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik32ea9dn-5m",
        "outputId": "2b62b87a-ae38-47b2-9418-148a5501e041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1963 1299\n",
            "1963 1299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mini Data Vizualization Inset"
      ],
      "metadata": {
        "id": "YbT_r953oBA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_tags = np.zeros(46)\n",
        "for i in cat_train_tags_y:\n",
        "  for t in i:\n",
        "    distribution_tags += t"
      ],
      "metadata": {
        "id": "YIyRpFGVoKlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = sum(distribution_tags[1:])\n",
        "norm = [float(i)/s for i in distribution_tags[1:]]"
      ],
      "metadata": {
        "id": "OAbGJViloL0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt "
      ],
      "metadata": {
        "id": "sPiOzlwroPuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(np.arange(len(norm)),norm)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "px2v7JyxoQrz",
        "outputId": "9aef2fee-4d4b-4e6a-f77b-f911239134a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#F1 Metric"
      ],
      "metadata": {
        "id": "l0m_PqWUoRsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric.\n",
        "\n",
        "    Only computes a batch-wise average of precision.\n",
        "\n",
        "    Computes the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric.\n",
        "\n",
        "    Only computes a batch-wise average of recall.\n",
        "\n",
        "    Computes the recall, a metric for multi-label classification of\n",
        "    how many relevant items are selected.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def f1_binary(y_true, y_pred):\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
        "\n",
        "def metric_wrapper(index):\n",
        "  def f1_for_class(y_true, y_pred):\n",
        "    #get only the desired class\n",
        "    true = y_true[:,:,index]\n",
        "    pred = y_pred[:,:,index]\n",
        "    #return dice per class\n",
        "    tmp = f1_binary(true,pred)\n",
        "    return tmp\n",
        "  f1_for_class.__name__ = 'f1_' + str(index)\n",
        "  return f1_for_class\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    result = 0.0\n",
        "    for class_id in no_punct_indexes:\n",
        "        y_true_single_class = y_true[:,:,class_id]\n",
        "        y_pred_single_class = y_pred[:,:,class_id]\n",
        "        f1_single = f1_binary(y_true_single_class, y_pred_single_class)\n",
        "        result += f1_single / float(n_classes)\n",
        "    return result"
      ],
      "metadata": {
        "id": "ZDaBrpk3oYXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model compile and fit"
      ],
      "metadata": {
        "id": "sQz388M4oaCW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsVGByW_sdec",
        "outputId": "cce26397-3675-40b5-d1ed-c5fb407fe620"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(embedding_layer)\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(Dense(len(tag2index)))\n",
        "model.add(Dense(len(tag2index)))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=Adam(0.001),\n",
        "             metrics=['accuracy', f1, [metric_wrapper(i) for i in no_punct_indexes]])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 249, 100)          1094900   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 249, 512)         731136    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 249, 46)           23598     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 249, 46)           2162      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,851,796\n",
            "Trainable params: 756,896\n",
            "Non-trainable params: 1,094,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "history = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=40, validation_data=(valid_sentences_X, cat_val_tags_y), callbacks=[callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWIokkJwokrV",
        "outputId": "3f464924-4fbd-4c90-a3cb-19b87e8e5b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "16/16 [==============================] - 94s 5s/step - loss: 0.8611 - accuracy: 0.8449 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.3737 - val_accuracy: 0.9167 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.3292 - accuracy: 0.9143 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2990 - val_accuracy: 0.9221 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - 74s 5s/step - loss: 0.2879 - accuracy: 0.9258 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2752 - val_accuracy: 0.9318 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.2671 - accuracy: 0.9332 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2594 - val_accuracy: 0.9355 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.2508 - accuracy: 0.9384 - f1: 9.6451e-06 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 3.8580e-04 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2428 - val_accuracy: 0.9409 - val_f1: 4.5004e-05 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0018 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.2330 - accuracy: 0.9443 - f1: 0.0024 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0977 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2245 - val_accuracy: 0.9466 - val_f1: 0.0014 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.0018 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0558 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.2129 - accuracy: 0.9482 - f1: 0.0086 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0243 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.0179 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.2833 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0175 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2036 - val_accuracy: 0.9492 - val_f1: 0.0127 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0422 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.0939 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.2421 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.1316 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.1915 - accuracy: 0.9509 - f1: 0.0400 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.3946 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.1235 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.4304 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.6507 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1831 - val_accuracy: 0.9522 - val_f1: 0.0519 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0957 - val_f1_6: 0.0000e+00 - val_f1_7: 0.5190 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.1984 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.3691 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.8946 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - 86s 5s/step - loss: 0.1712 - accuracy: 0.9548 - f1: 0.0710 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.4346 - f1_6: 0.0000e+00 - f1_7: 0.6346 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.2697 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0050 - f1_18: 0.5476 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9060 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0440 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1654 - val_accuracy: 0.9571 - val_f1: 0.0827 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.7941 - val_f1_6: 0.0000e+00 - val_f1_7: 0.6422 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.2800 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0278 - val_f1_18: 0.5162 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9175 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.1288 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.1537 - accuracy: 0.9594 - f1: 0.0956 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.9021 - f1_6: 0.0000e+00 - f1_7: 0.7371 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.3960 - f1_15: 0.0200 - f1_16: 0.0000e+00 - f1_17: 0.0823 - f1_18: 0.6073 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9222 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.1589 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1498 - val_accuracy: 0.9609 - val_f1: 0.1035 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.9836 - val_f1_6: 0.0000e+00 - val_f1_7: 0.7545 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.4233 - val_f1_15: 0.0450 - val_f1_16: 0.0000e+00 - val_f1_17: 0.1659 - val_f1_18: 0.5452 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9219 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0130 - val_f1_33: 0.0000e+00 - val_f1_34: 0.2862 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.1390 - accuracy: 0.9633 - f1: 0.1142 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.9846 - f1_6: 0.0000e+00 - f1_7: 0.8057 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.4914 - f1_15: 0.1431 - f1_16: 0.0000e+00 - f1_17: 0.2593 - f1_18: 0.6313 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9244 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0234 - f1_33: 0.0000e+00 - f1_34: 0.3033 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1367 - val_accuracy: 0.9643 - val_f1: 0.1231 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.9984 - val_f1_6: 0.0000e+00 - val_f1_7: 0.8143 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.4790 - val_f1_15: 0.1563 - val_f1_16: 0.0000e+00 - val_f1_17: 0.3412 - val_f1_18: 0.6236 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9239 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.1197 - val_f1_33: 0.0000e+00 - val_f1_34: 0.4643 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0034\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - 78s 5s/step - loss: 0.1263 - accuracy: 0.9669 - f1: 0.1371 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0089 - f1_5: 0.9973 - f1_6: 0.0000e+00 - f1_7: 0.8436 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.5470 - f1_15: 0.2650 - f1_16: 0.0000e+00 - f1_17: 0.4026 - f1_18: 0.6779 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9271 - f1_26: 0.0044 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.1776 - f1_33: 0.0000e+00 - f1_34: 0.4208 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.2122 - val_loss: 0.1253 - val_accuracy: 0.9675 - val_f1: 0.1479 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0025 - val_f1_5: 1.0000 - val_f1_6: 0.0000e+00 - val_f1_7: 0.8523 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.5669 - val_f1_15: 0.2641 - val_f1_16: 0.0000e+00 - val_f1_17: 0.4545 - val_f1_18: 0.6350 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9241 - val_f1_26: 0.0155 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.2657 - val_f1_33: 0.0000e+00 - val_f1_34: 0.5610 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.3752\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.1156 - accuracy: 0.9702 - f1: 0.1631 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0896 - f1_5: 0.9985 - f1_6: 0.0000e+00 - f1_7: 0.8663 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.5945 - f1_15: 0.3620 - f1_16: 0.0000e+00 - f1_17: 0.5006 - f1_18: 0.7036 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0081 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9293 - f1_26: 0.0544 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.3333 - f1_33: 0.0000e+00 - f1_34: 0.5317 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.5521 - val_loss: 0.1160 - val_accuracy: 0.9701 - val_f1: 0.1761 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.1242 - val_f1_5: 0.9995 - val_f1_6: 0.0000e+00 - val_f1_7: 0.8683 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.5977 - val_f1_15: 0.3538 - val_f1_16: 0.0000e+00 - val_f1_17: 0.5328 - val_f1_18: 0.6472 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0567 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9247 - val_f1_26: 0.1165 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.4370 - val_f1_33: 0.0000e+00 - val_f1_34: 0.6536 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.7309\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.1066 - accuracy: 0.9729 - f1: 0.1937 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.2547 - f1_5: 0.9985 - f1_6: 0.0000e+00 - f1_7: 0.8806 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.6248 - f1_15: 0.4256 - f1_16: 0.0000e+00 - f1_17: 0.5672 - f1_18: 0.7253 - f1_19: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.3030 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9325 - f1_26: 0.1490 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.4753 - f1_33: 0.0000e+00 - f1_34: 0.6052 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0097 - f1_44: 0.0000e+00 - f1_45: 0.7957 - val_loss: 0.1081 - val_accuracy: 0.9723 - val_f1: 0.1978 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.2130 - val_f1_5: 1.0000 - val_f1_6: 0.0000e+00 - val_f1_7: 0.8874 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.5817 - val_f1_15: 0.4045 - val_f1_16: 0.0000e+00 - val_f1_17: 0.5863 - val_f1_18: 0.6787 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.3271 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9273 - val_f1_26: 0.2949 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0035 - val_f1_31: 0.5289 - val_f1_33: 0.0000e+00 - val_f1_34: 0.6752 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0363 - val_f1_44: 0.0000e+00 - val_f1_45: 0.7663\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0987 - accuracy: 0.9749 - f1: 0.2266 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.4227 - f1_5: 0.9987 - f1_6: 0.0000e+00 - f1_7: 0.8897 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.6536 - f1_15: 0.4769 - f1_16: 0.0000e+00 - f1_17: 0.6262 - f1_18: 0.7446 - f1_19: 0.0196 - f1_21: 0.0000e+00 - f1_22: 0.6542 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9328 - f1_26: 0.3944 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0014 - f1_31: 0.5844 - f1_33: 0.0000e+00 - f1_34: 0.6604 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.1825 - f1_44: 0.0000e+00 - f1_45: 0.8204 - val_loss: 0.1008 - val_accuracy: 0.9740 - val_f1: 0.2365 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0089 - val_f1_3: 0.0000e+00 - val_f1_4: 0.4224 - val_f1_5: 1.0000 - val_f1_6: 0.0000e+00 - val_f1_7: 0.8919 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.6428 - val_f1_15: 0.4556 - val_f1_16: 0.0000e+00 - val_f1_17: 0.6483 - val_f1_18: 0.6721 - val_f1_19: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.8286 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9267 - val_f1_26: 0.4372 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0035 - val_f1_31: 0.6391 - val_f1_33: 0.0000e+00 - val_f1_34: 0.7092 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.4031 - val_f1_44: 0.0000e+00 - val_f1_45: 0.7720\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - 74s 5s/step - loss: 0.0920 - accuracy: 0.9765 - f1: 0.2628 - f1_1: 0.0000e+00 - f1_2: 0.1143 - f1_3: 0.0299 - f1_4: 0.5134 - f1_5: 0.9986 - f1_6: 0.0000e+00 - f1_7: 0.8930 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.6723 - f1_15: 0.5137 - f1_16: 0.0000e+00 - f1_17: 0.6688 - f1_18: 0.7571 - f1_19: 0.0656 - f1_21: 0.0000e+00 - f1_22: 0.9137 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9342 - f1_26: 0.5902 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0105 - f1_31: 0.6271 - f1_33: 0.0000e+00 - f1_34: 0.7077 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.6745 - f1_44: 0.0000e+00 - f1_45: 0.8272 - val_loss: 0.0945 - val_accuracy: 0.9752 - val_f1: 0.2687 - val_f1_1: 0.0000e+00 - val_f1_2: 0.1075 - val_f1_3: 0.1168 - val_f1_4: 0.4477 - val_f1_5: 1.0000 - val_f1_6: 0.0000e+00 - val_f1_7: 0.8983 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.6976 - val_f1_15: 0.4680 - val_f1_16: 0.0000e+00 - val_f1_17: 0.6096 - val_f1_18: 0.6865 - val_f1_19: 0.0856 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9353 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9338 - val_f1_26: 0.7619 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0097 - val_f1_31: 0.6725 - val_f1_33: 0.0000e+00 - val_f1_34: 0.7370 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.8052 - val_f1_44: 0.0000e+00 - val_f1_45: 0.7737\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0863 - accuracy: 0.9780 - f1: 0.2942 - f1_1: 0.0000e+00 - f1_2: 0.3996 - f1_3: 0.2938 - f1_4: 0.5781 - f1_5: 0.9981 - f1_6: 0.0123 - f1_7: 0.8980 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.7013 - f1_15: 0.5433 - f1_16: 0.0000e+00 - f1_17: 0.6861 - f1_18: 0.7526 - f1_19: 0.1575 - f1_21: 0.0000e+00 - f1_22: 0.9533 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9389 - f1_26: 0.7311 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0203 - f1_31: 0.6934 - f1_33: 0.0000e+00 - f1_34: 0.7450 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.8474 - f1_44: 0.0000e+00 - f1_45: 0.8174 - val_loss: 0.0911 - val_accuracy: 0.9760 - val_f1: 0.2996 - val_f1_1: 0.0000e+00 - val_f1_2: 0.5201 - val_f1_3: 0.3832 - val_f1_4: 0.5420 - val_f1_5: 1.0000 - val_f1_6: 0.0073 - val_f1_7: 0.9009 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.6552 - val_f1_15: 0.4437 - val_f1_16: 0.0000e+00 - val_f1_17: 0.6288 - val_f1_18: 0.7393 - val_f1_19: 0.3600 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9511 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9323 - val_f1_26: 0.8025 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0300 - val_f1_31: 0.6573 - val_f1_33: 0.0000e+00 - val_f1_34: 0.7881 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.8674 - val_f1_44: 0.0000e+00 - val_f1_45: 0.7737\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0810 - accuracy: 0.9792 - f1: 0.3148 - f1_1: 0.0000e+00 - f1_2: 0.5690 - f1_3: 0.4518 - f1_4: 0.6162 - f1_5: 0.9981 - f1_6: 0.0628 - f1_7: 0.9035 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.7156 - f1_15: 0.5608 - f1_16: 0.0000e+00 - f1_17: 0.7079 - f1_18: 0.7782 - f1_19: 0.3069 - f1_21: 0.0000e+00 - f1_22: 0.9759 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9390 - f1_26: 0.7477 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0352 - f1_31: 0.6970 - f1_33: 0.0000e+00 - f1_34: 0.7608 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0336 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.9028 - f1_44: 0.0000e+00 - f1_45: 0.8297 - val_loss: 0.0849 - val_accuracy: 0.9779 - val_f1: 0.3258 - val_f1_1: 0.0000e+00 - val_f1_2: 0.6929 - val_f1_3: 0.5288 - val_f1_4: 0.5892 - val_f1_5: 1.0000 - val_f1_6: 0.0995 - val_f1_7: 0.9064 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.6907 - val_f1_15: 0.5055 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7008 - val_f1_18: 0.7166 - val_f1_19: 0.4319 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9851 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9386 - val_f1_26: 0.8240 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0550 - val_f1_31: 0.7065 - val_f1_33: 0.0000e+00 - val_f1_34: 0.7666 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.2187 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9041 - val_f1_44: 0.0000e+00 - val_f1_45: 0.7730\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - 74s 5s/step - loss: 0.0763 - accuracy: 0.9804 - f1: 0.3386 - f1_1: 0.0000e+00 - f1_2: 0.6844 - f1_3: 0.6321 - f1_4: 0.6594 - f1_5: 0.9982 - f1_6: 0.1300 - f1_7: 0.9075 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.7331 - f1_15: 0.5826 - f1_16: 0.0000e+00 - f1_17: 0.7376 - f1_18: 0.7869 - f1_19: 0.3669 - f1_21: 0.0000e+00 - f1_22: 0.9939 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9393 - f1_26: 0.8319 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0832 - f1_31: 0.7371 - f1_33: 0.0000e+00 - f1_34: 0.7838 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.2049 - f1_39: 0.0049 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.9189 - f1_44: 0.0000e+00 - f1_45: 0.8265 - val_loss: 0.0811 - val_accuracy: 0.9786 - val_f1: 0.3373 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7235 - val_f1_3: 0.6196 - val_f1_4: 0.6047 - val_f1_5: 1.0000 - val_f1_6: 0.1285 - val_f1_7: 0.9122 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.6876 - val_f1_15: 0.4983 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7062 - val_f1_18: 0.7555 - val_f1_19: 0.4773 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9745 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9399 - val_f1_26: 0.8702 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0752 - val_f1_31: 0.7458 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8139 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.2527 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9302 - val_f1_44: 0.0000e+00 - val_f1_45: 0.7747\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0723 - accuracy: 0.9814 - f1: 0.3529 - f1_1: 0.0000e+00 - f1_2: 0.7362 - f1_3: 0.7300 - f1_4: 0.6683 - f1_5: 0.9983 - f1_6: 0.2200 - f1_7: 0.9107 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.7501 - f1_15: 0.5948 - f1_16: 0.0000e+00 - f1_17: 0.7516 - f1_18: 0.7959 - f1_19: 0.3969 - f1_21: 0.0000e+00 - f1_22: 0.9953 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9413 - f1_26: 0.8516 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.1050 - f1_31: 0.7656 - f1_33: 0.0000e+00 - f1_34: 0.7984 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.3636 - f1_39: 0.0078 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.9069 - f1_44: 0.0000e+00 - f1_45: 0.8292 - val_loss: 0.0772 - val_accuracy: 0.9795 - val_f1: 0.3502 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7878 - val_f1_3: 0.6335 - val_f1_4: 0.6243 - val_f1_5: 1.0000 - val_f1_6: 0.1601 - val_f1_7: 0.9111 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.7496 - val_f1_15: 0.5511 - val_f1_16: 0.0000e+00 - val_f1_17: 0.6739 - val_f1_18: 0.7513 - val_f1_19: 0.4990 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9904 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9429 - val_f1_26: 0.9107 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.1611 - val_f1_31: 0.7521 - val_f1_33: 0.0000e+00 - val_f1_34: 0.7895 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.3994 - val_f1_39: 0.0036 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9352 - val_f1_44: 0.0000e+00 - val_f1_45: 0.7823\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0687 - accuracy: 0.9823 - f1: 0.3678 - f1_1: 0.0000e+00 - f1_2: 0.7726 - f1_3: 0.7970 - f1_4: 0.6983 - f1_5: 0.9980 - f1_6: 0.3022 - f1_7: 0.9120 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.7658 - f1_15: 0.6125 - f1_16: 0.0000e+00 - f1_17: 0.7680 - f1_18: 0.8036 - f1_19: 0.3973 - f1_21: 0.0000e+00 - f1_22: 0.9936 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9434 - f1_26: 0.8858 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.1639 - f1_31: 0.7768 - f1_33: 0.0000e+00 - f1_34: 0.8123 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.5315 - f1_39: 0.0287 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.9110 - f1_44: 0.0000e+00 - f1_45: 0.8379 - val_loss: 0.0736 - val_accuracy: 0.9805 - val_f1: 0.3654 - val_f1_1: 0.0000e+00 - val_f1_2: 0.8209 - val_f1_3: 0.6779 - val_f1_4: 0.6504 - val_f1_5: 1.0000 - val_f1_6: 0.2883 - val_f1_7: 0.9165 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.7447 - val_f1_15: 0.5613 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7505 - val_f1_18: 0.7496 - val_f1_19: 0.5127 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9952 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9438 - val_f1_26: 0.9147 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.1627 - val_f1_31: 0.7700 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8096 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.5986 - val_f1_39: 0.0231 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9343 - val_f1_44: 0.0000e+00 - val_f1_45: 0.7916\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.0654 - accuracy: 0.9831 - f1: 0.3793 - f1_1: 0.0000e+00 - f1_2: 0.8092 - f1_3: 0.7656 - f1_4: 0.7088 - f1_5: 0.9981 - f1_6: 0.3887 - f1_7: 0.9172 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.7709 - f1_15: 0.6278 - f1_16: 0.0000e+00 - f1_17: 0.7843 - f1_18: 0.8128 - f1_19: 0.4404 - f1_21: 0.0000e+00 - f1_22: 0.9932 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9456 - f1_26: 0.8916 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.2135 - f1_31: 0.7933 - f1_33: 0.0000e+00 - f1_34: 0.8188 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.6518 - f1_39: 0.0607 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.9253 - f1_44: 0.0000e+00 - f1_45: 0.8540 - val_loss: 0.0712 - val_accuracy: 0.9811 - val_f1: 0.3775 - val_f1_1: 0.0000e+00 - val_f1_2: 0.8371 - val_f1_3: 0.7497 - val_f1_4: 0.6279 - val_f1_5: 1.0000 - val_f1_6: 0.4694 - val_f1_7: 0.9190 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.7355 - val_f1_15: 0.5909 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7598 - val_f1_18: 0.7282 - val_f1_19: 0.5221 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9952 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9450 - val_f1_26: 0.9119 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.2397 - val_f1_31: 0.7973 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8459 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.6559 - val_f1_39: 0.0281 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9374 - val_f1_44: 0.0000e+00 - val_f1_45: 0.8032\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0628 - accuracy: 0.9836 - f1: 0.3909 - f1_1: 0.0000e+00 - f1_2: 0.8293 - f1_3: 0.8317 - f1_4: 0.7311 - f1_5: 0.9982 - f1_6: 0.4456 - f1_7: 0.9188 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.7782 - f1_15: 0.6426 - f1_16: 0.0000e+00 - f1_17: 0.7973 - f1_18: 0.8086 - f1_19: 0.4378 - f1_21: 0.0000e+00 - f1_22: 0.9917 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9447 - f1_26: 0.9129 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.2709 - f1_31: 0.7932 - f1_33: 0.0000e+00 - f1_34: 0.8446 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.7761 - f1_39: 0.1017 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.9249 - f1_44: 0.0000e+00 - f1_45: 0.8555 - val_loss: 0.0714 - val_accuracy: 0.9807 - val_f1: 0.3811 - val_f1_1: 0.0000e+00 - val_f1_2: 0.8473 - val_f1_3: 0.8077 - val_f1_4: 0.6794 - val_f1_5: 1.0000 - val_f1_6: 0.4108 - val_f1_7: 0.9195 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.6840 - val_f1_15: 0.5666 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7131 - val_f1_18: 0.7686 - val_f1_19: 0.5584 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9952 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9465 - val_f1_26: 0.9451 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.2882 - val_f1_31: 0.7887 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8455 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.6621 - val_f1_39: 0.0490 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9297 - val_f1_44: 0.0000e+00 - val_f1_45: 0.8374\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0603 - accuracy: 0.9841 - f1: 0.3992 - f1_1: 0.0000e+00 - f1_2: 0.8408 - f1_3: 0.8492 - f1_4: 0.7508 - f1_5: 0.9982 - f1_6: 0.5094 - f1_7: 0.9232 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.7858 - f1_15: 0.6528 - f1_16: 0.0000e+00 - f1_17: 0.8049 - f1_18: 0.8152 - f1_19: 0.4901 - f1_21: 0.0000e+00 - f1_22: 0.9932 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9465 - f1_26: 0.9179 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.3077 - f1_31: 0.8106 - f1_33: 0.0000e+00 - f1_34: 0.8470 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.7682 - f1_39: 0.1558 - f1_40: 0.0000e+00 - f1_41: 0.0069 - f1_42: 0.0000e+00 - f1_43: 0.9256 - f1_44: 0.0000e+00 - f1_45: 0.8696 - val_loss: 0.0662 - val_accuracy: 0.9825 - val_f1: 0.3966 - val_f1_1: 0.0000e+00 - val_f1_2: 0.8725 - val_f1_3: 0.8120 - val_f1_4: 0.7048 - val_f1_5: 1.0000 - val_f1_6: 0.5150 - val_f1_7: 0.9213 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.7516 - val_f1_15: 0.6264 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7691 - val_f1_18: 0.7641 - val_f1_19: 0.5561 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9952 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9481 - val_f1_26: 0.9301 - val_f1_27: 0.0107 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.3181 - val_f1_31: 0.8104 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8510 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.8490 - val_f1_39: 0.0966 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9374 - val_f1_44: 0.0000e+00 - val_f1_45: 0.8225\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0577 - accuracy: 0.9849 - f1: 0.4102 - f1_1: 0.0000e+00 - f1_2: 0.8557 - f1_3: 0.8499 - f1_4: 0.7668 - f1_5: 0.9982 - f1_6: 0.5270 - f1_7: 0.9247 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.7963 - f1_15: 0.6692 - f1_16: 0.0000e+00 - f1_17: 0.8128 - f1_18: 0.8239 - f1_19: 0.5031 - f1_21: 0.0000e+00 - f1_22: 0.9939 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9483 - f1_26: 0.9333 - f1_27: 0.0542 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.3707 - f1_31: 0.8196 - f1_33: 0.0000e+00 - f1_34: 0.8535 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.8518 - f1_39: 0.2186 - f1_40: 0.0000e+00 - f1_41: 0.0489 - f1_42: 0.0000e+00 - f1_43: 0.9206 - f1_44: 0.0000e+00 - f1_45: 0.8683 - val_loss: 0.0647 - val_accuracy: 0.9826 - val_f1: 0.4045 - val_f1_1: 0.0000e+00 - val_f1_2: 0.8664 - val_f1_3: 0.8130 - val_f1_4: 0.7284 - val_f1_5: 1.0000 - val_f1_6: 0.4678 - val_f1_7: 0.9265 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.7382 - val_f1_15: 0.6224 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7532 - val_f1_18: 0.7892 - val_f1_19: 0.5859 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9952 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9502 - val_f1_26: 0.9432 - val_f1_27: 0.1474 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.3542 - val_f1_31: 0.8043 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8538 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.8528 - val_f1_39: 0.1895 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0197 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9374 - val_f1_44: 0.0000e+00 - val_f1_45: 0.8414\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0556 - accuracy: 0.9852 - f1: 0.4215 - f1_1: 0.0000e+00 - f1_2: 0.8685 - f1_3: 0.8649 - f1_4: 0.7801 - f1_5: 0.9981 - f1_6: 0.5497 - f1_7: 0.9305 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.8008 - f1_15: 0.6738 - f1_16: 0.0000e+00 - f1_17: 0.8221 - f1_18: 0.8285 - f1_19: 0.5377 - f1_21: 0.0000e+00 - f1_22: 0.9946 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9503 - f1_26: 0.9314 - f1_27: 0.1818 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.3807 - f1_31: 0.8295 - f1_33: 0.0000e+00 - f1_34: 0.8637 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.8826 - f1_39: 0.2972 - f1_40: 0.0000e+00 - f1_41: 0.0909 - f1_42: 0.0000e+00 - f1_43: 0.9141 - f1_44: 0.0000e+00 - f1_45: 0.8892 - val_loss: 0.0619 - val_accuracy: 0.9833 - val_f1: 0.4153 - val_f1_1: 0.0000e+00 - val_f1_2: 0.8774 - val_f1_3: 0.8178 - val_f1_4: 0.7688 - val_f1_5: 1.0000 - val_f1_6: 0.5385 - val_f1_7: 0.9237 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.7886 - val_f1_15: 0.6011 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7772 - val_f1_18: 0.7873 - val_f1_19: 0.5635 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9952 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9517 - val_f1_26: 0.9469 - val_f1_27: 0.1785 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.3592 - val_f1_31: 0.8243 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8576 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.8662 - val_f1_39: 0.1924 - val_f1_40: 0.0000e+00 - val_f1_41: 0.2229 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9331 - val_f1_44: 0.0000e+00 - val_f1_45: 0.8416\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.0535 - accuracy: 0.9857 - f1: 0.4382 - f1_1: 0.0000e+00 - f1_2: 0.8727 - f1_3: 0.8722 - f1_4: 0.7992 - f1_5: 0.9980 - f1_6: 0.5832 - f1_7: 0.9309 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.8084 - f1_15: 0.6756 - f1_16: 0.0000e+00 - f1_17: 0.8297 - f1_18: 0.8309 - f1_19: 0.6078 - f1_21: 0.0000e+00 - f1_22: 0.9943 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9526 - f1_26: 0.9433 - f1_27: 0.3822 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.4241 - f1_31: 0.8394 - f1_33: 0.0000e+00 - f1_34: 0.8726 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.8912 - f1_39: 0.3132 - f1_40: 0.0000e+00 - f1_41: 0.2953 - f1_42: 0.0000e+00 - f1_43: 0.9228 - f1_44: 0.0000e+00 - f1_45: 0.8901 - val_loss: 0.0599 - val_accuracy: 0.9838 - val_f1: 0.4330 - val_f1_1: 0.0000e+00 - val_f1_2: 0.8764 - val_f1_3: 0.8292 - val_f1_4: 0.7219 - val_f1_5: 1.0000 - val_f1_6: 0.5552 - val_f1_7: 0.9295 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.7896 - val_f1_15: 0.6357 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7891 - val_f1_18: 0.7747 - val_f1_19: 0.5851 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9952 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9522 - val_f1_26: 0.9469 - val_f1_27: 0.4980 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.3980 - val_f1_31: 0.8298 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8754 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.8931 - val_f1_39: 0.2091 - val_f1_40: 0.0000e+00 - val_f1_41: 0.4324 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9374 - val_f1_44: 0.0000e+00 - val_f1_45: 0.8661\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0522 - accuracy: 0.9860 - f1: 0.4483 - f1_1: 0.0000e+00 - f1_2: 0.8830 - f1_3: 0.8884 - f1_4: 0.7968 - f1_5: 0.9981 - f1_6: 0.5997 - f1_7: 0.9340 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.8094 - f1_15: 0.6921 - f1_16: 0.0417 - f1_17: 0.8392 - f1_18: 0.8290 - f1_19: 0.6272 - f1_21: 0.0000e+00 - f1_22: 0.9940 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9555 - f1_26: 0.9441 - f1_27: 0.4353 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.4631 - f1_31: 0.8497 - f1_33: 0.0000e+00 - f1_34: 0.8665 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.9175 - f1_39: 0.3488 - f1_40: 0.0000e+00 - f1_41: 0.3843 - f1_42: 0.0000e+00 - f1_43: 0.9269 - f1_44: 0.0000e+00 - f1_45: 0.9091 - val_loss: 0.0586 - val_accuracy: 0.9842 - val_f1: 0.4429 - val_f1_1: 0.0000e+00 - val_f1_2: 0.8913 - val_f1_3: 0.8248 - val_f1_4: 0.7666 - val_f1_5: 1.0000 - val_f1_6: 0.5701 - val_f1_7: 0.9317 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.8017 - val_f1_15: 0.6267 - val_f1_16: 0.0182 - val_f1_17: 0.7844 - val_f1_18: 0.7744 - val_f1_19: 0.6146 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9952 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9535 - val_f1_26: 0.9537 - val_f1_27: 0.4456 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.4755 - val_f1_31: 0.8327 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8556 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.9213 - val_f1_39: 0.2920 - val_f1_40: 0.0000e+00 - val_f1_41: 0.5716 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9374 - val_f1_44: 0.0000e+00 - val_f1_45: 0.8780\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0502 - accuracy: 0.9864 - f1: 0.4595 - f1_1: 0.0000e+00 - f1_2: 0.8826 - f1_3: 0.8842 - f1_4: 0.8068 - f1_5: 0.9980 - f1_6: 0.6086 - f1_7: 0.9384 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.8194 - f1_15: 0.6983 - f1_16: 0.0333 - f1_17: 0.8418 - f1_18: 0.8378 - f1_19: 0.6464 - f1_21: 0.0000e+00 - f1_22: 0.9947 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9572 - f1_26: 0.9488 - f1_27: 0.5312 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.4893 - f1_31: 0.8473 - f1_33: 0.0000e+00 - f1_34: 0.8824 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.9350 - f1_39: 0.4013 - f1_40: 0.0000e+00 - f1_41: 0.5448 - f1_42: 0.0000e+00 - f1_43: 0.9288 - f1_44: 0.0000e+00 - f1_45: 0.9232 - val_loss: 0.0577 - val_accuracy: 0.9842 - val_f1: 0.4519 - val_f1_1: 0.0000e+00 - val_f1_2: 0.8928 - val_f1_3: 0.8345 - val_f1_4: 0.7940 - val_f1_5: 1.0000 - val_f1_6: 0.5917 - val_f1_7: 0.9346 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.7723 - val_f1_15: 0.6347 - val_f1_16: 0.0485 - val_f1_17: 0.7970 - val_f1_18: 0.7955 - val_f1_19: 0.6558 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9975 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9574 - val_f1_26: 0.9512 - val_f1_27: 0.5357 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.4558 - val_f1_31: 0.8400 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8743 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.9381 - val_f1_39: 0.3537 - val_f1_40: 0.0000e+00 - val_f1_41: 0.6050 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9374 - val_f1_44: 0.0000e+00 - val_f1_45: 0.8765\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.0489 - accuracy: 0.9866 - f1: 0.4690 - f1_1: 0.0000e+00 - f1_2: 0.8998 - f1_3: 0.8934 - f1_4: 0.8180 - f1_5: 0.9980 - f1_6: 0.6420 - f1_7: 0.9387 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.8201 - f1_15: 0.7050 - f1_16: 0.0250 - f1_17: 0.8469 - f1_18: 0.8356 - f1_19: 0.6711 - f1_21: 0.0000e+00 - f1_22: 0.9943 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9584 - f1_26: 0.9487 - f1_27: 0.6534 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.4989 - f1_31: 0.8564 - f1_33: 0.0000e+00 - f1_34: 0.8793 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.9518 - f1_39: 0.4296 - f1_40: 0.0000e+00 - f1_41: 0.6229 - f1_42: 0.0000e+00 - f1_43: 0.9360 - f1_44: 0.0000e+00 - f1_45: 0.9354 - val_loss: 0.0574 - val_accuracy: 0.9841 - val_f1: 0.4590 - val_f1_1: 0.0000e+00 - val_f1_2: 0.8941 - val_f1_3: 0.8502 - val_f1_4: 0.8010 - val_f1_5: 1.0000 - val_f1_6: 0.6107 - val_f1_7: 0.9296 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.7530 - val_f1_15: 0.6285 - val_f1_16: 0.0485 - val_f1_17: 0.8059 - val_f1_18: 0.8025 - val_f1_19: 0.6973 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9975 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9600 - val_f1_26: 0.9634 - val_f1_27: 0.5912 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.5116 - val_f1_31: 0.8329 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8794 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.9290 - val_f1_39: 0.3805 - val_f1_40: 0.0000e+00 - val_f1_41: 0.6450 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9352 - val_f1_44: 0.0000e+00 - val_f1_45: 0.9147\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0477 - accuracy: 0.9869 - f1: 0.4778 - f1_1: 0.0000e+00 - f1_2: 0.9032 - f1_3: 0.8903 - f1_4: 0.8235 - f1_5: 0.9983 - f1_6: 0.6559 - f1_7: 0.9401 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.8201 - f1_15: 0.7096 - f1_16: 0.0625 - f1_17: 0.8391 - f1_18: 0.8368 - f1_19: 0.7067 - f1_21: 0.0000e+00 - f1_22: 0.9908 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9602 - f1_26: 0.9548 - f1_27: 0.7034 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.5256 - f1_31: 0.8489 - f1_33: 0.0000e+00 - f1_34: 0.8825 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0139 - f1_38: 0.9579 - f1_39: 0.4816 - f1_40: 0.0000e+00 - f1_41: 0.7364 - f1_42: 0.0000e+00 - f1_43: 0.9326 - f1_44: 0.0000e+00 - f1_45: 0.9383 - val_loss: 0.0548 - val_accuracy: 0.9847 - val_f1: 0.4636 - val_f1_1: 0.0000e+00 - val_f1_2: 0.9026 - val_f1_3: 0.8528 - val_f1_4: 0.7945 - val_f1_5: 1.0000 - val_f1_6: 0.5995 - val_f1_7: 0.9335 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.8006 - val_f1_15: 0.6309 - val_f1_16: 0.0485 - val_f1_17: 0.8084 - val_f1_18: 0.7945 - val_f1_19: 0.7070 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9952 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9604 - val_f1_26: 0.9620 - val_f1_27: 0.6411 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.5042 - val_f1_31: 0.8423 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8766 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.9371 - val_f1_39: 0.3950 - val_f1_40: 0.0000e+00 - val_f1_41: 0.6919 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9391 - val_f1_44: 0.0000e+00 - val_f1_45: 0.9258\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0460 - accuracy: 0.9874 - f1: 0.4828 - f1_1: 0.0000e+00 - f1_2: 0.9110 - f1_3: 0.9068 - f1_4: 0.8369 - f1_5: 0.9986 - f1_6: 0.6542 - f1_7: 0.9456 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.8312 - f1_15: 0.7160 - f1_16: 0.1079 - f1_17: 0.8500 - f1_18: 0.8443 - f1_19: 0.7217 - f1_21: 0.0000e+00 - f1_22: 0.9941 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9632 - f1_26: 0.9592 - f1_27: 0.7160 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.5474 - f1_31: 0.8620 - f1_33: 0.0000e+00 - f1_34: 0.8953 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.9539 - f1_39: 0.5033 - f1_40: 0.0000e+00 - f1_41: 0.7081 - f1_42: 0.0000e+00 - f1_43: 0.9327 - f1_44: 0.0000e+00 - f1_45: 0.9521 - val_loss: 0.0537 - val_accuracy: 0.9850 - val_f1: 0.4647 - val_f1_1: 0.0000e+00 - val_f1_2: 0.8994 - val_f1_3: 0.8594 - val_f1_4: 0.7704 - val_f1_5: 1.0000 - val_f1_6: 0.6337 - val_f1_7: 0.9387 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.8100 - val_f1_15: 0.6953 - val_f1_16: 0.0485 - val_f1_17: 0.7854 - val_f1_18: 0.8057 - val_f1_19: 0.7100 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9952 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9622 - val_f1_26: 0.9552 - val_f1_27: 0.6541 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.4233 - val_f1_31: 0.8358 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8777 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.9378 - val_f1_39: 0.3907 - val_f1_40: 0.0000e+00 - val_f1_41: 0.7628 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9352 - val_f1_44: 0.0000e+00 - val_f1_45: 0.9017\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0450 - accuracy: 0.9876 - f1: 0.4898 - f1_1: 0.0000e+00 - f1_2: 0.9152 - f1_3: 0.9119 - f1_4: 0.8442 - f1_5: 0.9987 - f1_6: 0.6716 - f1_7: 0.9450 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.8345 - f1_15: 0.7187 - f1_16: 0.1375 - f1_17: 0.8542 - f1_18: 0.8471 - f1_19: 0.7247 - f1_21: 0.0000e+00 - f1_22: 0.9955 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9639 - f1_26: 0.9533 - f1_27: 0.7586 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.5684 - f1_31: 0.8708 - f1_33: 0.0000e+00 - f1_34: 0.8912 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0406 - f1_38: 0.9605 - f1_39: 0.5356 - f1_40: 0.0000e+00 - f1_41: 0.7595 - f1_42: 0.0000e+00 - f1_43: 0.9385 - f1_44: 0.0000e+00 - f1_45: 0.9518 - val_loss: 0.0527 - val_accuracy: 0.9851 - val_f1: 0.4721 - val_f1_1: 0.0000e+00 - val_f1_2: 0.9010 - val_f1_3: 0.8546 - val_f1_4: 0.7979 - val_f1_5: 1.0000 - val_f1_6: 0.6322 - val_f1_7: 0.9402 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.8093 - val_f1_15: 0.7096 - val_f1_16: 0.0485 - val_f1_17: 0.8202 - val_f1_18: 0.7827 - val_f1_19: 0.7277 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9952 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9632 - val_f1_26: 0.9565 - val_f1_27: 0.6541 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.4910 - val_f1_31: 0.8438 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8649 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0303 - val_f1_38: 0.9458 - val_f1_39: 0.4902 - val_f1_40: 0.0000e+00 - val_f1_41: 0.7678 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9429 - val_f1_44: 0.0000e+00 - val_f1_45: 0.9146\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - 74s 5s/step - loss: 0.0435 - accuracy: 0.9880 - f1: 0.4948 - f1_1: 0.0000e+00 - f1_2: 0.9183 - f1_3: 0.9171 - f1_4: 0.8483 - f1_5: 0.9986 - f1_6: 0.6736 - f1_7: 0.9484 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.8331 - f1_15: 0.7306 - f1_16: 0.1549 - f1_17: 0.8610 - f1_18: 0.8496 - f1_19: 0.7395 - f1_21: 0.0000e+00 - f1_22: 0.9932 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9662 - f1_26: 0.9576 - f1_27: 0.8223 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.5856 - f1_31: 0.8734 - f1_33: 0.0000e+00 - f1_34: 0.8931 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0439 - f1_38: 0.9506 - f1_39: 0.5421 - f1_40: 0.0000e+00 - f1_41: 0.7922 - f1_42: 0.0000e+00 - f1_43: 0.9401 - f1_44: 0.0000e+00 - f1_45: 0.9594 - val_loss: 0.0515 - val_accuracy: 0.9854 - val_f1: 0.4762 - val_f1_1: 0.0000e+00 - val_f1_2: 0.9012 - val_f1_3: 0.8581 - val_f1_4: 0.8153 - val_f1_5: 1.0000 - val_f1_6: 0.6269 - val_f1_7: 0.9405 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.8039 - val_f1_15: 0.6851 - val_f1_16: 0.1682 - val_f1_17: 0.8231 - val_f1_18: 0.8039 - val_f1_19: 0.7325 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9975 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9666 - val_f1_26: 0.9559 - val_f1_27: 0.6632 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.5373 - val_f1_31: 0.8508 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8859 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0303 - val_f1_38: 0.9338 - val_f1_39: 0.4534 - val_f1_40: 0.0000e+00 - val_f1_41: 0.7549 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9406 - val_f1_44: 0.0000e+00 - val_f1_45: 0.9194\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0424 - accuracy: 0.9883 - f1: 0.4985 - f1_1: 0.0000e+00 - f1_2: 0.9212 - f1_3: 0.9270 - f1_4: 0.8562 - f1_5: 0.9986 - f1_6: 0.6914 - f1_7: 0.9485 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.8390 - f1_15: 0.7365 - f1_16: 0.1257 - f1_17: 0.8649 - f1_18: 0.8574 - f1_19: 0.7552 - f1_21: 0.0000e+00 - f1_22: 0.9920 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9684 - f1_26: 0.9623 - f1_27: 0.8033 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.6088 - f1_31: 0.8730 - f1_33: 0.0000e+00 - f1_34: 0.9069 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0712 - f1_38: 0.9647 - f1_39: 0.5593 - f1_40: 0.0000e+00 - f1_41: 0.8016 - f1_42: 0.0000e+00 - f1_43: 0.9440 - f1_44: 0.0000e+00 - f1_45: 0.9631 - val_loss: 0.0512 - val_accuracy: 0.9854 - val_f1: 0.4823 - val_f1_1: 0.0000e+00 - val_f1_2: 0.9079 - val_f1_3: 0.9073 - val_f1_4: 0.8125 - val_f1_5: 1.0000 - val_f1_6: 0.6495 - val_f1_7: 0.9419 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.7882 - val_f1_15: 0.6873 - val_f1_16: 0.1981 - val_f1_17: 0.8227 - val_f1_18: 0.8106 - val_f1_19: 0.7509 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9975 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9677 - val_f1_26: 0.9627 - val_f1_27: 0.6541 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.5599 - val_f1_31: 0.8521 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8876 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.9621 - val_f1_39: 0.5103 - val_f1_40: 0.0000e+00 - val_f1_41: 0.7814 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9427 - val_f1_44: 0.0000e+00 - val_f1_45: 0.9384\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - 74s 5s/step - loss: 0.0414 - accuracy: 0.9884 - f1: 0.5038 - f1_1: 0.0000e+00 - f1_2: 0.9262 - f1_3: 0.9459 - f1_4: 0.8506 - f1_5: 0.9985 - f1_6: 0.6978 - f1_7: 0.9510 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.8412 - f1_15: 0.7275 - f1_16: 0.2661 - f1_17: 0.8683 - f1_18: 0.8571 - f1_19: 0.7641 - f1_21: 0.0000e+00 - f1_22: 0.9946 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9695 - f1_26: 0.9582 - f1_27: 0.8000 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.6143 - f1_31: 0.8773 - f1_33: 0.0000e+00 - f1_34: 0.9071 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0595 - f1_38: 0.9700 - f1_39: 0.5774 - f1_40: 0.0000e+00 - f1_41: 0.8078 - f1_42: 0.0000e+00 - f1_43: 0.9576 - f1_44: 0.0000e+00 - f1_45: 0.9659 - val_loss: 0.0498 - val_accuracy: 0.9857 - val_f1: 0.4826 - val_f1_1: 0.0000e+00 - val_f1_2: 0.9016 - val_f1_3: 0.9034 - val_f1_4: 0.8028 - val_f1_5: 1.0000 - val_f1_6: 0.6519 - val_f1_7: 0.9431 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.8198 - val_f1_15: 0.7083 - val_f1_16: 0.2176 - val_f1_17: 0.8032 - val_f1_18: 0.7953 - val_f1_19: 0.7153 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9975 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9708 - val_f1_26: 0.9578 - val_f1_27: 0.6541 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.6256 - val_f1_31: 0.8538 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8916 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0303 - val_f1_38: 0.9503 - val_f1_39: 0.4421 - val_f1_40: 0.0000e+00 - val_f1_41: 0.7682 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9615 - val_f1_44: 0.0000e+00 - val_f1_45: 0.9385\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0404 - accuracy: 0.9887 - f1: 0.5094 - f1_1: 0.0000e+00 - f1_2: 0.9240 - f1_3: 0.9510 - f1_4: 0.8606 - f1_5: 0.9986 - f1_6: 0.7065 - f1_7: 0.9503 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.8454 - f1_15: 0.7495 - f1_16: 0.3178 - f1_17: 0.8626 - f1_18: 0.8650 - f1_19: 0.7604 - f1_21: 0.0000e+00 - f1_22: 0.9924 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9708 - f1_26: 0.9605 - f1_27: 0.8391 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.6362 - f1_31: 0.8845 - f1_33: 0.0000e+00 - f1_34: 0.9049 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0701 - f1_38: 0.9685 - f1_39: 0.5857 - f1_40: 0.0000e+00 - f1_41: 0.8434 - f1_42: 0.0000e+00 - f1_43: 0.9614 - f1_44: 0.0000e+00 - f1_45: 0.9668 - val_loss: 0.0496 - val_accuracy: 0.9858 - val_f1: 0.4875 - val_f1_1: 0.0000e+00 - val_f1_2: 0.9170 - val_f1_3: 0.9059 - val_f1_4: 0.8104 - val_f1_5: 1.0000 - val_f1_6: 0.6827 - val_f1_7: 0.9422 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.8160 - val_f1_15: 0.7177 - val_f1_16: 0.1981 - val_f1_17: 0.8219 - val_f1_18: 0.7871 - val_f1_19: 0.7582 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9975 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9714 - val_f1_26: 0.9667 - val_f1_27: 0.6541 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.6321 - val_f1_31: 0.8609 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8750 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.9690 - val_f1_39: 0.5259 - val_f1_40: 0.0000e+00 - val_f1_41: 0.7993 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9534 - val_f1_44: 0.0000e+00 - val_f1_45: 0.9391\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.0393 - accuracy: 0.9890 - f1: 0.5115 - f1_1: 0.0000e+00 - f1_2: 0.9292 - f1_3: 0.9580 - f1_4: 0.8572 - f1_5: 0.9987 - f1_6: 0.7089 - f1_7: 0.9495 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.8481 - f1_15: 0.7552 - f1_16: 0.2523 - f1_17: 0.8792 - f1_18: 0.8642 - f1_19: 0.7831 - f1_21: 0.0000e+00 - f1_22: 0.9943 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9724 - f1_26: 0.9651 - f1_27: 0.8201 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.6546 - f1_31: 0.8791 - f1_33: 0.0000e+00 - f1_34: 0.9052 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.1011 - f1_38: 0.9708 - f1_39: 0.6082 - f1_40: 0.0000e+00 - f1_41: 0.8655 - f1_42: 0.0000e+00 - f1_43: 0.9690 - f1_44: 0.0000e+00 - f1_45: 0.9712 - val_loss: 0.0490 - val_accuracy: 0.9858 - val_f1: 0.4913 - val_f1_1: 0.0000e+00 - val_f1_2: 0.9150 - val_f1_3: 0.9248 - val_f1_4: 0.8140 - val_f1_5: 1.0000 - val_f1_6: 0.6880 - val_f1_7: 0.9478 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.8114 - val_f1_15: 0.7006 - val_f1_16: 0.2111 - val_f1_17: 0.8347 - val_f1_18: 0.7903 - val_f1_19: 0.7665 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9975 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9723 - val_f1_26: 0.9604 - val_f1_27: 0.6541 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.6551 - val_f1_31: 0.8571 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8871 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.1017 - val_f1_38: 0.9415 - val_f1_39: 0.5282 - val_f1_40: 0.0000e+00 - val_f1_41: 0.7797 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9746 - val_f1_44: 0.0000e+00 - val_f1_45: 0.9369\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - 81s 5s/step - loss: 0.0386 - accuracy: 0.9892 - f1: 0.5164 - f1_1: 0.0000e+00 - f1_2: 0.9354 - f1_3: 0.9697 - f1_4: 0.8678 - f1_5: 0.9985 - f1_6: 0.7113 - f1_7: 0.9541 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.8518 - f1_15: 0.7487 - f1_16: 0.3642 - f1_17: 0.8820 - f1_18: 0.8620 - f1_19: 0.7927 - f1_21: 0.0000e+00 - f1_22: 0.9928 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9730 - f1_26: 0.9682 - f1_27: 0.8043 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.6752 - f1_31: 0.8889 - f1_33: 0.0000e+00 - f1_34: 0.9074 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.1238 - f1_38: 0.9724 - f1_39: 0.6095 - f1_40: 0.0000e+00 - f1_41: 0.8592 - f1_42: 0.0000e+00 - f1_43: 0.9700 - f1_44: 0.0000e+00 - f1_45: 0.9733 - val_loss: 0.0481 - val_accuracy: 0.9863 - val_f1: 0.4932 - val_f1_1: 0.0000e+00 - val_f1_2: 0.9162 - val_f1_3: 0.9378 - val_f1_4: 0.8038 - val_f1_5: 1.0000 - val_f1_6: 0.6832 - val_f1_7: 0.9433 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.8117 - val_f1_15: 0.7338 - val_f1_16: 0.2361 - val_f1_17: 0.8320 - val_f1_18: 0.8071 - val_f1_19: 0.7803 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9975 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9742 - val_f1_26: 0.9612 - val_f1_27: 0.6632 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.6019 - val_f1_31: 0.8598 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8914 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0563 - val_f1_38: 0.9475 - val_f1_39: 0.5568 - val_f1_40: 0.0000e+00 - val_f1_41: 0.7958 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9743 - val_f1_44: 0.0000e+00 - val_f1_45: 0.9632\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0379 - accuracy: 0.9893 - f1: 0.5183 - f1_1: 0.0000e+00 - f1_2: 0.9349 - f1_3: 0.9650 - f1_4: 0.8667 - f1_5: 0.9987 - f1_6: 0.7236 - f1_7: 0.9544 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_13: 0.8505 - f1_15: 0.7666 - f1_16: 0.3519 - f1_17: 0.8737 - f1_18: 0.8619 - f1_19: 0.7934 - f1_21: 0.0000e+00 - f1_22: 0.9926 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.9739 - f1_26: 0.9636 - f1_27: 0.8447 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.6810 - f1_31: 0.8992 - f1_33: 0.0000e+00 - f1_34: 0.9019 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.1033 - f1_38: 0.9706 - f1_39: 0.6127 - f1_40: 0.0000e+00 - f1_41: 0.8894 - f1_42: 0.0000e+00 - f1_43: 0.9800 - f1_44: 0.0000e+00 - f1_45: 0.9778 - val_loss: 0.0478 - val_accuracy: 0.9860 - val_f1: 0.4941 - val_f1_1: 0.0000e+00 - val_f1_2: 0.9244 - val_f1_3: 0.9360 - val_f1_4: 0.8065 - val_f1_5: 1.0000 - val_f1_6: 0.6878 - val_f1_7: 0.9478 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_13: 0.8240 - val_f1_15: 0.7307 - val_f1_16: 0.2199 - val_f1_17: 0.8158 - val_f1_18: 0.7890 - val_f1_19: 0.7121 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9975 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.9736 - val_f1_26: 0.9602 - val_f1_27: 0.6745 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.5885 - val_f1_31: 0.8586 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8982 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.1913 - val_f1_38: 0.9621 - val_f1_39: 0.4987 - val_f1_40: 0.0000e+00 - val_f1_41: 0.8131 - val_f1_42: 0.0096 - val_f1_43: 0.9796 - val_f1_44: 0.0000e+00 - val_f1_45: 0.9626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#History f1 for class"
      ],
      "metadata": {
        "id": "Q_XNg1aXonOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_val_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_val_list[i] = history.history['val_f1_{}'.format(i)][-1]\n",
        "f1_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_list[i] = history.history['f1_{}'.format(i)][-1]"
      ],
      "metadata": {
        "id": "DIeuPw8AorGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index2tag = {value : key for (key, value) in tag2index.items()}\n",
        "index2tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlQ8zRSMou1-",
        "outputId": "980365fc-eef6-46f0-92c0-14dced4e5ced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '-PAD-',\n",
              " 1: 'LS',\n",
              " 2: 'VBZ',\n",
              " 3: 'PRP$',\n",
              " 4: 'VBD',\n",
              " 5: 'TO',\n",
              " 6: 'VBN',\n",
              " 7: 'IN',\n",
              " 8: '``',\n",
              " 9: 'SYM',\n",
              " 10: 'EX',\n",
              " 11: 'PDT',\n",
              " 12: '.',\n",
              " 13: 'NN',\n",
              " 14: \"''\",\n",
              " 15: 'JJ',\n",
              " 16: 'RP',\n",
              " 17: 'NNS',\n",
              " 18: 'NNP',\n",
              " 19: 'VBP',\n",
              " 20: ':',\n",
              " 21: '#',\n",
              " 22: '$',\n",
              " 23: 'WP$',\n",
              " 24: 'NNPS',\n",
              " 25: 'DT',\n",
              " 26: 'PRP',\n",
              " 27: 'WP',\n",
              " 28: 'WRB',\n",
              " 29: '-RRB-',\n",
              " 30: 'RB',\n",
              " 31: 'VB',\n",
              " 32: ',',\n",
              " 33: 'UH',\n",
              " 34: 'CD',\n",
              " 35: '-LRB-',\n",
              " 36: 'FW',\n",
              " 37: 'RBR',\n",
              " 38: 'MD',\n",
              " 39: 'VBG',\n",
              " 40: 'RBS',\n",
              " 41: 'WDT',\n",
              " 42: 'JJR',\n",
              " 43: 'POS',\n",
              " 44: 'JJS',\n",
              " 45: 'CC'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- F1: {}'.format(index2tag[i], f1_list[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKsg3Ui-owlf",
        "outputId": "51c950f8-f7e8-4a05-d3a9-37883eb766ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: LS --- F1: 0.0\n",
            "Tag: VBZ --- F1: 0.9348716139793396\n",
            "Tag: PRP$ --- F1: 0.9649853706359863\n",
            "Tag: VBD --- F1: 0.8666579127311707\n",
            "Tag: TO --- F1: 0.9986684918403625\n",
            "Tag: VBN --- F1: 0.7236140370368958\n",
            "Tag: IN --- F1: 0.9543850421905518\n",
            "Tag: SYM --- F1: 0.0\n",
            "Tag: EX --- F1: 0.0\n",
            "Tag: PDT --- F1: 0.0\n",
            "Tag: NN --- F1: 0.8504886627197266\n",
            "Tag: JJ --- F1: 0.7665518522262573\n",
            "Tag: RP --- F1: 0.3519098162651062\n",
            "Tag: NNS --- F1: 0.8736972808837891\n",
            "Tag: NNP --- F1: 0.8619480133056641\n",
            "Tag: VBP --- F1: 0.7934486269950867\n",
            "Tag: # --- F1: 0.0\n",
            "Tag: $ --- F1: 0.9926410913467407\n",
            "Tag: WP$ --- F1: 0.0\n",
            "Tag: NNPS --- F1: 0.0\n",
            "Tag: DT --- F1: 0.9739046692848206\n",
            "Tag: PRP --- F1: 0.963615357875824\n",
            "Tag: WP --- F1: 0.8446728587150574\n",
            "Tag: WRB --- F1: 0.0\n",
            "Tag: -RRB- --- F1: 0.0\n",
            "Tag: RB --- F1: 0.6810007095336914\n",
            "Tag: VB --- F1: 0.8991740345954895\n",
            "Tag: UH --- F1: 0.0\n",
            "Tag: CD --- F1: 0.9018657803535461\n",
            "Tag: -LRB- --- F1: 0.0\n",
            "Tag: FW --- F1: 0.0\n",
            "Tag: RBR --- F1: 0.10327379405498505\n",
            "Tag: MD --- F1: 0.9705798029899597\n",
            "Tag: VBG --- F1: 0.6126755475997925\n",
            "Tag: RBS --- F1: 0.0\n",
            "Tag: WDT --- F1: 0.8893740773200989\n",
            "Tag: JJR --- F1: 0.0\n",
            "Tag: POS --- F1: 0.9800447821617126\n",
            "Tag: JJS --- F1: 0.0\n",
            "Tag: CC --- F1: 0.9777823090553284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- Val_F1: {}'.format(index2tag[i], f1_val_list[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9wM9cNloyzc",
        "outputId": "41574326-ed1c-4d59-8b05-38f7afa1df48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: LS --- Val_F1: 0.0\n",
            "Tag: VBZ --- Val_F1: 0.9243705868721008\n",
            "Tag: PRP$ --- Val_F1: 0.9359757900238037\n",
            "Tag: VBD --- Val_F1: 0.8065037131309509\n",
            "Tag: TO --- Val_F1: 1.0\n",
            "Tag: VBN --- Val_F1: 0.6878329515457153\n",
            "Tag: IN --- Val_F1: 0.9477909207344055\n",
            "Tag: SYM --- Val_F1: 0.0\n",
            "Tag: EX --- Val_F1: 0.0\n",
            "Tag: PDT --- Val_F1: 0.0\n",
            "Tag: NN --- Val_F1: 0.8239640593528748\n",
            "Tag: JJ --- Val_F1: 0.7306520938873291\n",
            "Tag: RP --- Val_F1: 0.21992650628089905\n",
            "Tag: NNS --- Val_F1: 0.8158490657806396\n",
            "Tag: NNP --- Val_F1: 0.7889589071273804\n",
            "Tag: VBP --- Val_F1: 0.7120714783668518\n",
            "Tag: # --- Val_F1: 0.0\n",
            "Tag: $ --- Val_F1: 0.9975429773330688\n",
            "Tag: WP$ --- Val_F1: 0.0\n",
            "Tag: NNPS --- Val_F1: 0.0\n",
            "Tag: DT --- Val_F1: 0.9735861420631409\n",
            "Tag: PRP --- Val_F1: 0.9601926207542419\n",
            "Tag: WP --- Val_F1: 0.674496591091156\n",
            "Tag: WRB --- Val_F1: 0.0\n",
            "Tag: -RRB- --- Val_F1: 0.0\n",
            "Tag: RB --- Val_F1: 0.5885060429573059\n",
            "Tag: VB --- Val_F1: 0.8586425185203552\n",
            "Tag: UH --- Val_F1: 0.0\n",
            "Tag: CD --- Val_F1: 0.8981855511665344\n",
            "Tag: -LRB- --- Val_F1: 0.0\n",
            "Tag: FW --- Val_F1: 0.0\n",
            "Tag: RBR --- Val_F1: 0.19134198129177094\n",
            "Tag: MD --- Val_F1: 0.9621387720108032\n",
            "Tag: VBG --- Val_F1: 0.4987384080886841\n",
            "Tag: RBS --- Val_F1: 0.0\n",
            "Tag: WDT --- Val_F1: 0.8131476044654846\n",
            "Tag: JJR --- Val_F1: 0.009569376707077026\n",
            "Tag: POS --- Val_F1: 0.9795570969581604\n",
            "Tag: JJS --- Val_F1: 0.0\n",
            "Tag: CC --- Val_F1: 0.9625670909881592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Infos"
      ],
      "metadata": {
        "id": "1uv6mtuMo1Mh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#40 epochs, batch 128\n",
        "loss: 0.0370 - accuracy: 0.9899 - f1: 0.5241\n",
        "\n",
        "loss: 0.0373 - accuracy: 0.9896 - f1: 0.5214"
      ],
      "metadata": {
        "id": "Jot_8JtVo37_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Evaluation"
      ],
      "metadata": {
        "id": "KOQVTC5DSMvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences_X,  test_tags_y = [], []\n",
        "\n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        "\n",
        "for s in test_tags:\n",
        "    test_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ],
      "metadata": {
        "id": "w9Y9N9RYoBNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_pred = model.predict(test_sentences_X)\n",
        "\n",
        "y_pred = np.zeros((y_val_pred.shape[0], y_val_pred.shape[1]), dtype=int)\n",
        "for i in range(len(y_val_pred)):\n",
        "  for j in range(len(y_val_pred[i])):\n",
        "    y_pred[i][j] = np.argmax(y_val_pred[i][j])"
      ],
      "metadata": {
        "id": "bFug9E5DoKWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the f1_score from sklearn"
      ],
      "metadata": {
        "id": "Trdg6NLXSdGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "tags_flat = test_tags_y.flatten()\n",
        "pred_flat = y_pred.flatten()\n",
        "\n",
        "f1_model = f1_score(tags_flat, pred_flat, labels = no_punct_indexes, average='macro', zero_division=0)\n",
        "f1_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bsytk51_PLmR",
        "outputId": "8f4a6675-b437-4f5f-95c5-faba1cabbca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5424090124111094"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ]
}