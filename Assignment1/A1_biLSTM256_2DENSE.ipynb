{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_biLSTM256_2DENSE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Due to**: 23/12/2021 (dd/mm/yyyy)\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Part-of Speech (POS) tagging as Sequence Labelling using Recurrent Neural Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4_wqPdlBcKS"
      },
      "source": [
        "# Intro\n",
        "\n",
        "In this assignment  we will ask you to perform POS tagging using neural architectures\n",
        "\n",
        "You are asked to follow these steps:\n",
        "*   Download the corpora and split it in training and test sets, structuring a dataframe.\n",
        "*   Embed the words using GloVe embeddings\n",
        "*   Create a baseline model, using a simple neural architecture\n",
        "*   Experiment doing small modifications to the baseline model, choose hyperparameters using the validation set\n",
        "*   Evaluate your two best model\n",
        "*   Analyze the errors of your model\n",
        "\n",
        "\n",
        "**Task**: given a corpus of documents, predict the POS tag for each word\n",
        "\n",
        "**Corpus**:\n",
        "Ignore the numeric value in the third column, use only the words/symbols and its label. \n",
        "The corpus is available at:\n",
        "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
        "\n",
        "**Splits**: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
        "\n",
        "\n",
        "**Features**: you MUST use GloVe embeddings as the only input features to the model.\n",
        "\n",
        "**Splitting**: you can decide to split documents into sentences or not, the choice is yours.\n",
        "\n",
        "**I/O structure**: The input data will have three dimensions: 1-documents/sentences, 2-token, 3-features; for the output there are 2 possibilities: if you use one-hot encoding it will be 1-documents/sentences, 2-token labels, 3-classes, if you use a single integer that indicates the number of the class it will be 1-documents/sentences, 2-token labels.\n",
        "\n",
        "**Baseline**: two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top; the choice of hyper-parameters is yours.\n",
        "\n",
        "**Architectures**: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and adding an additional dense layer; do not mix these variantions.\n",
        "\n",
        "\n",
        "**Training and Experiments**: all the experiments must involve only the training and validation sets.\n",
        "\n",
        "**Evaluation**: in the end, only the two best models of your choice (according to the validation set) must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech. DO NOT CONSIDER THE PUNCTUATION CLASSES.\n",
        "\n",
        "**Metrics**: the metric you must use to evaluate your final model is the F1-macro, WITHOUT considering punctuation/symbols classes; during the training process you can use accuracy because you can't use the F1 metric unless you use a single (gigantic) batch because there is no way to aggregate \"partial\" F1 scores computed on mini-batches.\n",
        "\n",
        "**Discussion and Error Analysis** : verify and discuss if the results on the test sets are coherent with those on the validation set; analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
        "\n",
        "**Report**: you are asked to deliver the code of your experiments and a small pdf report of about 2 pages; the pdf must begin with the names of the people of your team and a small abstract (4-5 lines) that sums up your findings.\n",
        "\n",
        "# Out Of Vocabulary (OOV) terms\n",
        "\n",
        "How to handle words that are not in GloVe vocabulary?\n",
        "You can handle them as you want (random embedding, placeholder, whatever!), but they must be STATIC embeddings (you cannot train them).\n",
        "\n",
        "But there is a very important caveat! As usual, the element of the test set must not influence the elements of the other splits!\n",
        "\n",
        "So, when you compute new embeddings for train+validation, you must forget about test documents.\n",
        "The motivation is to emulate a real-world scenario, where you select and train a model in the first stage, without knowing nothing about the testing environment.\n",
        "\n",
        "For implementation convenience, you CAN use a single vocabulary file/matrix/whatever. The principle of the previous point is that the embeddings inside that file/matrix must be generated independently for train and test splits.\n",
        "\n",
        "Basically in a real-world scenario, this is what would happen:\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Training of the model(s)\n",
        "5. Compute embeddings for terms OOV2 of the validation split \n",
        "6. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "7. Validation of the model(s)\n",
        "8. Compute embeddings for terms OOV3 of the test split \n",
        "9. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2+OOV3\n",
        "10. Testing of the final model\n",
        "\n",
        "In this case, where we already have all the documents, we can simplify the process a bit, but the procedure must remain rigorous.\n",
        "\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Compute embeddings for terms OOV2 of the validation split \n",
        "5. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "6. Compute embeddings for terms OOV3 of the test split \n",
        "7. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2\n",
        "8. Training of the model(s)\n",
        "9. Validation of the model(s)\n",
        "10. Testing of the final model\n",
        "\n",
        "Step 2 and step 6 must be completely independent of each other, for what concerns the method and the documents. But they can rely on the previous vocabulary (V1 for step 2 and V3 for step 6)\n",
        "THEREFORE if a word is present both in the training set and the test split and not in the starting vocabulary, its embedding is computed in step 2) and it is not considered OOV anymore in step 6).\n",
        "\n",
        "# Report\n",
        "The report must not be just a copy and paste of graphs and tables!\n",
        "\n",
        "The report must not be longer than 2 pages and must contain:\n",
        "* The names of the member of your team\n",
        "* A short abstract (4-5 lines) that sum ups everything\n",
        "* A general description of the task you have addressed and how you have addressed it\n",
        "* A short description of the models you have used\n",
        "* Some tables that sum up your findings in validation and test and a discussion of those results\n",
        "* The most relevant findings of your error analysis\n",
        "\n",
        "# Evaluation Criterion\n",
        "\n",
        "The goal of this assignment is not to prove you can find best model ever, but to face a common task, structure it correctly, and follow a correct and rigorous experimental procedure.\n",
        "In other words, we don't care if you final models are awful as long as you have followed the correct procedure and wrote a decent report.\n",
        "\n",
        "The score of the assignment will be computed roughly as follows\n",
        "* 1 point for the general setting of the problem\n",
        "* 1 point for the handling of OOV terms\n",
        "* 1 point for the models\n",
        "* 1 point for train-validation-test procedure\n",
        "* 2 point for the discussion of the results, error analysis, and report\n",
        "\n",
        "This distribution of scores is tentative and we may decide to alter it at any moment.\n",
        "We also reserve the right to assign a small bonus (0.5 points) to any assignment that is particularly worthy. Similarly, in case of grave errors, we may decide to assign an equivalent malus (-0.5 points).\n",
        "\n",
        "# Contacts\n",
        "\n",
        "In case of any doubt, question, issue, or help we highly recommend you to check the [course useful material](https://virtuale.unibo.it/pluginfile.php/1036039/mod_resource/content/2/NLP_Course_Useful_Material.pdf) for additional information, and to use the Virtuale forums to discuss with other students.\n",
        "\n",
        "You can always contact us at the following email addresses. To increase the probability of a prompt response, we reccomend you to write to both the teaching assistants.\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Andrea Galassi -> a.galassi@unibo.it\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it\n",
        "\n",
        "\n",
        "# FAQ\n",
        "* You can use a non-trainable Embedding layer to load the glove embeddings\n",
        "* You can use any library of your choice to implement the networks. Two options are tensorflow/keras or pythorch. Both these libraries have all the classes you need to implement these simple architectures and there are plenty of tutorials around, where you can learn how to use them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emdubW66fnlD"
      },
      "source": [
        "import os, shutil  #  file management\n",
        "import sys \n",
        "import pandas as pd  #  dataframe management\n",
        "import numpy as np  #  data manipulation\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request  #  download files\n",
        "import zipfile  #  unzip files\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B-w2pqyfpF1"
      },
      "source": [
        "from keras import metrics \n",
        "from tensorflow.python.keras.metrics import Metric\n",
        "from sklearn.metrics import f1_score\n",
        "import keras.backend as K"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFvajV2sni19",
        "outputId": "84e7ab29-d96b-4d8f-dbba-ef291c0810b3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3xn1MZPgA_v",
        "outputId": "d4d76d20-dc2e-4ff3-a14a-c7e5363b590d"
      },
      "source": [
        "dataset_folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\")\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "    os.makedirs(dataset_folder)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
        "\n",
        "dataset_path = os.path.join(dataset_folder, \"treebank.zip\")\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    urllib.request.urlretrieve(url, dataset_path)\n",
        "    print(\"Successful download\")\n",
        "\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_folder)\n",
        "print(\"Successful extraction\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful download\n",
            "Successful extraction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre Processing\n",
        "- splitting"
      ],
      "metadata": {
        "id": "FiO1v6SJmm37"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKbUsrVHhGAG"
      },
      "source": [
        "dataset_name = \"dependency_treebank\"\n",
        "\n",
        "\n",
        "folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\", dataset_name)\n",
        "\n",
        "pre_train = []\n",
        "pre_valid = []\n",
        "pre_test = []\n",
        "i = 1\n",
        "\n",
        "file_list = sorted(os.listdir(folder))\n",
        "\n",
        "for filename in file_list:\n",
        "  file_path = os.path.join(folder, filename)\n",
        "  if os.path.isfile(file_path):\n",
        "    # open the file\n",
        "      text = []\n",
        "      with open(file_path, mode='r', encoding='utf-8') as text_file:\n",
        "        text = text_file.read()\n",
        "        if i <= 100:\n",
        "          pre_train.append(text)\n",
        "        elif i <= 150:\n",
        "          pre_valid.append(text)\n",
        "        else:\n",
        "          pre_test.append(text)  \n",
        "  i+=1\n",
        "\n",
        "tr = []\n",
        "val = []\n",
        "tes = []\n",
        "\n",
        "for paragraph in pre_train:\n",
        "   tr.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_valid:\n",
        "   val.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_test:\n",
        "   tes.append(paragraph.split('\\n'))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUgbTXi-F7Sa"
      },
      "source": [
        "train = []\n",
        "valid = []\n",
        "test = []\n",
        "\n",
        "for i in tr:\n",
        "  for j in i:\n",
        "    train.append(j.split('\\t'))\n",
        "\n",
        "for i in val:\n",
        "  for j in i:\n",
        "    valid.append(j.split('\\t'))\n",
        "\n",
        "for i in tes:\n",
        "  for j in i:\n",
        "    test.append(j.split('\\t'))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmzVaRuQFp0M"
      },
      "source": [
        "train_sentences = []\n",
        "valid_sentences = []\n",
        "test_sentences = []\n",
        "train_tags = []\n",
        "valid_tags = []\n",
        "test_tags = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in train:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    train_sentences.append(s)\n",
        "    train_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in valid:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    valid_sentences.append(s)\n",
        "    valid_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in test:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    test_sentences.append(s)\n",
        "    test_tags.append(t)\n",
        "    s = []\n",
        "    t = []"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB65cUFLIeEr"
      },
      "source": [
        "flat_train = [item for sublist in train_sentences for item in sublist]\n",
        "flat_valid = [item for sublist in valid_sentences for item in sublist]\n",
        "flat_test = [item for sublist in test_sentences for item in sublist]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "- vocabularies\n",
        "- GloVe download"
      ],
      "metadata": {
        "id": "d4IenRMamtLg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZO6ga1EGHdh"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "train_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "train_tokenizer.fit_on_texts(train_sentences)                    # fit tokeniser on data\n",
        "train_encoded = train_tokenizer.texts_to_sequences(train_sentences)\n",
        "\n",
        "valid_tokenizer = Tokenizer()       \n",
        "valid_tokenizer.fit_on_texts(valid_sentences)                  # instantiate tokeniser\n",
        "valid_encoded = valid_tokenizer.texts_to_sequences(valid_sentences)\n",
        "\n",
        "test_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "test_tokenizer.fit_on_texts(test_sentences)                    # fit tokeniser on data\n",
        "test_encoded = test_tokenizer.texts_to_sequences(test_sentences)  # use the tokeniser to encode input sequence"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(train_tags)\n",
        "tags_encoded = tag_tokenizer.texts_to_sequences(train_tags)"
      ],
      "metadata": {
        "id": "MSvhz4IvmziU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMsG0Yw1l50L",
        "outputId": "92184d96-c466-4d17-e591-ba3e60a9336f"
      },
      "source": [
        "voc = list(set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(valid_tokenizer.word_index.keys()) - set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(test_tokenizer.word_index.keys()) - set(voc))\n",
        "print(len(voc))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7404\n",
            "9901\n",
            "10947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS9RwnOtJuas"
      },
      "source": [
        "word_index = dict(zip(voc, range(2, len(voc)+3)))\n",
        "word_index['-PAD-'] = 0\n",
        "word_index['-OOV-'] = 1\n",
        "\n",
        "tags = set([item for sublist in train_tags for item in sublist])\n",
        "\n",
        "tag2index = {t: i + 1 for i, t in enumerate(tags)}\n",
        "tag2index['-PAD-'] = 0 "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoj4Zx51J2MU",
        "outputId": "769a4575-138e-48e7-f036-564754bd1d4e"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-16 15:19:08--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-12-16 15:19:09--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-12-16 15:19:10--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.42MB/s    in 2m 41s  \n",
            "\n",
            "2021-12-16 15:21:52 (5.10 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc-T6FcLKtS9",
        "outputId": "c9f5bfd7-6e6c-4852-fea5-39d3ea0216b1"
      },
      "source": [
        "path_to_glove_file = os.path.join(os.getcwd(), \"glove.6B.100d.txt\")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding\n",
        "- matrix creation\n",
        "- padding\n",
        "- one hot encoding of tags\n",
        "- punctuation identification"
      ],
      "metadata": {
        "id": "E2IN3Mh-m85T"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0mOStEPMeXt",
        "outputId": "f636869e-8ea4-4d7c-9657-18861b7bbead"
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        embedding_matrix += np.random.uniform(low=-0.05, high=0.05, size=embedding_dim)\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 10271 words (678 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpYrHs0RMtQJ"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8EZM31Js06-",
        "outputId": "c2f80b4f-6f66-4844-b397-a664e278ba6c"
      },
      "source": [
        "MAX_LENGTH = len(max(train_sentences, key=len))\n",
        "print(MAX_LENGTH)\n",
        "MAX_LENGTH2 = len(max(valid_sentences, key=len))\n",
        "print(MAX_LENGTH2)\n",
        "MAX_LENGTH3 = len(max(test_sentences, key=len))\n",
        "print(MAX_LENGTH3)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "249\n",
            "81\n",
            "58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSRa9SwTuNrX"
      },
      "source": [
        "train_sentences_X, valid_sentences_X, train_tags_y, valid_tags_y, = [], [], [], []\n",
        " \n",
        "for s in train_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        "\n",
        "for s in valid_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    valid_sentences_X.append(s_int)\n",
        "\n",
        "\n",
        "for s in train_tags:\n",
        "    train_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "for s in valid_tags:\n",
        "    valid_tags_y.append([tag2index[t] for t in s])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf-XlSrStQx3"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_sentences_X = pad_sequences(valid_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_tags_y = pad_sequences(valid_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)"
      ],
      "metadata": {
        "id": "nUtevCQrnskt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "point = [tag2index['.']]\n",
        "virg = [tag2index[',']]\n",
        "weird_apex = [tag2index['``']]\n",
        "single_apex = [tag2index[\"''\"]]\n",
        "two_dots = [tag2index[':']]\n",
        "pad = [tag2index['-PAD-']]\n",
        "\n",
        "punct_cat_classes = to_categorical([point, virg, weird_apex, single_apex, two_dots, pad], len(tag2index))\n",
        "punct_cat_classes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT8PjDIynuHG",
        "outputId": "4e2e09fb-7de3-4f5e-fd41-8742e41d42da"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 1, 46)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cum_tags = np.zeros(46)\n",
        "for i in punct_cat_classes:\n",
        "  print(i[0])\n",
        "  cum_tags += i[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAqZ0Xkrnw_d",
        "outputId": "16f32edf-48fb-47cb-ab45-d2902db5c145"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "where_tags = np.where(np.logical_not(cum_tags))\n",
        "where_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzJNfc1xn7jA",
        "outputId": "00618ca2-eeff-42ba-8b5c-78379376bee9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 11, 12, 13, 14, 15, 16, 17, 18,\n",
              "        20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 33, 34, 35, 36, 38, 39,\n",
              "        40, 41, 42, 43, 44, 45]),)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_punct_indexes = where_tags[0]\n",
        "n_classes = len(no_punct_indexes)"
      ],
      "metadata": {
        "id": "LrzwDP5Wn9TK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "cat_val_tags_y = to_categorical(valid_tags_y, len(tag2index))\n",
        "print(len(cat_train_tags_y), len(cat_val_tags_y))\n",
        "print(len(train_sentences_X), len(valid_sentences_X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik32ea9dn-5m",
        "outputId": "73a23b2f-b5ae-4097-b950-a89137b901f5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1963 1299\n",
            "1963 1299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mini Data Vizualization Inset"
      ],
      "metadata": {
        "id": "YbT_r953oBA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_tags = np.zeros(46)\n",
        "for i in cat_train_tags_y:\n",
        "  for t in i:\n",
        "    distribution_tags += t"
      ],
      "metadata": {
        "id": "YIyRpFGVoKlH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = sum(distribution_tags[1:])\n",
        "norm = [float(i)/s for i in distribution_tags[1:]]"
      ],
      "metadata": {
        "id": "OAbGJViloL0O"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt "
      ],
      "metadata": {
        "id": "sPiOzlwroPuJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(np.arange(len(norm)),norm)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "px2v7JyxoQrz",
        "outputId": "0557ad6a-f69a-48c5-b72c-9db393e3b0d1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAARNUlEQVR4nO3df6zddX3H8edrRdBpBIU7oy2sNdQtZTo2a3GZcwYiK8NRlxUpulkXlm6JzVzUuLolqJ1LYFnEJfKHjbChzAFhc7sZdY0TExej2Av+YIUxr4hSdFJ+iGMGsfDeH+fbeHq89H7LPffe3s99PpKb+/1+vp9z7vt8eu/rfPr9dVJVSJLa9VOLXYAkaX4Z9JLUOINekhpn0EtS4wx6SWrccYtdwKhTTjmlVq9evdhlSNKScuuttz5QVRMzbTvmgn716tVMTU0tdhmStKQk+eZTbXPXjSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNe6YuzJWi2/1jptmbL/nsvMXuBJJ4+CMXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMb1CvokG5PclWQ6yY4Ztr86yW1JDibZPNR+ZpLPJ9mX5KtJLhpn8ZKk2c0a9ElWAFcC5wHrgIuTrBvp9i3gLcDHR9p/ALy5qs4ANgIfTHLSXIuWJPXX5xOmNgDTVXU3QJLrgE3AHYc6VNU93bYnhx9YVf89tPztJPcDE8D35ly5JKmXPrtuVgL3Dq3v79qOSpINwPHA12fYti3JVJKpAwcOHO1TS5KOYEEOxiZ5IfAx4Per6snR7VW1q6rWV9X6iYmJhShJkpaNPkF/H3Dq0Pqqrq2XJM8FbgL+vKq+cHTlSZLmqk/Q7wXWJlmT5HhgCzDZ58m7/p8APlpVNz79MiVJT9esQV9VB4HtwB7gTuCGqtqXZGeSCwCSvCLJfuBC4MNJ9nUPfwPwauAtSb7cfZ05L69EkjSjPmfdUFW7gd0jbZcOLe9lsEtn9HHXAtfOsUZJ0hx4ZawkNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxvW6BYIkzdXqHTf9RNs9l52/CJUsP87oJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnOfRS9I8OVauHXBGL0mNM+glqXEGvSQ1rlfQJ9mY5K4k00l2zLD91UluS3IwyeaRbVuTfK372jquwiVJ/cwa9ElWAFcC5wHrgIuTrBvp9i3gLcDHRx77fOA9wFnABuA9SZ4397IlSX31mdFvAKar6u6qehy4Dtg03KGq7qmqrwJPjjz2N4BPVdVDVfUw8Clg4xjqliT11CfoVwL3Dq3v79r66PXYJNuSTCWZOnDgQM+nliT1cUwcjK2qXVW1vqrWT0xMLHY5ktSUPkF/H3Dq0Pqqrq2PuTxWkjQGfYJ+L7A2yZokxwNbgMmez78HODfJ87qDsOd2bZKkBTJr0FfVQWA7g4C+E7ihqvYl2ZnkAoAkr0iyH7gQ+HCSfd1jHwL+gsGbxV5gZ9cmSVogve51U1W7gd0jbZcOLe9lsFtmpsdeDVw9hxolSXNwTByMlSTNH4Nekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxvX64BFJ6mP1jpt+ou2ey85fhEo0zBm9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJalyvoE+yMcldSaaT7Jhh+wlJru+235Jkddf+jCTXJLk9yZ1J3j3e8iVJs5k16JOsAK4EzgPWARcnWTfS7RLg4ao6HbgCuLxrvxA4oapeCrwc+MNDbwKSpIXRZ0a/AZiuqrur6nHgOmDTSJ9NwDXd8o3AOUkCFPDsJMcBzwIeB74/lsolSb30CfqVwL1D6/u7thn7VNVB4BHgZAah/3/Ad4BvAX9dVQ+N/oAk25JMJZk6cODAUb8ISdJTm++DsRuAJ4AXAWuAdyR58WinqtpVVeurav3ExMQ8lyRJy0ufoL8POHVofVXXNmOfbjfNicCDwBuBf6uqH1XV/cDngPVzLVqS1F+foN8LrE2yJsnxwBZgcqTPJLC1W94M3FxVxWB3zdkASZ4NvBL4r3EULknqZ9ag7/a5bwf2AHcCN1TVviQ7k1zQdbsKODnJNPB24NApmFcCz0myj8Ebxt9W1VfH/SIkSU+t122Kq2o3sHuk7dKh5ccYnEo5+rhHZ2qXJC0cr4yVpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1Ljen04uJau1Ttu+om2ey47fxEqmdlM9cGxVaO01Dmjl6TG9Qr6JBuT3JVkOsmOGbafkOT6bvstSVYPbXtZks8n2Zfk9iTPHF/5kqTZzBr0SVYAVwLnAeuAi5OsG+l2CfBwVZ0OXAFc3j32OOBa4I+q6gzgNcCPxla9JGlWfWb0G4Dpqrq7qh4HrgM2jfTZBFzTLd8InJMkwLnAV6vqKwBV9WBVPTGe0iVJffQJ+pXAvUPr+7u2GftU1UHgEeBk4CVAJdmT5LYk75rpByTZlmQqydSBAweO9jVIko5gvg/GHge8CnhT9/23k5wz2qmqdlXV+qpaPzExMc8lSdLy0ifo7wNOHVpf1bXN2KfbL38i8CCD2f9nq+qBqvoBsBv45bkWLUnqr0/Q7wXWJlmT5HhgCzA50mcS2NotbwZurqoC9gAvTfLT3RvArwN3jKd0SVIfs14wVVUHk2xnENorgKural+SncBUVU0CVwEfSzINPMTgzYCqejjJBxi8WRSwu6pmvkJGkjQvel0ZW1W7Gex2GW67dGj5MeDCp3jstQxOsZQkLQKvjJWkxhn0ktQ4g16SGufdKyUds7y76Xg4o5ekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1LheQZ9kY5K7kkwn2THD9hOSXN9tvyXJ6pHtpyV5NMk7x1O2JKmvWT8zNskK4ErgtcB+YG+Syaq6Y6jbJcDDVXV6ki3A5cBFQ9s/AHxyfGVLGjbTZ6v6uao6pM+MfgMwXVV3V9XjwHXAppE+m4BruuUbgXOSBCDJ64FvAPvGU7Ik6Wj0CfqVwL1D6/u7thn7VNVB4BHg5CTPAf4UeN+RfkCSbUmmkkwdOHCgb+2SpB7m+2Dse4ErqurRI3Wqql1Vtb6q1k9MTMxzSZK0vMy6jx64Dzh1aH1V1zZTn/1JjgNOBB4EzgI2J/kr4CTgySSPVdWH5ly5JKmXPkG/F1ibZA2DQN8CvHGkzySwFfg8sBm4uaoK+LVDHZK8F3jUkJekhTVr0FfVwSTbgT3ACuDqqtqXZCcwVVWTwFXAx5JMAw8xeDOQJB0D+szoqardwO6RtkuHlh8DLpzlOd77NOqTJM2RV8ZKUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxvU6vVLjM9NdBsE7DUqaP87oJalxzuhn4X2+JS11Br2kReXuzPnnrhtJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhrnefTSEuH55nq6nNFLUuOc0UtzcKRbZHj7DB0rnNFLUuMMeklqXK+gT7IxyV1JppPsmGH7CUmu77bfkmR11/7aJLcmub37fvZ4y5ckzWbWffRJVgBXAq8F9gN7k0xW1R1D3S4BHq6q05NsAS4HLgIeAH6rqr6d5BeAPcDKcb+IVnhWheaDxwrUZ0a/AZiuqrur6nHgOmDTSJ9NwDXd8o3AOUlSVV+qqm937fuAZyU5YRyFS5L66XPWzUrg3qH1/cBZT9Wnqg4meQQ4mcGM/pDfAW6rqh8+/XI1Lv7vQVo+FuT0yiRnMNidc+5TbN8GbAM47bTTFqIkSVo2+uy6uQ84dWh9Vdc2Y58kxwEnAg9266uATwBvrqqvz/QDqmpXVa2vqvUTExNH9wokSUfUJ+j3AmuTrElyPLAFmBzpMwls7ZY3AzdXVSU5CbgJ2FFVnxtX0ZKk/mYN+qo6CGxncMbMncANVbUvyc4kF3TdrgJOTjINvB04dArmduB04NIkX+6+fmbsr0KS9JR67aOvqt3A7pG2S4eWHwMunOFx7wfeP8caJUlz4JWxktQ4b2omSbNY6qcjL5ugX+r/UJL0dLnrRpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGrdsrow9kqV+1ayfCXo4x2N58N+5P4NeS5J/5FJ/7rqRpMY5o58nzjglHSsMekk/Yakft9Lh3HUjSY0z6CWpce66WSLc5y/p6XJGL0mNc0Y/B86yD3ek8XCspMXjjF6SGtdrRp9kI/A3wArgI1V12cj2E4CPAi8HHgQuqqp7um3vBi4BngD+uKr2jK16NW0hT/HzdML+HKvDLYX/rc4a9ElWAFcCrwX2A3uTTFbVHUPdLgEerqrTk2wBLgcuSrIO2AKcAbwI+PckL6mqJ8b9QrQwlsIv9VJmiI6Hv6eH6zOj3wBMV9XdAEmuAzYBw0G/CXhvt3wj8KEk6dqvq6ofAt9IMt093+fHU750dAwAPZWW32RTVUfukGwGNlbVH3TrvwecVVXbh/r8Z9dnf7f+deAsBuH/haq6tmu/CvhkVd048jO2Adu61Z8D7pr7S+MU4IExPE8rHI/DOR6HczwOtxTH42eramKmDcfEWTdVtQvYNc7nTDJVVevH+ZxLmeNxOMfjcI7H4Vobjz5n3dwHnDq0vqprm7FPkuOAExkclO3zWEnSPOoT9HuBtUnWJDmewcHVyZE+k8DWbnkzcHMN9glNAluSnJBkDbAW+OJ4Spck9THrrpuqOphkO7CHwemVV1fVviQ7gamqmgSuAj7WHWx9iMGbAV2/GxgcuD0IvHUBz7gZ666gBjgeh3M8Dud4HK6p8Zj1YKwkaWnzylhJapxBL0mNazLok2xMcleS6SQ7FruehZbk6iT3d9c3HGp7fpJPJfla9/15i1njQkpyapLPJLkjyb4kb+val+WYJHlmki8m+Uo3Hu/r2tckuaX7u7m+O/liWUiyIsmXkvxrt97UWDQX9EO3bDgPWAdc3N2KYTn5O2DjSNsO4NNVtRb4dLe+XBwE3lFV64BXAm/tfieW65j8EDi7qn4ROBPYmOSVDG5dckVVnQ48zODWJsvF24A7h9abGovmgp6hWzZU1ePAoVs2LBtV9VkGZz8N2wRc0y1fA7x+QYtaRFX1naq6rVv+XwZ/0CtZpmNSA492q8/ovgo4m8EtTGAZjUeSVcD5wEe69dDYWLQY9CuBe4fW93dty90Lquo73fL/AC9YzGIWS5LVwC8Bt7CMx6TbVfFl4H7gU8DXge9V1cGuy3L6u/kg8C7gyW79ZBobixaDXrPoLmZbdufVJnkO8I/An1TV94e3LbcxqaonqupMBlerbwB+fpFLWhRJXgfcX1W3LnYt8+mYuNfNmHnbhZl9N8kLq+o7SV7IYCa3bCR5BoOQ//uq+qeueVmPCUBVfS/JZ4BfAU5Kclw3k10ufze/ClyQ5DeBZwLPZfDZG02NRYsz+j63bFiOhm9TsRX4l0WsZUF1+1yvAu6sqg8MbVqWY5JkIslJ3fKzGHzWxJ3AZxjcwgSWyXhU1buralVVrWaQFTdX1ZtobCyavDK2e3f+ID++ZcNfLnJJCyrJPwCvYXCr1e8C7wH+GbgBOA34JvCGqho9YNukJK8C/gO4nR/vh/0zBvvpl92YJHkZgwOMKxhM9m6oqp1JXszg5IXnA18Cfrf7LIllIclrgHdW1etaG4smg16S9GMt7rqRJA0x6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1Lj/h+NaSWrb5I/TQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#F1 Metric"
      ],
      "metadata": {
        "id": "l0m_PqWUoRsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric.\n",
        "\n",
        "    Only computes a batch-wise average of precision.\n",
        "\n",
        "    Computes the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric.\n",
        "\n",
        "    Only computes a batch-wise average of recall.\n",
        "\n",
        "    Computes the recall, a metric for multi-label classification of\n",
        "    how many relevant items are selected.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def f1_binary(y_true, y_pred):\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
        "\n",
        "def metric_wrapper(index):\n",
        "  def f1_for_class(y_true, y_pred):\n",
        "    #get only the desired class\n",
        "    true = y_true[:,:,index]\n",
        "    pred = y_pred[:,:,index]\n",
        "    #return dice per class\n",
        "    tmp = f1_binary(true,pred)\n",
        "    return tmp\n",
        "  f1_for_class.__name__ = 'f1_' + str(index)\n",
        "  return f1_for_class\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    result = 0.0\n",
        "    for class_id in no_punct_indexes:\n",
        "        y_true_single_class = y_true[:,:,class_id]\n",
        "        y_pred_single_class = y_pred[:,:,class_id]\n",
        "        f1_single = f1_binary(y_true_single_class, y_pred_single_class)\n",
        "        result += f1_single / float(n_classes)\n",
        "    return result"
      ],
      "metadata": {
        "id": "ZDaBrpk3oYXO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model compile and fit"
      ],
      "metadata": {
        "id": "sQz388M4oaCW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsVGByW_sdec",
        "outputId": "f671907e-8d50-4af7-cc08-34dad813c8cb"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(embedding_layer)\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(Dense(len(tag2index)))\n",
        "model.add(Dense(len(tag2index)))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=Adam(0.001),\n",
        "             metrics=['accuracy', f1, [metric_wrapper(i) for i in no_punct_indexes]])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 249, 100)          1094900   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 249, 512)         731136    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 249, 46)           23598     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 249, 46)           2162      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,851,796\n",
            "Trainable params: 756,896\n",
            "Non-trainable params: 1,094,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "history = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=40, validation_data=(valid_sentences_X, cat_val_tags_y), callbacks=[callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWIokkJwokrV",
        "outputId": "cfb240f0-197a-4cca-b6ab-97ab5bd7773b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "16/16 [==============================] - 100s 5s/step - loss: 0.7646 - accuracy: 0.8461 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.3532 - val_accuracy: 0.9125 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.3298 - accuracy: 0.9137 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.3076 - val_accuracy: 0.9192 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - 78s 5s/step - loss: 0.2958 - accuracy: 0.9221 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2826 - val_accuracy: 0.9289 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.2744 - accuracy: 0.9326 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2651 - val_accuracy: 0.9368 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.2559 - accuracy: 0.9385 - f1: 8.7047e-06 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 3.4819e-04 - f1_36: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2468 - val_accuracy: 0.9413 - val_f1: 6.3114e-05 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0025 - val_f1_36: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.2364 - accuracy: 0.9436 - f1: 0.0021 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0855 - f1_36: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2264 - val_accuracy: 0.9459 - val_f1: 8.7854e-04 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 5.7904e-04 - val_f1_35: 0.0346 - val_f1_36: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - 79s 5s/step - loss: 0.2147 - accuracy: 0.9471 - f1: 0.0118 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.1307 - f1_35: 0.2772 - f1_36: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0630 - f1_45: 0.0000e+00 - val_loss: 0.2042 - val_accuracy: 0.9493 - val_f1: 0.0268 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.3829 - val_f1_35: 0.3490 - val_f1_36: 0.0031 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.3389 - val_f1_45: 0.0000e+00\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - 78s 5s/step - loss: 0.1919 - accuracy: 0.9510 - f1: 0.0451 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0339 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.7905 - f1_35: 0.4604 - f1_36: 0.0179 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.5020 - f1_45: 0.0000e+00 - val_loss: 0.1822 - val_accuracy: 0.9533 - val_f1: 0.0531 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0529 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8908 - val_f1_35: 0.4220 - val_f1_36: 0.1160 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.6443 - val_f1_45: 0.0000e+00\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.1709 - accuracy: 0.9548 - f1: 0.0641 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0022 - f1_9: 0.0000e+00 - f1_11: 0.0032 - f1_12: 0.0000e+00 - f1_13: 0.1203 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.8973 - f1_35: 0.5726 - f1_36: 0.2326 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.7376 - f1_45: 0.0000e+00 - val_loss: 0.1633 - val_accuracy: 0.9560 - val_f1: 0.0730 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0625 - val_f1_9: 0.0000e+00 - val_f1_11: 0.0139 - val_f1_12: 0.0000e+00 - val_f1_13: 0.2619 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8997 - val_f1_35: 0.5549 - val_f1_36: 0.3390 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.7901 - val_f1_45: 0.0000e+00\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.1532 - accuracy: 0.9582 - f1: 0.0878 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.4133 - f1_9: 0.0000e+00 - f1_11: 0.0520 - f1_12: 0.0000e+00 - f1_13: 0.2950 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9050 - f1_35: 0.6405 - f1_36: 0.3916 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.8143 - f1_45: 0.0000e+00 - val_loss: 0.1482 - val_accuracy: 0.9596 - val_f1: 0.1015 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.7373 - val_f1_9: 0.0000e+00 - val_f1_11: 0.1144 - val_f1_12: 0.0000e+00 - val_f1_13: 0.4558 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9038 - val_f1_35: 0.5684 - val_f1_36: 0.4636 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.8186 - val_f1_45: 0.0000e+00\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.1386 - accuracy: 0.9626 - f1: 0.1120 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0211 - f1_7: 0.0000e+00 - f1_8: 0.8945 - f1_9: 0.0000e+00 - f1_11: 0.2035 - f1_12: 0.0000e+00 - f1_13: 0.4307 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0031 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9143 - f1_35: 0.6628 - f1_36: 0.5014 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.8468 - f1_45: 0.0000e+00 - val_loss: 0.1351 - val_accuracy: 0.9637 - val_f1: 0.1200 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0538 - val_f1_7: 0.0000e+00 - val_f1_8: 0.9800 - val_f1_9: 0.0000e+00 - val_f1_11: 0.2422 - val_f1_12: 0.0000e+00 - val_f1_13: 0.4543 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0061 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0020 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0368 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9292 - val_f1_35: 0.6371 - val_f1_36: 0.6030 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.8574 - val_f1_45: 0.0000e+00\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - 79s 5s/step - loss: 0.1264 - accuracy: 0.9663 - f1: 0.1383 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.3315 - f1_7: 0.0040 - f1_8: 0.9874 - f1_9: 0.0000e+00 - f1_11: 0.3419 - f1_12: 0.0162 - f1_13: 0.5092 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0327 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.1143 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.1276 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9290 - f1_35: 0.6873 - f1_36: 0.5832 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.8687 - f1_45: 0.0000e+00 - val_loss: 0.1242 - val_accuracy: 0.9669 - val_f1: 0.1533 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.6173 - val_f1_7: 0.0339 - val_f1_8: 0.9953 - val_f1_9: 0.0000e+00 - val_f1_11: 0.3846 - val_f1_12: 0.0169 - val_f1_13: 0.5147 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0689 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.1690 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.2159 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9417 - val_f1_35: 0.6494 - val_f1_36: 0.6565 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.8660 - val_f1_45: 0.0000e+00\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.1160 - accuracy: 0.9692 - f1: 0.1792 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0046 - f1_5: 0.0000e+00 - f1_6: 0.8077 - f1_7: 0.1156 - f1_8: 0.9973 - f1_9: 0.0000e+00 - f1_11: 0.4636 - f1_12: 0.1078 - f1_13: 0.5675 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.1239 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.4828 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.3221 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9407 - f1_35: 0.7104 - f1_36: 0.6465 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.8768 - f1_45: 0.0000e+00 - val_loss: 0.1156 - val_accuracy: 0.9695 - val_f1: 0.1930 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0035 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9337 - val_f1_7: 0.1836 - val_f1_8: 0.9994 - val_f1_9: 0.0000e+00 - val_f1_11: 0.4885 - val_f1_12: 0.1480 - val_f1_13: 0.5343 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.1074 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.6627 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.4823 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9451 - val_f1_35: 0.6872 - val_f1_36: 0.6696 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.8740 - val_f1_45: 0.0000e+00\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.1072 - accuracy: 0.9719 - f1: 0.2089 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0210 - f1_5: 0.0000e+00 - f1_6: 0.9851 - f1_7: 0.2709 - f1_8: 0.9985 - f1_9: 0.0000e+00 - f1_11: 0.5552 - f1_12: 0.2261 - f1_13: 0.5956 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.2345 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0019 - f1_25: 0.7340 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.4941 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9435 - f1_35: 0.7299 - f1_36: 0.6787 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.8857 - f1_45: 0.0000e+00 - val_loss: 0.1074 - val_accuracy: 0.9714 - val_f1: 0.2098 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0118 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9952 - val_f1_7: 0.2510 - val_f1_8: 1.0000 - val_f1_9: 0.0000e+00 - val_f1_11: 0.4575 - val_f1_12: 0.3004 - val_f1_13: 0.6705 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.2450 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0104 - val_f1_25: 0.7346 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.5297 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9461 - val_f1_35: 0.6603 - val_f1_36: 0.7040 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.8762 - val_f1_45: 0.0000e+00\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0994 - accuracy: 0.9744 - f1: 0.2279 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0039 - f1_4: 0.0274 - f1_5: 0.0000e+00 - f1_6: 0.9944 - f1_7: 0.3950 - f1_8: 0.9987 - f1_9: 0.0000e+00 - f1_11: 0.5873 - f1_12: 0.3603 - f1_13: 0.6513 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.3275 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.1026 - f1_25: 0.7931 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.5791 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9469 - f1_35: 0.7414 - f1_36: 0.7127 - f1_38: 0.0032 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.8897 - f1_45: 0.0000e+00 - val_loss: 0.1009 - val_accuracy: 0.9738 - val_f1: 0.2326 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0253 - val_f1_4: 0.0234 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9975 - val_f1_7: 0.4371 - val_f1_8: 1.0000 - val_f1_9: 0.0000e+00 - val_f1_11: 0.5534 - val_f1_12: 0.5283 - val_f1_13: 0.5986 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.3469 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.1891 - val_f1_25: 0.7599 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.5572 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9493 - val_f1_35: 0.7182 - val_f1_36: 0.7200 - val_f1_38: 0.0145 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.8868 - val_f1_45: 0.0000e+00\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0924 - accuracy: 0.9764 - f1: 0.2518 - f1_1: 0.0000e+00 - f1_2: 0.0137 - f1_3: 0.1319 - f1_4: 0.0639 - f1_5: 0.0000e+00 - f1_6: 0.9950 - f1_7: 0.5008 - f1_8: 0.9987 - f1_9: 0.0000e+00 - f1_11: 0.6336 - f1_12: 0.4741 - f1_13: 0.6690 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.3917 - f1_18: 0.0000e+00 - f1_20: 0.0831 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.3115 - f1_25: 0.8131 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.6329 - f1_30: 0.0000e+00 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9471 - f1_35: 0.7566 - f1_36: 0.7273 - f1_38: 0.0302 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.8960 - f1_45: 0.0000e+00 - val_loss: 0.0941 - val_accuracy: 0.9755 - val_f1: 0.2678 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0209 - val_f1_3: 0.2904 - val_f1_4: 0.0707 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9975 - val_f1_7: 0.5048 - val_f1_8: 1.0000 - val_f1_9: 0.0000e+00 - val_f1_11: 0.5790 - val_f1_12: 0.6063 - val_f1_13: 0.6817 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.3576 - val_f1_18: 0.0000e+00 - val_f1_20: 0.3196 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.4120 - val_f1_25: 0.7658 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.6464 - val_f1_30: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9497 - val_f1_35: 0.7110 - val_f1_36: 0.7508 - val_f1_38: 0.1569 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.8915 - val_f1_45: 0.0000e+00\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0864 - accuracy: 0.9777 - f1: 0.2913 - f1_1: 0.0000e+00 - f1_2: 0.1066 - f1_3: 0.3655 - f1_4: 0.0852 - f1_5: 0.0000e+00 - f1_6: 0.9941 - f1_7: 0.5905 - f1_8: 0.9972 - f1_9: 0.0000e+00 - f1_11: 0.6570 - f1_12: 0.6035 - f1_13: 0.6952 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.4461 - f1_18: 0.0000e+00 - f1_20: 0.6097 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.4778 - f1_25: 0.8276 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.6813 - f1_30: 0.0017 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9496 - f1_35: 0.7733 - f1_36: 0.7503 - f1_38: 0.1389 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.8996 - f1_45: 0.0000e+00 - val_loss: 0.0886 - val_accuracy: 0.9765 - val_f1: 0.3028 - val_f1_1: 0.0000e+00 - val_f1_2: 0.2438 - val_f1_3: 0.3719 - val_f1_4: 0.0896 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9975 - val_f1_7: 0.5773 - val_f1_8: 1.0000 - val_f1_9: 0.0000e+00 - val_f1_11: 0.6208 - val_f1_12: 0.6430 - val_f1_13: 0.7176 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.3955 - val_f1_18: 0.0000e+00 - val_f1_20: 0.7128 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.5933 - val_f1_25: 0.7693 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.7076 - val_f1_30: 0.0354 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9503 - val_f1_35: 0.7023 - val_f1_36: 0.7744 - val_f1_38: 0.3122 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.8958 - val_f1_45: 0.0000e+00\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0810 - accuracy: 0.9791 - f1: 0.3233 - f1_1: 0.0000e+00 - f1_2: 0.2244 - f1_3: 0.4552 - f1_4: 0.1189 - f1_5: 0.0000e+00 - f1_6: 0.9932 - f1_7: 0.6496 - f1_8: 0.9986 - f1_9: 0.0000e+00 - f1_11: 0.6929 - f1_12: 0.6803 - f1_13: 0.7200 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.4740 - f1_18: 0.0000e+00 - f1_20: 0.8383 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.6807 - f1_25: 0.8380 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.7250 - f1_30: 0.0461 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9511 - f1_35: 0.7787 - f1_36: 0.7672 - f1_38: 0.3853 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0069 - f1_43: 0.0000e+00 - f1_44: 0.9080 - f1_45: 0.0000e+00 - val_loss: 0.0843 - val_accuracy: 0.9773 - val_f1: 0.3247 - val_f1_1: 0.0000e+00 - val_f1_2: 0.2773 - val_f1_3: 0.4257 - val_f1_4: 0.1401 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9975 - val_f1_7: 0.6595 - val_f1_8: 1.0000 - val_f1_9: 0.0000e+00 - val_f1_11: 0.6396 - val_f1_12: 0.7402 - val_f1_13: 0.7129 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.3792 - val_f1_18: 0.0000e+00 - val_f1_20: 0.8616 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.7076 - val_f1_25: 0.8007 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.7095 - val_f1_30: 0.0899 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9514 - val_f1_35: 0.7398 - val_f1_36: 0.7815 - val_f1_38: 0.4690 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0057 - val_f1_43: 0.0000e+00 - val_f1_44: 0.8976 - val_f1_45: 0.0000e+00\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0764 - accuracy: 0.9799 - f1: 0.3447 - f1_1: 0.0000e+00 - f1_2: 0.3503 - f1_3: 0.5178 - f1_4: 0.1687 - f1_5: 0.0000e+00 - f1_6: 0.9932 - f1_7: 0.6894 - f1_8: 0.9985 - f1_9: 0.0000e+00 - f1_11: 0.7141 - f1_12: 0.7496 - f1_13: 0.7353 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.5052 - f1_18: 0.0000e+00 - f1_20: 0.8967 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.7433 - f1_25: 0.8500 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.7515 - f1_30: 0.1100 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9512 - f1_35: 0.7895 - f1_36: 0.7768 - f1_38: 0.5588 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0269 - f1_43: 0.0000e+00 - f1_44: 0.9094 - f1_45: 0.0000e+00 - val_loss: 0.0796 - val_accuracy: 0.9785 - val_f1: 0.3416 - val_f1_1: 0.0000e+00 - val_f1_2: 0.3353 - val_f1_3: 0.4660 - val_f1_4: 0.2304 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9975 - val_f1_7: 0.6709 - val_f1_8: 1.0000 - val_f1_9: 0.0000e+00 - val_f1_11: 0.6856 - val_f1_12: 0.7272 - val_f1_13: 0.7303 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.4568 - val_f1_18: 0.0000e+00 - val_f1_20: 0.9126 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.7354 - val_f1_25: 0.8105 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.7387 - val_f1_30: 0.1170 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9525 - val_f1_35: 0.7473 - val_f1_36: 0.7989 - val_f1_38: 0.6237 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0129 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9126 - val_f1_45: 0.0000e+00\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0724 - accuracy: 0.9809 - f1: 0.3632 - f1_1: 0.0000e+00 - f1_2: 0.3582 - f1_3: 0.5995 - f1_4: 0.2441 - f1_5: 0.0000e+00 - f1_6: 0.9924 - f1_7: 0.7119 - f1_8: 0.9987 - f1_9: 0.0000e+00 - f1_11: 0.7415 - f1_12: 0.7930 - f1_13: 0.7439 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.5214 - f1_18: 0.0000e+00 - f1_20: 0.9221 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.7646 - f1_25: 0.8775 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.7718 - f1_30: 0.2221 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9519 - f1_35: 0.7943 - f1_36: 0.8086 - f1_38: 0.7440 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0512 - f1_43: 0.0000e+00 - f1_44: 0.9144 - f1_45: 0.0000e+00 - val_loss: 0.0761 - val_accuracy: 0.9795 - val_f1: 0.3579 - val_f1_1: 0.0000e+00 - val_f1_2: 0.5234 - val_f1_3: 0.4708 - val_f1_4: 0.2839 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9975 - val_f1_7: 0.6983 - val_f1_8: 1.0000 - val_f1_9: 0.0000e+00 - val_f1_11: 0.6883 - val_f1_12: 0.7903 - val_f1_13: 0.7060 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.5021 - val_f1_18: 0.0000e+00 - val_f1_20: 0.9202 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.7612 - val_f1_25: 0.8244 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.7771 - val_f1_30: 0.2129 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9533 - val_f1_35: 0.7460 - val_f1_36: 0.8216 - val_f1_38: 0.6626 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0573 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9172 - val_f1_45: 0.0000e+00\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0688 - accuracy: 0.9818 - f1: 0.3747 - f1_1: 0.0000e+00 - f1_2: 0.4794 - f1_3: 0.6001 - f1_4: 0.2633 - f1_5: 0.0000e+00 - f1_6: 0.9950 - f1_7: 0.7367 - f1_8: 0.9987 - f1_9: 0.0000e+00 - f1_11: 0.7465 - f1_12: 0.8349 - f1_13: 0.7508 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.5401 - f1_18: 0.0000e+00 - f1_20: 0.9270 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.7930 - f1_25: 0.8904 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.8010 - f1_30: 0.2512 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9514 - f1_35: 0.8085 - f1_36: 0.8219 - f1_38: 0.7918 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0907 - f1_43: 0.0000e+00 - f1_44: 0.9173 - f1_45: 0.0000e+00 - val_loss: 0.0729 - val_accuracy: 0.9802 - val_f1: 0.3701 - val_f1_1: 0.0000e+00 - val_f1_2: 0.5214 - val_f1_3: 0.6033 - val_f1_4: 0.2971 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9975 - val_f1_7: 0.7086 - val_f1_8: 1.0000 - val_f1_9: 0.0000e+00 - val_f1_11: 0.6837 - val_f1_12: 0.7904 - val_f1_13: 0.7652 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.5022 - val_f1_18: 0.0000e+00 - val_f1_20: 0.9394 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.7998 - val_f1_25: 0.8460 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.7638 - val_f1_30: 0.3214 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9543 - val_f1_35: 0.7566 - val_f1_36: 0.8172 - val_f1_38: 0.7443 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0765 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9148 - val_f1_45: 0.0000e+00\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0654 - accuracy: 0.9826 - f1: 0.3882 - f1_1: 0.0000e+00 - f1_2: 0.5155 - f1_3: 0.6868 - f1_4: 0.3516 - f1_5: 0.0000e+00 - f1_6: 0.9873 - f1_7: 0.7449 - f1_8: 0.9984 - f1_9: 0.0000e+00 - f1_11: 0.7664 - f1_12: 0.8569 - f1_13: 0.7662 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.5664 - f1_18: 0.0000e+00 - f1_20: 0.9424 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.8093 - f1_25: 0.9136 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.8006 - f1_30: 0.3728 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9531 - f1_35: 0.8072 - f1_36: 0.8286 - f1_38: 0.7987 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.1420 - f1_43: 0.0000e+00 - f1_44: 0.9189 - f1_45: 0.0000e+00 - val_loss: 0.0697 - val_accuracy: 0.9810 - val_f1: 0.3840 - val_f1_1: 0.0000e+00 - val_f1_2: 0.5620 - val_f1_3: 0.6593 - val_f1_4: 0.2701 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9975 - val_f1_7: 0.7235 - val_f1_8: 1.0000 - val_f1_9: 0.0000e+00 - val_f1_11: 0.7244 - val_f1_12: 0.8434 - val_f1_13: 0.7435 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.5311 - val_f1_18: 0.0000e+00 - val_f1_20: 0.9472 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.8006 - val_f1_25: 0.8787 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8155 - val_f1_30: 0.4036 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9557 - val_f1_35: 0.7680 - val_f1_36: 0.8416 - val_f1_38: 0.8120 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.1596 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9214 - val_f1_45: 0.0000e+00\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0623 - accuracy: 0.9834 - f1: 0.3990 - f1_1: 0.0000e+00 - f1_2: 0.5566 - f1_3: 0.7469 - f1_4: 0.3604 - f1_5: 0.0000e+00 - f1_6: 0.9934 - f1_7: 0.7556 - f1_8: 0.9987 - f1_9: 0.0000e+00 - f1_11: 0.7827 - f1_12: 0.8764 - f1_13: 0.7696 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.5852 - f1_18: 0.0000e+00 - f1_20: 0.9416 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.8294 - f1_25: 0.9156 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.8236 - f1_30: 0.4280 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9546 - f1_35: 0.8086 - f1_36: 0.8462 - f1_38: 0.8557 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.2038 - f1_43: 0.0000e+00 - f1_44: 0.9267 - f1_45: 0.0000e+00 - val_loss: 0.0678 - val_accuracy: 0.9812 - val_f1: 0.3909 - val_f1_1: 0.0000e+00 - val_f1_2: 0.6070 - val_f1_3: 0.7044 - val_f1_4: 0.3053 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9975 - val_f1_7: 0.7306 - val_f1_8: 1.0000 - val_f1_9: 0.0000e+00 - val_f1_11: 0.7034 - val_f1_12: 0.8432 - val_f1_13: 0.7641 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.4814 - val_f1_18: 0.0000e+00 - val_f1_20: 0.9445 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.8122 - val_f1_25: 0.9205 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8069 - val_f1_30: 0.4374 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9574 - val_f1_35: 0.7689 - val_f1_36: 0.8263 - val_f1_38: 0.8815 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.2238 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9212 - val_f1_45: 0.0000e+00\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0599 - accuracy: 0.9839 - f1: 0.4081 - f1_1: 0.0000e+00 - f1_2: 0.5885 - f1_3: 0.7707 - f1_4: 0.4030 - f1_5: 0.0000e+00 - f1_6: 0.9943 - f1_7: 0.7793 - f1_8: 0.9985 - f1_9: 0.0000e+00 - f1_11: 0.7936 - f1_12: 0.8738 - f1_13: 0.7807 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.5900 - f1_18: 0.0000e+00 - f1_20: 0.9426 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.8452 - f1_25: 0.9336 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.8292 - f1_30: 0.4700 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9542 - f1_35: 0.8176 - f1_36: 0.8543 - f1_38: 0.8926 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.2831 - f1_43: 0.0000e+00 - f1_44: 0.9278 - f1_45: 0.0000e+00 - val_loss: 0.0649 - val_accuracy: 0.9821 - val_f1: 0.4011 - val_f1_1: 0.0000e+00 - val_f1_2: 0.5969 - val_f1_3: 0.7606 - val_f1_4: 0.3893 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9975 - val_f1_7: 0.7321 - val_f1_8: 1.0000 - val_f1_9: 0.0114 - val_f1_11: 0.7379 - val_f1_12: 0.8563 - val_f1_13: 0.7837 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.5364 - val_f1_18: 0.0000e+00 - val_f1_20: 0.9484 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.8212 - val_f1_25: 0.8944 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8156 - val_f1_30: 0.4877 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9602 - val_f1_35: 0.7599 - val_f1_36: 0.8497 - val_f1_38: 0.8826 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.2986 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9251 - val_f1_45: 0.0000e+00\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0574 - accuracy: 0.9846 - f1: 0.4179 - f1_1: 0.0000e+00 - f1_2: 0.6033 - f1_3: 0.8311 - f1_4: 0.4731 - f1_5: 0.0000e+00 - f1_6: 0.9940 - f1_7: 0.7889 - f1_8: 0.9987 - f1_9: 0.0274 - f1_11: 0.8018 - f1_12: 0.8893 - f1_13: 0.7856 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.6106 - f1_18: 0.0000e+00 - f1_20: 0.9380 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.8571 - f1_25: 0.9433 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.8384 - f1_30: 0.5353 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9549 - f1_35: 0.8218 - f1_36: 0.8610 - f1_38: 0.9049 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.3243 - f1_43: 0.0000e+00 - f1_44: 0.9335 - f1_45: 0.0000e+00 - val_loss: 0.0625 - val_accuracy: 0.9828 - val_f1: 0.4134 - val_f1_1: 0.0000e+00 - val_f1_2: 0.6253 - val_f1_3: 0.7810 - val_f1_4: 0.4658 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9975 - val_f1_7: 0.7317 - val_f1_8: 1.0000 - val_f1_9: 0.1313 - val_f1_11: 0.7539 - val_f1_12: 0.8713 - val_f1_13: 0.7832 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.5717 - val_f1_18: 0.0000e+00 - val_f1_20: 0.9484 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.8386 - val_f1_25: 0.9098 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8299 - val_f1_30: 0.5060 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9613 - val_f1_35: 0.7641 - val_f1_36: 0.8798 - val_f1_38: 0.9161 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.3426 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9270 - val_f1_45: 0.0000e+00\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0550 - accuracy: 0.9851 - f1: 0.4285 - f1_1: 0.0000e+00 - f1_2: 0.6248 - f1_3: 0.8483 - f1_4: 0.4994 - f1_5: 0.0000e+00 - f1_6: 0.9947 - f1_7: 0.7969 - f1_8: 0.9985 - f1_9: 0.2043 - f1_11: 0.8152 - f1_12: 0.9018 - f1_13: 0.7961 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.6279 - f1_18: 0.0000e+00 - f1_20: 0.9344 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.8678 - f1_25: 0.9452 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.8470 - f1_30: 0.5429 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9564 - f1_35: 0.8280 - f1_36: 0.8749 - f1_38: 0.9298 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.3699 - f1_43: 0.0000e+00 - f1_44: 0.9351 - f1_45: 0.0000e+00 - val_loss: 0.0605 - val_accuracy: 0.9833 - val_f1: 0.4183 - val_f1_1: 0.0000e+00 - val_f1_2: 0.6495 - val_f1_3: 0.8002 - val_f1_4: 0.5203 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9975 - val_f1_7: 0.7513 - val_f1_8: 1.0000 - val_f1_9: 0.1312 - val_f1_11: 0.7653 - val_f1_12: 0.8856 - val_f1_13: 0.7748 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.5650 - val_f1_18: 0.0000e+00 - val_f1_20: 0.9485 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.8506 - val_f1_25: 0.9208 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8293 - val_f1_30: 0.5550 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9621 - val_f1_35: 0.7844 - val_f1_36: 0.8698 - val_f1_38: 0.9051 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.3333 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9311 - val_f1_45: 0.0000e+00\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0531 - accuracy: 0.9855 - f1: 0.4379 - f1_1: 0.0000e+00 - f1_2: 0.6364 - f1_3: 0.8692 - f1_4: 0.5442 - f1_5: 0.0000e+00 - f1_6: 0.9925 - f1_7: 0.8098 - f1_8: 0.9986 - f1_9: 0.3373 - f1_11: 0.8215 - f1_12: 0.9109 - f1_13: 0.7977 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.6325 - f1_18: 0.0000e+00 - f1_20: 0.9450 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.8780 - f1_25: 0.9539 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.8458 - f1_30: 0.5865 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9589 - f1_35: 0.8307 - f1_36: 0.8780 - f1_38: 0.9381 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.4142 - f1_43: 0.0000e+00 - f1_44: 0.9371 - f1_45: 0.0000e+00 - val_loss: 0.0588 - val_accuracy: 0.9837 - val_f1: 0.4276 - val_f1_1: 0.0000e+00 - val_f1_2: 0.6537 - val_f1_3: 0.8582 - val_f1_4: 0.5253 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9975 - val_f1_7: 0.7479 - val_f1_8: 1.0000 - val_f1_9: 0.3271 - val_f1_11: 0.7633 - val_f1_12: 0.8875 - val_f1_13: 0.7827 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.5804 - val_f1_18: 0.0000e+00 - val_f1_20: 0.9498 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.8521 - val_f1_25: 0.9447 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8360 - val_f1_30: 0.5345 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9631 - val_f1_35: 0.7883 - val_f1_36: 0.8880 - val_f1_38: 0.9510 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.3432 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9305 - val_f1_45: 0.0000e+00\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0519 - accuracy: 0.9857 - f1: 0.4428 - f1_1: 0.0000e+00 - f1_2: 0.6410 - f1_3: 0.8889 - f1_4: 0.5705 - f1_5: 0.0000e+00 - f1_6: 0.9324 - f1_7: 0.8174 - f1_8: 0.9987 - f1_9: 0.4043 - f1_11: 0.8168 - f1_12: 0.9225 - f1_13: 0.8040 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0125 - f1_17: 0.6542 - f1_18: 0.0000e+00 - f1_20: 0.9464 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.8850 - f1_25: 0.9623 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.8566 - f1_30: 0.5893 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9607 - f1_35: 0.8260 - f1_36: 0.8789 - f1_38: 0.9496 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.4533 - f1_43: 0.0000e+00 - f1_44: 0.9398 - f1_45: 0.0000e+00 - val_loss: 0.0589 - val_accuracy: 0.9835 - val_f1: 0.4405 - val_f1_1: 0.0000e+00 - val_f1_2: 0.6892 - val_f1_3: 0.8361 - val_f1_4: 0.5878 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9975 - val_f1_7: 0.7562 - val_f1_8: 1.0000 - val_f1_9: 0.6336 - val_f1_11: 0.7925 - val_f1_12: 0.8953 - val_f1_13: 0.7923 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0303 - val_f1_17: 0.6059 - val_f1_18: 0.0000e+00 - val_f1_20: 0.9558 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.8551 - val_f1_25: 0.9498 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8370 - val_f1_30: 0.5526 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9675 - val_f1_35: 0.7014 - val_f1_36: 0.8918 - val_f1_38: 0.9277 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.4328 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9329 - val_f1_45: 0.0000e+00\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0501 - accuracy: 0.9862 - f1: 0.4511 - f1_1: 0.0000e+00 - f1_2: 0.6549 - f1_3: 0.8946 - f1_4: 0.5827 - f1_5: 0.0000e+00 - f1_6: 0.9943 - f1_7: 0.8242 - f1_8: 0.9985 - f1_9: 0.5016 - f1_11: 0.8295 - f1_12: 0.9248 - f1_13: 0.8091 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0437 - f1_17: 0.6574 - f1_18: 0.0000e+00 - f1_20: 0.9663 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.8889 - f1_25: 0.9616 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.8512 - f1_30: 0.6225 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9632 - f1_35: 0.8355 - f1_36: 0.8917 - f1_38: 0.9512 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.4560 - f1_43: 0.0000e+00 - f1_44: 0.9408 - f1_45: 0.0000e+00 - val_loss: 0.0561 - val_accuracy: 0.9843 - val_f1: 0.4457 - val_f1_1: 0.0000e+00 - val_f1_2: 0.6455 - val_f1_3: 0.8719 - val_f1_4: 0.6315 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9975 - val_f1_7: 0.7896 - val_f1_8: 1.0000 - val_f1_9: 0.5931 - val_f1_11: 0.7833 - val_f1_12: 0.9101 - val_f1_13: 0.7649 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0303 - val_f1_17: 0.6009 - val_f1_18: 0.0000e+00 - val_f1_20: 0.9687 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.8749 - val_f1_25: 0.9356 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8454 - val_f1_30: 0.5594 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9654 - val_f1_35: 0.7984 - val_f1_36: 0.8979 - val_f1_38: 0.9586 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.4630 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9419 - val_f1_45: 0.0000e+00\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0481 - accuracy: 0.9868 - f1: 0.4597 - f1_1: 0.0000e+00 - f1_2: 0.6670 - f1_3: 0.9102 - f1_4: 0.6088 - f1_5: 0.0000e+00 - f1_6: 0.9953 - f1_7: 0.8352 - f1_8: 0.9981 - f1_9: 0.6197 - f1_11: 0.8375 - f1_12: 0.9281 - f1_13: 0.8164 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0887 - f1_17: 0.6819 - f1_18: 0.0000e+00 - f1_20: 0.9714 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.8977 - f1_25: 0.9671 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.8714 - f1_30: 0.6083 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9644 - f1_35: 0.8476 - f1_36: 0.8927 - f1_38: 0.9607 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.4744 - f1_43: 0.0000e+00 - f1_44: 0.9439 - f1_45: 0.0000e+00 - val_loss: 0.0557 - val_accuracy: 0.9841 - val_f1: 0.4478 - val_f1_1: 0.0000e+00 - val_f1_2: 0.6740 - val_f1_3: 0.8641 - val_f1_4: 0.6139 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9975 - val_f1_7: 0.7996 - val_f1_8: 1.0000 - val_f1_9: 0.5436 - val_f1_11: 0.7688 - val_f1_12: 0.9164 - val_f1_13: 0.7660 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.1452 - val_f1_17: 0.5889 - val_f1_18: 0.0000e+00 - val_f1_20: 0.9687 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.8757 - val_f1_25: 0.9499 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8443 - val_f1_30: 0.5849 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9696 - val_f1_35: 0.7975 - val_f1_36: 0.8921 - val_f1_38: 0.9633 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.4484 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9403 - val_f1_45: 0.0000e+00\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0467 - accuracy: 0.9871 - f1: 0.4668 - f1_1: 0.0000e+00 - f1_2: 0.7042 - f1_3: 0.9179 - f1_4: 0.6216 - f1_5: 0.0304 - f1_6: 0.9936 - f1_7: 0.8388 - f1_8: 0.9976 - f1_9: 0.6198 - f1_11: 0.8393 - f1_12: 0.9375 - f1_13: 0.8182 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.1729 - f1_17: 0.6786 - f1_18: 0.0000e+00 - f1_20: 0.9796 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.9066 - f1_25: 0.9682 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.8685 - f1_30: 0.6241 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9651 - f1_35: 0.8486 - f1_36: 0.8931 - f1_38: 0.9668 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.5332 - f1_43: 0.0000e+00 - f1_44: 0.9462 - f1_45: 0.0000e+00 - val_loss: 0.0531 - val_accuracy: 0.9852 - val_f1: 0.4550 - val_f1_1: 0.0000e+00 - val_f1_2: 0.6789 - val_f1_3: 0.8936 - val_f1_4: 0.6279 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9975 - val_f1_7: 0.7837 - val_f1_8: 1.0000 - val_f1_9: 0.6463 - val_f1_11: 0.7828 - val_f1_12: 0.9144 - val_f1_13: 0.7980 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0712 - val_f1_17: 0.6629 - val_f1_18: 0.0000e+00 - val_f1_20: 0.9767 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.8844 - val_f1_25: 0.9538 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8447 - val_f1_30: 0.6052 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9725 - val_f1_35: 0.8033 - val_f1_36: 0.8967 - val_f1_38: 0.9674 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.4975 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9397 - val_f1_45: 0.0000e+00\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0454 - accuracy: 0.9875 - f1: 0.4728 - f1_1: 0.0000e+00 - f1_2: 0.7119 - f1_3: 0.9204 - f1_4: 0.6488 - f1_5: 0.1469 - f1_6: 0.9942 - f1_7: 0.8386 - f1_8: 0.9987 - f1_9: 0.6583 - f1_11: 0.8519 - f1_12: 0.9395 - f1_13: 0.8219 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.1584 - f1_17: 0.6843 - f1_18: 0.0000e+00 - f1_20: 0.9784 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.9087 - f1_25: 0.9715 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.8766 - f1_30: 0.6380 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9673 - f1_35: 0.8520 - f1_36: 0.9008 - f1_38: 0.9692 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.5307 - f1_43: 0.0000e+00 - f1_44: 0.9471 - f1_45: 0.0000e+00 - val_loss: 0.0520 - val_accuracy: 0.9852 - val_f1: 0.4598 - val_f1_1: 0.0000e+00 - val_f1_2: 0.6986 - val_f1_3: 0.9037 - val_f1_4: 0.6014 - val_f1_5: 0.0848 - val_f1_6: 0.9975 - val_f1_7: 0.7892 - val_f1_8: 1.0000 - val_f1_9: 0.6740 - val_f1_11: 0.8026 - val_f1_12: 0.9057 - val_f1_13: 0.7983 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.2075 - val_f1_17: 0.6121 - val_f1_18: 0.0000e+00 - val_f1_20: 0.9876 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.8856 - val_f1_25: 0.9528 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8472 - val_f1_30: 0.6100 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9721 - val_f1_35: 0.8090 - val_f1_36: 0.8998 - val_f1_38: 0.9674 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.4412 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9443 - val_f1_45: 0.0000e+00\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0439 - accuracy: 0.9879 - f1: 0.4806 - f1_1: 0.0000e+00 - f1_2: 0.7140 - f1_3: 0.9250 - f1_4: 0.6718 - f1_5: 0.2905 - f1_6: 0.9939 - f1_7: 0.8418 - f1_8: 0.9987 - f1_9: 0.6509 - f1_11: 0.8521 - f1_12: 0.9483 - f1_13: 0.8287 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.2020 - f1_17: 0.6965 - f1_18: 0.0000e+00 - f1_20: 0.9885 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.9132 - f1_25: 0.9748 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.8784 - f1_30: 0.6596 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9671 - f1_35: 0.8563 - f1_36: 0.9048 - f1_38: 0.9721 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.5476 - f1_43: 0.0000e+00 - f1_44: 0.9492 - f1_45: 0.0000e+00 - val_loss: 0.0507 - val_accuracy: 0.9857 - val_f1: 0.4658 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7005 - val_f1_3: 0.9019 - val_f1_4: 0.6733 - val_f1_5: 0.0971 - val_f1_6: 0.9975 - val_f1_7: 0.8002 - val_f1_8: 1.0000 - val_f1_9: 0.7042 - val_f1_11: 0.8043 - val_f1_12: 0.9289 - val_f1_13: 0.8060 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.1928 - val_f1_17: 0.6548 - val_f1_18: 0.0000e+00 - val_f1_20: 0.9867 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.8934 - val_f1_25: 0.9597 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8532 - val_f1_30: 0.6192 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9755 - val_f1_35: 0.8050 - val_f1_36: 0.9019 - val_f1_38: 0.9695 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.4637 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9434 - val_f1_45: 0.0000e+00\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0430 - accuracy: 0.9880 - f1: 0.4893 - f1_1: 0.0000e+00 - f1_2: 0.7162 - f1_3: 0.9328 - f1_4: 0.6789 - f1_5: 0.4294 - f1_6: 0.9928 - f1_7: 0.8601 - f1_8: 0.9987 - f1_9: 0.6728 - f1_11: 0.8580 - f1_12: 0.9524 - f1_13: 0.8293 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.2837 - f1_17: 0.7083 - f1_18: 0.0000e+00 - f1_20: 0.9862 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.9141 - f1_25: 0.9785 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.8822 - f1_30: 0.6689 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9716 - f1_35: 0.8540 - f1_36: 0.9035 - f1_38: 0.9752 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.5735 - f1_43: 0.0000e+00 - f1_44: 0.9504 - f1_45: 0.0000e+00 - val_loss: 0.0502 - val_accuracy: 0.9859 - val_f1: 0.4776 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7251 - val_f1_3: 0.9247 - val_f1_4: 0.6684 - val_f1_5: 0.3094 - val_f1_6: 0.9975 - val_f1_7: 0.7826 - val_f1_8: 1.0000 - val_f1_9: 0.7145 - val_f1_11: 0.8029 - val_f1_12: 0.9248 - val_f1_13: 0.8166 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.2858 - val_f1_17: 0.6833 - val_f1_18: 0.0000e+00 - val_f1_20: 0.9921 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.8884 - val_f1_25: 0.9646 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8525 - val_f1_30: 0.5927 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9757 - val_f1_35: 0.8104 - val_f1_36: 0.9001 - val_f1_38: 0.9765 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.5709 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9442 - val_f1_45: 0.0000e+00\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0418 - accuracy: 0.9883 - f1: 0.4965 - f1_1: 0.0000e+00 - f1_2: 0.7355 - f1_3: 0.9427 - f1_4: 0.6942 - f1_5: 0.4978 - f1_6: 0.9940 - f1_7: 0.8597 - f1_8: 0.9985 - f1_9: 0.7442 - f1_11: 0.8596 - f1_12: 0.9563 - f1_13: 0.8365 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.3294 - f1_17: 0.7194 - f1_18: 0.0000e+00 - f1_20: 0.9880 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.9192 - f1_25: 0.9811 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.8842 - f1_30: 0.6594 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9729 - f1_35: 0.8605 - f1_36: 0.9122 - f1_38: 0.9789 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.5843 - f1_43: 0.0000e+00 - f1_44: 0.9509 - f1_45: 0.0000e+00 - val_loss: 0.0491 - val_accuracy: 0.9861 - val_f1: 0.4826 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7277 - val_f1_3: 0.9294 - val_f1_4: 0.6885 - val_f1_5: 0.3969 - val_f1_6: 0.9975 - val_f1_7: 0.8068 - val_f1_8: 1.0000 - val_f1_9: 0.7359 - val_f1_11: 0.8169 - val_f1_12: 0.9327 - val_f1_13: 0.7884 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.3075 - val_f1_17: 0.6858 - val_f1_18: 0.0000e+00 - val_f1_20: 0.9939 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.9031 - val_f1_25: 0.9660 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8550 - val_f1_30: 0.6166 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9760 - val_f1_35: 0.8190 - val_f1_36: 0.8998 - val_f1_38: 0.9744 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.5387 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9454 - val_f1_45: 0.0000e+00\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0408 - accuracy: 0.9887 - f1: 0.5038 - f1_1: 0.0000e+00 - f1_2: 0.7488 - f1_3: 0.9397 - f1_4: 0.7087 - f1_5: 0.6341 - f1_6: 0.9946 - f1_7: 0.8649 - f1_8: 0.9987 - f1_9: 0.7599 - f1_11: 0.8640 - f1_12: 0.9616 - f1_13: 0.8381 - f1_14: 0.0000e+00 - f1_15: 0.0096 - f1_16: 0.3637 - f1_17: 0.7187 - f1_18: 0.0000e+00 - f1_20: 0.9939 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.9232 - f1_25: 0.9842 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.8902 - f1_30: 0.6886 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9732 - f1_35: 0.8643 - f1_36: 0.9114 - f1_38: 0.9745 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.5918 - f1_43: 0.0000e+00 - f1_44: 0.9529 - f1_45: 0.0000e+00 - val_loss: 0.0480 - val_accuracy: 0.9864 - val_f1: 0.4847 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7058 - val_f1_3: 0.9346 - val_f1_4: 0.6594 - val_f1_5: 0.3843 - val_f1_6: 0.9975 - val_f1_7: 0.8213 - val_f1_8: 1.0000 - val_f1_9: 0.8022 - val_f1_11: 0.8272 - val_f1_12: 0.9350 - val_f1_13: 0.8040 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.3161 - val_f1_17: 0.6533 - val_f1_18: 0.0000e+00 - val_f1_20: 0.9930 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.9037 - val_f1_25: 0.9689 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8561 - val_f1_30: 0.6450 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9769 - val_f1_35: 0.8233 - val_f1_36: 0.9029 - val_f1_38: 0.9744 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.5547 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9473 - val_f1_45: 0.0000e+00\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0400 - accuracy: 0.9888 - f1: 0.5059 - f1_1: 0.0000e+00 - f1_2: 0.7515 - f1_3: 0.9496 - f1_4: 0.7121 - f1_5: 0.6491 - f1_6: 0.9942 - f1_7: 0.8764 - f1_8: 0.9986 - f1_9: 0.7485 - f1_11: 0.8685 - f1_12: 0.9570 - f1_13: 0.8390 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.3727 - f1_17: 0.7338 - f1_18: 0.0000e+00 - f1_20: 0.9894 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.9252 - f1_25: 0.9851 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.8993 - f1_30: 0.6792 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9746 - f1_35: 0.8686 - f1_36: 0.9141 - f1_38: 0.9819 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.6119 - f1_43: 0.0000e+00 - f1_44: 0.9544 - f1_45: 0.0000e+00 - val_loss: 0.0476 - val_accuracy: 0.9864 - val_f1: 0.4903 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7456 - val_f1_3: 0.9488 - val_f1_4: 0.6735 - val_f1_5: 0.5774 - val_f1_6: 0.9975 - val_f1_7: 0.8211 - val_f1_8: 1.0000 - val_f1_9: 0.7921 - val_f1_11: 0.8185 - val_f1_12: 0.9315 - val_f1_13: 0.8257 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.2858 - val_f1_17: 0.6729 - val_f1_18: 0.0000e+00 - val_f1_20: 0.9939 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.9064 - val_f1_25: 0.9735 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8577 - val_f1_30: 0.6179 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9760 - val_f1_35: 0.8073 - val_f1_36: 0.9042 - val_f1_38: 0.9733 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.5630 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9493 - val_f1_45: 0.0000e+00\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0387 - accuracy: 0.9891 - f1: 0.5136 - f1_1: 0.0000e+00 - f1_2: 0.7600 - f1_3: 0.9516 - f1_4: 0.7211 - f1_5: 0.7307 - f1_6: 0.9914 - f1_7: 0.8703 - f1_8: 0.9986 - f1_9: 0.8240 - f1_11: 0.8767 - f1_12: 0.9607 - f1_13: 0.8460 - f1_14: 0.0000e+00 - f1_15: 0.0104 - f1_16: 0.4618 - f1_17: 0.7291 - f1_18: 0.0000e+00 - f1_20: 0.9938 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.9276 - f1_25: 0.9843 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.9001 - f1_30: 0.6849 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9760 - f1_35: 0.8707 - f1_36: 0.9138 - f1_38: 0.9811 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.6248 - f1_43: 0.0000e+00 - f1_44: 0.9555 - f1_45: 0.0000e+00 - val_loss: 0.0468 - val_accuracy: 0.9868 - val_f1: 0.4920 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7282 - val_f1_3: 0.9298 - val_f1_4: 0.7286 - val_f1_5: 0.4354 - val_f1_6: 0.9975 - val_f1_7: 0.8254 - val_f1_8: 1.0000 - val_f1_9: 0.7930 - val_f1_11: 0.8036 - val_f1_12: 0.9421 - val_f1_13: 0.8201 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0217 - val_f1_16: 0.3398 - val_f1_17: 0.6924 - val_f1_18: 0.0000e+00 - val_f1_20: 0.9928 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.9070 - val_f1_25: 0.9687 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8608 - val_f1_30: 0.6761 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9795 - val_f1_35: 0.8261 - val_f1_36: 0.9049 - val_f1_38: 0.9810 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.5819 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9448 - val_f1_45: 0.0000e+00\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0377 - accuracy: 0.9895 - f1: 0.5180 - f1_1: 0.0000e+00 - f1_2: 0.7716 - f1_3: 0.9590 - f1_4: 0.7350 - f1_5: 0.7347 - f1_6: 0.9937 - f1_7: 0.8768 - f1_8: 0.9975 - f1_9: 0.8333 - f1_11: 0.8772 - f1_12: 0.9621 - f1_13: 0.8517 - f1_14: 0.0000e+00 - f1_15: 0.0192 - f1_16: 0.4924 - f1_17: 0.7437 - f1_18: 0.0000e+00 - f1_20: 0.9938 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.9295 - f1_25: 0.9860 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.9046 - f1_30: 0.7120 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9760 - f1_35: 0.8767 - f1_36: 0.9206 - f1_38: 0.9799 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.6353 - f1_43: 0.0000e+00 - f1_44: 0.9564 - f1_45: 0.0000e+00 - val_loss: 0.0461 - val_accuracy: 0.9867 - val_f1: 0.4950 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7493 - val_f1_3: 0.9563 - val_f1_4: 0.6541 - val_f1_5: 0.5781 - val_f1_6: 0.9975 - val_f1_7: 0.8230 - val_f1_8: 1.0000 - val_f1_9: 0.8155 - val_f1_11: 0.8320 - val_f1_12: 0.9301 - val_f1_13: 0.8138 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0217 - val_f1_16: 0.3587 - val_f1_17: 0.6666 - val_f1_18: 0.0000e+00 - val_f1_20: 0.9921 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.9066 - val_f1_25: 0.9711 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8602 - val_f1_30: 0.6604 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9768 - val_f1_35: 0.8278 - val_f1_36: 0.9048 - val_f1_38: 0.9746 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.5809 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9470 - val_f1_45: 0.0000e+00\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0368 - accuracy: 0.9897 - f1: 0.5196 - f1_1: 0.0000e+00 - f1_2: 0.7724 - f1_3: 0.9576 - f1_4: 0.7416 - f1_5: 0.7172 - f1_6: 0.9897 - f1_7: 0.8841 - f1_8: 0.9985 - f1_9: 0.8296 - f1_11: 0.8708 - f1_12: 0.9587 - f1_13: 0.8525 - f1_14: 0.0000e+00 - f1_15: 0.0208 - f1_16: 0.5627 - f1_17: 0.7530 - f1_18: 0.0000e+00 - f1_20: 0.9926 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.9292 - f1_25: 0.9847 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.9030 - f1_30: 0.7032 - f1_31: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9762 - f1_35: 0.8779 - f1_36: 0.9230 - f1_38: 0.9866 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.6379 - f1_43: 0.0000e+00 - f1_44: 0.9594 - f1_45: 0.0000e+00 - val_loss: 0.0456 - val_accuracy: 0.9868 - val_f1: 0.4972 - val_f1_1: 0.0000e+00 - val_f1_2: 0.7308 - val_f1_3: 0.9474 - val_f1_4: 0.7493 - val_f1_5: 0.5571 - val_f1_6: 0.9975 - val_f1_7: 0.8314 - val_f1_8: 1.0000 - val_f1_9: 0.7720 - val_f1_11: 0.8080 - val_f1_12: 0.9612 - val_f1_13: 0.8269 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0577 - val_f1_16: 0.3134 - val_f1_17: 0.7089 - val_f1_18: 0.0000e+00 - val_f1_20: 0.9928 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.9141 - val_f1_25: 0.9739 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.8598 - val_f1_30: 0.6592 - val_f1_31: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9804 - val_f1_35: 0.8215 - val_f1_36: 0.9049 - val_f1_38: 0.9764 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.5931 - val_f1_43: 0.0000e+00 - val_f1_44: 0.9503 - val_f1_45: 0.0000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#History f1 for class"
      ],
      "metadata": {
        "id": "Q_XNg1aXonOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_val_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_val_list[i] = history.history['val_f1_{}'.format(i)][-1]\n",
        "f1_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_list[i] = history.history['f1_{}'.format(i)][-1]"
      ],
      "metadata": {
        "id": "DIeuPw8AorGd"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index2tag = {value : key for (key, value) in tag2index.items()}\n",
        "index2tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlQ8zRSMou1-",
        "outputId": "bfe4addb-92dd-4418-d556-65430e5aeefc"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '-PAD-',\n",
              " 1: 'FW',\n",
              " 2: 'VBP',\n",
              " 3: 'MD',\n",
              " 4: 'RB',\n",
              " 5: 'WP',\n",
              " 6: '$',\n",
              " 7: 'VBD',\n",
              " 8: 'TO',\n",
              " 9: 'WDT',\n",
              " 10: '``',\n",
              " 11: 'NNS',\n",
              " 12: 'PRP',\n",
              " 13: 'NN',\n",
              " 14: 'EX',\n",
              " 15: 'JJR',\n",
              " 16: 'RP',\n",
              " 17: 'JJ',\n",
              " 18: 'WRB',\n",
              " 19: \"''\",\n",
              " 20: 'POS',\n",
              " 21: 'RBS',\n",
              " 22: 'RBR',\n",
              " 23: 'WP$',\n",
              " 24: 'VBZ',\n",
              " 25: 'CC',\n",
              " 26: 'PDT',\n",
              " 27: '-RRB-',\n",
              " 28: 'VB',\n",
              " 29: '.',\n",
              " 30: 'VBN',\n",
              " 31: '#',\n",
              " 32: ':',\n",
              " 33: 'JJS',\n",
              " 34: 'DT',\n",
              " 35: 'NNP',\n",
              " 36: 'CD',\n",
              " 37: ',',\n",
              " 38: 'PRP$',\n",
              " 39: '-LRB-',\n",
              " 40: 'SYM',\n",
              " 41: 'LS',\n",
              " 42: 'VBG',\n",
              " 43: 'NNPS',\n",
              " 44: 'IN',\n",
              " 45: 'UH'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- F1: {}'.format(index2tag[i], f1_list[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKsg3Ui-owlf",
        "outputId": "ee3aa532-8bfd-4300-9a79-4c7a0149ccd7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: FW --- F1: 0.0\n",
            "Tag: VBP --- F1: 0.7723870277404785\n",
            "Tag: MD --- F1: 0.9575689435005188\n",
            "Tag: RB --- F1: 0.7415977716445923\n",
            "Tag: WP --- F1: 0.7172494530677795\n",
            "Tag: $ --- F1: 0.9896852970123291\n",
            "Tag: VBD --- F1: 0.8840832710266113\n",
            "Tag: TO --- F1: 0.9985074400901794\n",
            "Tag: WDT --- F1: 0.8296317458152771\n",
            "Tag: NNS --- F1: 0.8707593083381653\n",
            "Tag: PRP --- F1: 0.9587334394454956\n",
            "Tag: NN --- F1: 0.8525051474571228\n",
            "Tag: EX --- F1: 0.0\n",
            "Tag: JJR --- F1: 0.02083333022892475\n",
            "Tag: RP --- F1: 0.5626649856567383\n",
            "Tag: JJ --- F1: 0.7530472278594971\n",
            "Tag: WRB --- F1: 0.0\n",
            "Tag: POS --- F1: 0.9925894141197205\n",
            "Tag: RBS --- F1: 0.0\n",
            "Tag: RBR --- F1: 0.0\n",
            "Tag: WP$ --- F1: 0.0\n",
            "Tag: VBZ --- F1: 0.929163932800293\n",
            "Tag: CC --- F1: 0.9847011566162109\n",
            "Tag: PDT --- F1: 0.0\n",
            "Tag: -RRB- --- F1: 0.0\n",
            "Tag: VB --- F1: 0.9030479788780212\n",
            "Tag: VBN --- F1: 0.7032182812690735\n",
            "Tag: # --- F1: 0.0\n",
            "Tag: JJS --- F1: 0.0\n",
            "Tag: DT --- F1: 0.9762329459190369\n",
            "Tag: NNP --- F1: 0.8779379725456238\n",
            "Tag: CD --- F1: 0.9229854345321655\n",
            "Tag: PRP$ --- F1: 0.9866335391998291\n",
            "Tag: -LRB- --- F1: 0.0\n",
            "Tag: SYM --- F1: 0.0\n",
            "Tag: LS --- F1: 0.0\n",
            "Tag: VBG --- F1: 0.6378864645957947\n",
            "Tag: NNPS --- F1: 0.0\n",
            "Tag: IN --- F1: 0.9594036340713501\n",
            "Tag: UH --- F1: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- Val_F1: {}'.format(index2tag[i], f1_val_list[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9wM9cNloyzc",
        "outputId": "1fb4cb7d-aeb5-4d02-ebd3-c5f5979bcc7c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: FW --- Val_F1: 0.0\n",
            "Tag: VBP --- Val_F1: 0.7308312058448792\n",
            "Tag: MD --- Val_F1: 0.9474062323570251\n",
            "Tag: RB --- Val_F1: 0.7492861151695251\n",
            "Tag: WP --- Val_F1: 0.5571135878562927\n",
            "Tag: $ --- Val_F1: 0.9975429773330688\n",
            "Tag: VBD --- Val_F1: 0.8313964605331421\n",
            "Tag: TO --- Val_F1: 1.0\n",
            "Tag: WDT --- Val_F1: 0.7720422148704529\n",
            "Tag: NNS --- Val_F1: 0.8079582452774048\n",
            "Tag: PRP --- Val_F1: 0.9612150192260742\n",
            "Tag: NN --- Val_F1: 0.8268750905990601\n",
            "Tag: EX --- Val_F1: 0.0\n",
            "Tag: JJR --- Val_F1: 0.05767543613910675\n",
            "Tag: RP --- Val_F1: 0.31336086988449097\n",
            "Tag: JJ --- Val_F1: 0.7088984847068787\n",
            "Tag: WRB --- Val_F1: 0.0\n",
            "Tag: POS --- Val_F1: 0.9927909970283508\n",
            "Tag: RBS --- Val_F1: 0.0\n",
            "Tag: RBR --- Val_F1: 0.0\n",
            "Tag: WP$ --- Val_F1: 0.0\n",
            "Tag: VBZ --- Val_F1: 0.9140918254852295\n",
            "Tag: CC --- Val_F1: 0.9738811254501343\n",
            "Tag: PDT --- Val_F1: 0.0\n",
            "Tag: -RRB- --- Val_F1: 0.0\n",
            "Tag: VB --- Val_F1: 0.8597869277000427\n",
            "Tag: VBN --- Val_F1: 0.6591776609420776\n",
            "Tag: # --- Val_F1: 0.0\n",
            "Tag: JJS --- Val_F1: 0.0\n",
            "Tag: DT --- Val_F1: 0.9804108142852783\n",
            "Tag: NNP --- Val_F1: 0.8214727640151978\n",
            "Tag: CD --- Val_F1: 0.9049499034881592\n",
            "Tag: PRP$ --- Val_F1: 0.9763710498809814\n",
            "Tag: -LRB- --- Val_F1: 0.0\n",
            "Tag: SYM --- Val_F1: 0.0\n",
            "Tag: LS --- Val_F1: 0.0\n",
            "Tag: VBG --- Val_F1: 0.5930874347686768\n",
            "Tag: NNPS --- Val_F1: 0.0\n",
            "Tag: IN --- Val_F1: 0.9502965807914734\n",
            "Tag: UH --- Val_F1: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Infos"
      ],
      "metadata": {
        "id": "1uv6mtuMo1Mh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#40 epochs, batch 128\n",
        "loss: 0.0370 - accuracy: 0.9899 - f1: 0.5241\n",
        "\n",
        "loss: 0.0373 - accuracy: 0.9896 - f1: 0.5214"
      ],
      "metadata": {
        "id": "Jot_8JtVo37_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Evaluation"
      ],
      "metadata": {
        "id": "KOQVTC5DSMvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences_X,  test_tags_y = [], []\n",
        "\n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        "\n",
        "for s in test_tags:\n",
        "    test_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ],
      "metadata": {
        "id": "w9Y9N9RYoBNi"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "f1 function written for the test evaluation"
      ],
      "metadata": {
        "id": "ElKKPcGLSUHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_score_single(y_true, y_pred):\n",
        "    y_true = set(y_true)\n",
        "    y_pred = set(y_pred)\n",
        "    cross_size = len(y_true & y_pred)\n",
        "    if cross_size == 0: return 0.\n",
        "    p = 1. * cross_size / len(y_pred)\n",
        "    r = 1. * cross_size / len(y_true)\n",
        "    return 2 * p * r / (p + r)\n",
        "    \n",
        "def f1_test(y_true, y_pred):\n",
        "    return np.mean([f1_score_single(x, y) for x, y in zip(y_true, y_pred)])"
      ],
      "metadata": {
        "id": "YPt7KavO9GAr"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_pred = model.predict(test_sentences_X)\n",
        "\n",
        "y_pred = np.zeros((y_val_pred.shape[0], y_val_pred.shape[1]), dtype=int)\n",
        "for i in range(len(y_val_pred)):\n",
        "  for j in range(len(y_val_pred[i])):\n",
        "    y_pred[i][j] = np.argmax(y_val_pred[i][j])\n",
        "\n",
        "\n",
        "f1_val = f1_test(test_tags_y, y_pred)"
      ],
      "metadata": {
        "id": "bFug9E5DoKWe"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZwSHKWC9glW",
        "outputId": "e2e78d8a-f3a6-4e4b-d239-f76a66898592"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9359135051676167"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the f1_score from sklearn"
      ],
      "metadata": {
        "id": "Trdg6NLXSdGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "tags_flat = test_tags_y.flatten()\n",
        "pred_flat = y_pred.flatten()\n",
        "\n",
        "print(f1_score(tags_flat, pred_flat, average='weighted'))\n",
        "print(f1_score(tags_flat, pred_flat, average='macro'))\n",
        "print(f1_score(tags_flat, pred_flat, average='micro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bsytk51_PLmR",
        "outputId": "6574818b-539f-4a69-a3ac-acac726c9f9a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9876800156910079\n",
            "0.7037050045276343\n",
            "0.9881304358538449\n"
          ]
        }
      ]
    }
  ]
}