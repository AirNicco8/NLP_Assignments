{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_biLSTM256_2DENSE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Due to**: 23/12/2021 (dd/mm/yyyy)\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Part-of Speech (POS) tagging as Sequence Labelling using Recurrent Neural Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4_wqPdlBcKS"
      },
      "source": [
        "# Intro\n",
        "\n",
        "In this assignment  we will ask you to perform POS tagging using neural architectures\n",
        "\n",
        "You are asked to follow these steps:\n",
        "*   Download the corpora and split it in training and test sets, structuring a dataframe.\n",
        "*   Embed the words using GloVe embeddings\n",
        "*   Create a baseline model, using a simple neural architecture\n",
        "*   Experiment doing small modifications to the baseline model, choose hyperparameters using the validation set\n",
        "*   Evaluate your two best model\n",
        "*   Analyze the errors of your model\n",
        "\n",
        "\n",
        "**Task**: given a corpus of documents, predict the POS tag for each word\n",
        "\n",
        "**Corpus**:\n",
        "Ignore the numeric value in the third column, use only the words/symbols and its label. \n",
        "The corpus is available at:\n",
        "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
        "\n",
        "**Splits**: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
        "\n",
        "\n",
        "**Features**: you MUST use GloVe embeddings as the only input features to the model.\n",
        "\n",
        "**Splitting**: you can decide to split documents into sentences or not, the choice is yours.\n",
        "\n",
        "**I/O structure**: The input data will have three dimensions: 1-documents/sentences, 2-token, 3-features; for the output there are 2 possibilities: if you use one-hot encoding it will be 1-documents/sentences, 2-token labels, 3-classes, if you use a single integer that indicates the number of the class it will be 1-documents/sentences, 2-token labels.\n",
        "\n",
        "**Baseline**: two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top; the choice of hyper-parameters is yours.\n",
        "\n",
        "**Architectures**: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and adding an additional dense layer; do not mix these variantions.\n",
        "\n",
        "\n",
        "**Training and Experiments**: all the experiments must involve only the training and validation sets.\n",
        "\n",
        "**Evaluation**: in the end, only the two best models of your choice (according to the validation set) must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech. DO NOT CONSIDER THE PUNCTUATION CLASSES.\n",
        "\n",
        "**Metrics**: the metric you must use to evaluate your final model is the F1-macro, WITHOUT considering punctuation/symbols classes; during the training process you can use accuracy because you can't use the F1 metric unless you use a single (gigantic) batch because there is no way to aggregate \"partial\" F1 scores computed on mini-batches.\n",
        "\n",
        "**Discussion and Error Analysis** : verify and discuss if the results on the test sets are coherent with those on the validation set; analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
        "\n",
        "**Report**: you are asked to deliver the code of your experiments and a small pdf report of about 2 pages; the pdf must begin with the names of the people of your team and a small abstract (4-5 lines) that sums up your findings.\n",
        "\n",
        "# Out Of Vocabulary (OOV) terms\n",
        "\n",
        "How to handle words that are not in GloVe vocabulary?\n",
        "You can handle them as you want (random embedding, placeholder, whatever!), but they must be STATIC embeddings (you cannot train them).\n",
        "\n",
        "But there is a very important caveat! As usual, the element of the test set must not influence the elements of the other splits!\n",
        "\n",
        "So, when you compute new embeddings for train+validation, you must forget about test documents.\n",
        "The motivation is to emulate a real-world scenario, where you select and train a model in the first stage, without knowing nothing about the testing environment.\n",
        "\n",
        "For implementation convenience, you CAN use a single vocabulary file/matrix/whatever. The principle of the previous point is that the embeddings inside that file/matrix must be generated independently for train and test splits.\n",
        "\n",
        "Basically in a real-world scenario, this is what would happen:\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Training of the model(s)\n",
        "5. Compute embeddings for terms OOV2 of the validation split \n",
        "6. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "7. Validation of the model(s)\n",
        "8. Compute embeddings for terms OOV3 of the test split \n",
        "9. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2+OOV3\n",
        "10. Testing of the final model\n",
        "\n",
        "In this case, where we already have all the documents, we can simplify the process a bit, but the procedure must remain rigorous.\n",
        "\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Compute embeddings for terms OOV2 of the validation split \n",
        "5. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "6. Compute embeddings for terms OOV3 of the test split \n",
        "7. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2\n",
        "8. Training of the model(s)\n",
        "9. Validation of the model(s)\n",
        "10. Testing of the final model\n",
        "\n",
        "Step 2 and step 6 must be completely independent of each other, for what concerns the method and the documents. But they can rely on the previous vocabulary (V1 for step 2 and V3 for step 6)\n",
        "THEREFORE if a word is present both in the training set and the test split and not in the starting vocabulary, its embedding is computed in step 2) and it is not considered OOV anymore in step 6).\n",
        "\n",
        "# Report\n",
        "The report must not be just a copy and paste of graphs and tables!\n",
        "\n",
        "The report must not be longer than 2 pages and must contain:\n",
        "* The names of the member of your team\n",
        "* A short abstract (4-5 lines) that sum ups everything\n",
        "* A general description of the task you have addressed and how you have addressed it\n",
        "* A short description of the models you have used\n",
        "* Some tables that sum up your findings in validation and test and a discussion of those results\n",
        "* The most relevant findings of your error analysis\n",
        "\n",
        "# Evaluation Criterion\n",
        "\n",
        "The goal of this assignment is not to prove you can find best model ever, but to face a common task, structure it correctly, and follow a correct and rigorous experimental procedure.\n",
        "In other words, we don't care if you final models are awful as long as you have followed the correct procedure and wrote a decent report.\n",
        "\n",
        "The score of the assignment will be computed roughly as follows\n",
        "* 1 point for the general setting of the problem\n",
        "* 1 point for the handling of OOV terms\n",
        "* 1 point for the models\n",
        "* 1 point for train-validation-test procedure\n",
        "* 2 point for the discussion of the results, error analysis, and report\n",
        "\n",
        "This distribution of scores is tentative and we may decide to alter it at any moment.\n",
        "We also reserve the right to assign a small bonus (0.5 points) to any assignment that is particularly worthy. Similarly, in case of grave errors, we may decide to assign an equivalent malus (-0.5 points).\n",
        "\n",
        "# Contacts\n",
        "\n",
        "In case of any doubt, question, issue, or help we highly recommend you to check the [course useful material](https://virtuale.unibo.it/pluginfile.php/1036039/mod_resource/content/2/NLP_Course_Useful_Material.pdf) for additional information, and to use the Virtuale forums to discuss with other students.\n",
        "\n",
        "You can always contact us at the following email addresses. To increase the probability of a prompt response, we reccomend you to write to both the teaching assistants.\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Andrea Galassi -> a.galassi@unibo.it\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it\n",
        "\n",
        "\n",
        "# FAQ\n",
        "* You can use a non-trainable Embedding layer to load the glove embeddings\n",
        "* You can use any library of your choice to implement the networks. Two options are tensorflow/keras or pythorch. Both these libraries have all the classes you need to implement these simple architectures and there are plenty of tutorials around, where you can learn how to use them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emdubW66fnlD"
      },
      "source": [
        "import os, shutil  #  file management\n",
        "import sys \n",
        "import pandas as pd  #  dataframe management\n",
        "import numpy as np  #  data manipulation\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request  #  download files\n",
        "import zipfile  #  unzip files\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B-w2pqyfpF1"
      },
      "source": [
        "from keras import metrics \n",
        "from tensorflow.python.keras.metrics import Metric\n",
        "from sklearn.metrics import f1_score\n",
        "import keras.backend as K"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFvajV2sni19",
        "outputId": "2e9dc9c9-90dd-45b7-a613-9c5a9c4a8d10"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3xn1MZPgA_v",
        "outputId": "4782a3c3-8578-48c1-8bb1-8d1572a63c5e"
      },
      "source": [
        "dataset_folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\")\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "    os.makedirs(dataset_folder)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
        "\n",
        "dataset_path = os.path.join(dataset_folder, \"treebank.zip\")\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    urllib.request.urlretrieve(url, dataset_path)\n",
        "    print(\"Successful download\")\n",
        "\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_folder)\n",
        "print(\"Successful extraction\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful download\n",
            "Successful extraction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre Processing\n",
        "- splitting"
      ],
      "metadata": {
        "id": "FiO1v6SJmm37"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKbUsrVHhGAG"
      },
      "source": [
        "dataset_name = \"dependency_treebank\"\n",
        "\n",
        "\n",
        "folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\", dataset_name)\n",
        "\n",
        "pre_train = []\n",
        "pre_valid = []\n",
        "pre_test = []\n",
        "i = 1\n",
        "\n",
        "file_list = sorted(os.listdir(folder))\n",
        "\n",
        "for filename in file_list:\n",
        "  file_path = os.path.join(folder, filename)\n",
        "  if os.path.isfile(file_path):\n",
        "    # open the file\n",
        "      text = []\n",
        "      with open(file_path, mode='r', encoding='utf-8') as text_file:\n",
        "        text = text_file.read()\n",
        "        if i <= 100:\n",
        "          pre_train.append(text)\n",
        "        elif i <= 150:\n",
        "          pre_valid.append(text)\n",
        "        else:\n",
        "          pre_test.append(text)  \n",
        "  i+=1\n",
        "\n",
        "tr = []\n",
        "val = []\n",
        "tes = []\n",
        "\n",
        "for paragraph in pre_train:\n",
        "   tr.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_valid:\n",
        "   val.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_test:\n",
        "   tes.append(paragraph.split('\\n'))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUgbTXi-F7Sa"
      },
      "source": [
        "train = []\n",
        "valid = []\n",
        "test = []\n",
        "\n",
        "for i in tr:\n",
        "  for j in i:\n",
        "    train.append(j.split('\\t'))\n",
        "\n",
        "for i in val:\n",
        "  for j in i:\n",
        "    valid.append(j.split('\\t'))\n",
        "\n",
        "for i in tes:\n",
        "  for j in i:\n",
        "    test.append(j.split('\\t'))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmzVaRuQFp0M"
      },
      "source": [
        "train_sentences = []\n",
        "valid_sentences = []\n",
        "test_sentences = []\n",
        "train_tags = []\n",
        "valid_tags = []\n",
        "test_tags = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in train:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    train_sentences.append(s)\n",
        "    train_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in valid:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    valid_sentences.append(s)\n",
        "    valid_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in test:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    test_sentences.append(s)\n",
        "    test_tags.append(t)\n",
        "    s = []\n",
        "    t = []"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB65cUFLIeEr"
      },
      "source": [
        "flat_train = [item for sublist in train_sentences for item in sublist]\n",
        "flat_valid = [item for sublist in valid_sentences for item in sublist]\n",
        "flat_test = [item for sublist in test_sentences for item in sublist]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "- vocabularies\n",
        "- GloVe download"
      ],
      "metadata": {
        "id": "d4IenRMamtLg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZO6ga1EGHdh"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "train_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "train_tokenizer.fit_on_texts(train_sentences)                    # fit tokeniser on data\n",
        "train_encoded = train_tokenizer.texts_to_sequences(train_sentences)\n",
        "\n",
        "valid_tokenizer = Tokenizer()       \n",
        "valid_tokenizer.fit_on_texts(valid_sentences)                  # instantiate tokeniser\n",
        "valid_encoded = valid_tokenizer.texts_to_sequences(valid_sentences)\n",
        "\n",
        "test_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "test_tokenizer.fit_on_texts(test_sentences)                    # fit tokeniser on data\n",
        "test_encoded = test_tokenizer.texts_to_sequences(test_sentences)  # use the tokeniser to encode input sequence"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(train_tags)\n",
        "tags_encoded = tag_tokenizer.texts_to_sequences(train_tags)"
      ],
      "metadata": {
        "id": "MSvhz4IvmziU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMsG0Yw1l50L",
        "outputId": "3a537a8f-88e8-45c5-a2c4-fa329b05a400"
      },
      "source": [
        "voc = list(set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(valid_tokenizer.word_index.keys()) - set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(test_tokenizer.word_index.keys()) - set(voc))\n",
        "print(len(voc))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7404\n",
            "9901\n",
            "10947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS9RwnOtJuas"
      },
      "source": [
        "word_index = dict(zip(voc, range(2, len(voc)+3)))\n",
        "word_index['-PAD-'] = 0\n",
        "word_index['-OOV-'] = 1\n",
        "\n",
        "tags = set([item for sublist in train_tags for item in sublist])\n",
        "\n",
        "tag2index = {t: i + 1 for i, t in enumerate(tags)}\n",
        "tag2index['-PAD-'] = 0 "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoj4Zx51J2MU",
        "outputId": "5af1948e-112e-46ea-821f-1bed61bb6dcd"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-14 15:55:05--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-12-14 15:55:05--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-12-14 15:55:05--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.75MB/s    in 2m 41s  \n",
            "\n",
            "2021-12-14 15:57:46 (5.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc-T6FcLKtS9",
        "outputId": "f1fa7b57-0475-4d96-ad2c-724921292706"
      },
      "source": [
        "path_to_glove_file = os.path.join(os.getcwd(), \"glove.6B.100d.txt\")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding\n",
        "- matrix creation\n",
        "- padding\n",
        "- one hot encoding of tags\n",
        "- punctuation identification"
      ],
      "metadata": {
        "id": "E2IN3Mh-m85T"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0mOStEPMeXt",
        "outputId": "e23effb6-f786-4f58-cc87-f8c967aca8cf"
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        embedding_matrix += np.random.uniform(low=-0.05, high=0.05, size=embedding_dim)\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 10271 words (678 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpYrHs0RMtQJ"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8EZM31Js06-",
        "outputId": "23519aca-dd9e-46aa-aec6-47f65b4ede9f"
      },
      "source": [
        "MAX_LENGTH = len(max(train_sentences, key=len))\n",
        "print(MAX_LENGTH)\n",
        "MAX_LENGTH2 = len(max(valid_sentences, key=len))\n",
        "print(MAX_LENGTH2)\n",
        "MAX_LENGTH3 = len(max(test_sentences, key=len))\n",
        "print(MAX_LENGTH3)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "249\n",
            "81\n",
            "58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSRa9SwTuNrX"
      },
      "source": [
        "train_sentences_X, valid_sentences_X, train_tags_y, valid_tags_y, = [], [], [], []\n",
        " \n",
        "for s in train_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        "\n",
        "for s in valid_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    valid_sentences_X.append(s_int)\n",
        "\n",
        "\n",
        "for s in train_tags:\n",
        "    train_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "for s in valid_tags:\n",
        "    valid_tags_y.append([tag2index[t] for t in s])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf-XlSrStQx3"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_sentences_X = pad_sequences(valid_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_tags_y = pad_sequences(valid_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)"
      ],
      "metadata": {
        "id": "nUtevCQrnskt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "point = [tag2index['.']]\n",
        "virg = [tag2index[',']]\n",
        "weird_apex = [tag2index['``']]\n",
        "single_apex = [tag2index[\"''\"]]\n",
        "two_dots = [tag2index[':']]\n",
        "pad = [tag2index['-PAD-']]\n",
        "\n",
        "punct_cat_classes = to_categorical([point, virg, weird_apex, single_apex, two_dots, pad], len(tag2index))\n",
        "punct_cat_classes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT8PjDIynuHG",
        "outputId": "30a03bdf-3c30-4b63-f04a-90fe8c000d21"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 1, 46)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cum_tags = np.zeros(46)\n",
        "for i in punct_cat_classes:\n",
        "  print(i[0])\n",
        "  cum_tags += i[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAqZ0Xkrnw_d",
        "outputId": "44254c59-8f21-4963-889b-afef2dee93ea"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "where_tags = np.where(np.logical_not(cum_tags))\n",
        "where_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzJNfc1xn7jA",
        "outputId": "fc33bbaf-e5f4-477c-9287-6c378e9167de"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 1,  2,  3,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
              "        19, 20, 22, 23, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38,\n",
              "        39, 40, 41, 43, 44, 45]),)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_punct_indexes = where_tags[0]\n",
        "n_classes = len(no_punct_indexes)"
      ],
      "metadata": {
        "id": "LrzwDP5Wn9TK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "cat_val_tags_y = to_categorical(valid_tags_y, len(tag2index))\n",
        "print(len(cat_train_tags_y), len(cat_val_tags_y))\n",
        "print(len(train_sentences_X), len(valid_sentences_X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik32ea9dn-5m",
        "outputId": "e32efc0d-113b-4e0e-8373-aeb52c4c7462"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1963 1299\n",
            "1963 1299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mini Data Vizualization Inset"
      ],
      "metadata": {
        "id": "YbT_r953oBA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_tags = np.zeros(46)\n",
        "for i in cat_train_tags_y:\n",
        "  for t in i:\n",
        "    distribution_tags += t"
      ],
      "metadata": {
        "id": "YIyRpFGVoKlH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = sum(distribution_tags[1:])\n",
        "norm = [float(i)/s for i in distribution_tags[1:]]"
      ],
      "metadata": {
        "id": "OAbGJViloL0O"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt "
      ],
      "metadata": {
        "id": "sPiOzlwroPuJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(np.arange(len(norm)),norm)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "px2v7JyxoQrz",
        "outputId": "84770783-cf9b-47a0-8653-f50544619822"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#F1 Metric"
      ],
      "metadata": {
        "id": "l0m_PqWUoRsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric.\n",
        "\n",
        "    Only computes a batch-wise average of precision.\n",
        "\n",
        "    Computes the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric.\n",
        "\n",
        "    Only computes a batch-wise average of recall.\n",
        "\n",
        "    Computes the recall, a metric for multi-label classification of\n",
        "    how many relevant items are selected.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def f1_binary(y_true, y_pred):\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
        "\n",
        "def metric_wrapper(index):\n",
        "  def f1_for_class(y_true, y_pred):\n",
        "    #get only the desired class\n",
        "    true = y_true[:,:,index]\n",
        "    pred = y_pred[:,:,index]\n",
        "    #return dice per class\n",
        "    tmp = f1_binary(true,pred)\n",
        "    return tmp\n",
        "  f1_for_class.__name__ = 'f1_' + str(index)\n",
        "  return f1_for_class\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    result = 0.0\n",
        "    for class_id in no_punct_indexes:\n",
        "        y_true_single_class = y_true[:,:,class_id]\n",
        "        y_pred_single_class = y_pred[:,:,class_id]\n",
        "        f1_single = f1_binary(y_true_single_class, y_pred_single_class)\n",
        "        result += f1_single / float(n_classes)\n",
        "    return result"
      ],
      "metadata": {
        "id": "ZDaBrpk3oYXO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model compile and fit"
      ],
      "metadata": {
        "id": "sQz388M4oaCW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsVGByW_sdec",
        "outputId": "074f7d15-3220-4c62-cb4f-3de00439b824"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(embedding_layer)\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(Dense(len(tag2index)))\n",
        "model.add(Dense(len(tag2index)))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=Adam(0.001),\n",
        "             metrics=['accuracy', f1, [metric_wrapper(i) for i in no_punct_indexes]])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 249, 100)          1094900   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 249, 512)         731136    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 249, 46)           23598     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 249, 46)           2162      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,851,796\n",
            "Trainable params: 756,896\n",
            "Non-trainable params: 1,094,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "history = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=40, validation_data=(valid_sentences_X, cat_val_tags_y), callbacks=[callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWIokkJwokrV",
        "outputId": "225369b2-4939-4971-b4b8-633f81250705"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "16/16 [==============================] - 91s 5s/step - loss: 0.8508 - accuracy: 0.8461 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.3549 - val_accuracy: 0.9176 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - 71s 5s/step - loss: 0.3238 - accuracy: 0.9161 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.3019 - val_accuracy: 0.9195 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - 71s 5s/step - loss: 0.2917 - accuracy: 0.9250 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2810 - val_accuracy: 0.9290 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.2724 - accuracy: 0.9309 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2643 - val_accuracy: 0.9365 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.2556 - accuracy: 0.9384 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2480 - val_accuracy: 0.9422 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.2377 - accuracy: 0.9433 - f1: 6.5568e-04 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0262 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2290 - val_accuracy: 0.9455 - val_f1: 8.8479e-04 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0354 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.2172 - accuracy: 0.9472 - f1: 0.0058 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0039 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.2119 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0150 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2082 - val_accuracy: 0.9492 - val_f1: 0.0091 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0055 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.2481 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.1091 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.1952 - accuracy: 0.9513 - f1: 0.0274 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0463 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.4382 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.4488 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.1612 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1864 - val_accuracy: 0.9531 - val_f1: 0.0440 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0033 - val_f1_18: 0.0806 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.4254 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.7890 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0070 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.4545 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - 80s 5s/step - loss: 0.1739 - accuracy: 0.9545 - f1: 0.0569 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0019 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0146 - f1_18: 0.1631 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.5629 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.8481 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0944 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.5894 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1665 - val_accuracy: 0.9557 - val_f1: 0.0669 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0146 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0619 - val_f1_18: 0.2468 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.4900 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.9057 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.3103 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.6480 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.1552 - accuracy: 0.9580 - f1: 0.0827 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.3174 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0024 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.1013 - f1_18: 0.3261 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.6113 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.9056 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.3125 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.7304 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1499 - val_accuracy: 0.9598 - val_f1: 0.1001 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.7637 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 9.6200e-04 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.2106 - val_f1_18: 0.3965 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.5568 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.9114 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.4158 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.7468 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - 71s 5s/step - loss: 0.1391 - accuracy: 0.9628 - f1: 0.1107 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.8985 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0290 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.2358 - f1_18: 0.4405 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.0216 - f1_26: 0.0036 - f1_27: 0.6612 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.9086 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.4348 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.7949 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1360 - val_accuracy: 0.9635 - val_f1: 0.1192 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.9652 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0260 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.3850 - val_f1_18: 0.4851 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.0347 - val_f1_26: 0.0017 - val_f1_27: 0.6238 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.9097 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.5311 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.8076 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.1257 - accuracy: 0.9676 - f1: 0.1332 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.9836 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.1472 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.3996 - f1_18: 0.5330 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.1606 - f1_26: 0.0897 - f1_27: 0.6876 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0000e+00 - f1_32: 0.0370 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.9109 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.5488 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.8298 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1242 - val_accuracy: 0.9677 - val_f1: 0.1489 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.9955 - val_f1_5: 0.0036 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.1423 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.4075 - val_f1_18: 0.5583 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.2631 - val_f1_26: 0.1611 - val_f1_27: 0.6641 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0000e+00 - val_f1_32: 0.4193 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.9104 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.6016 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.8274 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.1146 - accuracy: 0.9712 - f1: 0.1788 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.9952 - f1_5: 0.0594 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.2757 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.4781 - f1_18: 0.5893 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.3562 - f1_26: 0.4593 - f1_27: 0.7159 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_30: 0.0377 - f1_32: 0.7984 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.9112 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.6228 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.8515 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1141 - val_accuracy: 0.9708 - val_f1: 0.1894 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 1.0000 - val_f1_5: 0.0708 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.2868 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.4758 - val_f1_18: 0.6435 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.3568 - val_f1_26: 0.6778 - val_f1_27: 0.6519 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_30: 0.0970 - val_f1_32: 0.8841 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0026 - val_f1_35: 0.9135 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.6710 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.8455 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.1052 - accuracy: 0.9737 - f1: 0.2191 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.9971 - f1_5: 0.2315 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0079 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.3758 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.5562 - f1_18: 0.6406 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.4879 - f1_26: 0.7282 - f1_27: 0.7391 - f1_28: 0.0020 - f1_29: 0.0000e+00 - f1_30: 0.5704 - f1_32: 0.9782 - f1_33: 0.0000e+00 - f1_34: 0.0108 - f1_35: 0.9113 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.6641 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.8618 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1060 - val_accuracy: 0.9731 - val_f1: 0.2293 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 1.0000 - val_f1_5: 0.2540 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.1126 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.3448 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.5708 - val_f1_18: 0.6018 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.5305 - val_f1_26: 0.7440 - val_f1_27: 0.7065 - val_f1_28: 0.0107 - val_f1_29: 0.0000e+00 - val_f1_30: 0.7522 - val_f1_32: 0.9928 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0827 - val_f1_35: 0.9145 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.7065 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.8476 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.0972 - accuracy: 0.9755 - f1: 0.2497 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.9986 - f1_5: 0.4229 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0039 - f1_9: 0.0000e+00 - f1_10: 0.1668 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.4287 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.6097 - f1_18: 0.6625 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.5988 - f1_26: 0.8029 - f1_27: 0.7541 - f1_28: 0.0569 - f1_29: 0.0000e+00 - f1_30: 0.8347 - f1_32: 0.9930 - f1_33: 0.0000e+00 - f1_34: 0.1472 - f1_35: 0.9139 - f1_36: 0.0075 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.7106 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.8767 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.0988 - val_accuracy: 0.9740 - val_f1: 0.2548 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 1.0000 - val_f1_5: 0.3745 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0053 - val_f1_9: 0.0000e+00 - val_f1_10: 0.2072 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.4001 - val_f1_14: 0.0049 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.6144 - val_f1_18: 0.7138 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.5757 - val_f1_26: 0.7721 - val_f1_27: 0.6749 - val_f1_28: 0.1119 - val_f1_29: 0.0000e+00 - val_f1_30: 0.8911 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.3200 - val_f1_35: 0.9158 - val_f1_36: 0.0095 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.7328 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.8707 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - 74s 5s/step - loss: 0.0903 - accuracy: 0.9770 - f1: 0.2755 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.9986 - f1_5: 0.5343 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0215 - f1_9: 0.0000e+00 - f1_10: 0.3853 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.4807 - f1_14: 0.0107 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.6403 - f1_18: 0.6910 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.6475 - f1_26: 0.8228 - f1_27: 0.7705 - f1_28: 0.1688 - f1_29: 0.0000e+00 - f1_30: 0.9219 - f1_32: 0.9940 - f1_33: 0.0000e+00 - f1_34: 0.3628 - f1_35: 0.9190 - f1_36: 0.0270 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.7378 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.8865 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.0924 - val_accuracy: 0.9752 - val_f1: 0.2822 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 1.0000 - val_f1_5: 0.4727 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0073 - val_f1_9: 0.0000e+00 - val_f1_10: 0.4854 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.3983 - val_f1_14: 0.0568 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.6400 - val_f1_18: 0.7179 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.6352 - val_f1_26: 0.7739 - val_f1_27: 0.7302 - val_f1_28: 0.2029 - val_f1_29: 0.0000e+00 - val_f1_30: 0.9296 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.6475 - val_f1_35: 0.9232 - val_f1_36: 0.0335 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.7583 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.8783 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0847 - accuracy: 0.9780 - f1: 0.2984 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.9985 - f1_5: 0.5868 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0320 - f1_9: 0.0000e+00 - f1_10: 0.5706 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.5062 - f1_14: 0.0987 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.6738 - f1_18: 0.7068 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.6912 - f1_26: 0.8302 - f1_27: 0.7788 - f1_28: 0.3019 - f1_29: 0.0000e+00 - f1_30: 0.9230 - f1_32: 0.9921 - f1_33: 0.0000e+00 - f1_34: 0.5900 - f1_35: 0.9244 - f1_36: 0.0837 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.7542 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.8947 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.0872 - val_accuracy: 0.9764 - val_f1: 0.2986 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 1.0000 - val_f1_5: 0.5050 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0543 - val_f1_9: 0.0000e+00 - val_f1_10: 0.6586 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.4582 - val_f1_14: 0.1416 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.6708 - val_f1_18: 0.7386 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.6551 - val_f1_26: 0.7812 - val_f1_27: 0.7097 - val_f1_28: 0.2899 - val_f1_29: 0.0000e+00 - val_f1_30: 0.9333 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.6828 - val_f1_35: 0.9283 - val_f1_36: 0.0917 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.7550 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.8932 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.0794 - accuracy: 0.9791 - f1: 0.3188 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.9986 - f1_5: 0.6317 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0847 - f1_9: 0.0000e+00 - f1_10: 0.7260 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.5473 - f1_14: 0.2965 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.6964 - f1_18: 0.7258 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.7232 - f1_26: 0.8368 - f1_27: 0.7864 - f1_28: 0.3284 - f1_29: 0.0000e+00 - f1_30: 0.9230 - f1_32: 0.9936 - f1_33: 0.0000e+00 - f1_34: 0.6966 - f1_35: 0.9305 - f1_36: 0.1523 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.7669 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.9068 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.0825 - val_accuracy: 0.9773 - val_f1: 0.3180 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 1.0000 - val_f1_5: 0.5741 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0815 - val_f1_9: 0.0000e+00 - val_f1_10: 0.7843 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.4369 - val_f1_14: 0.3032 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.6895 - val_f1_18: 0.7517 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.6923 - val_f1_26: 0.7908 - val_f1_27: 0.7445 - val_f1_28: 0.4546 - val_f1_29: 0.0000e+00 - val_f1_30: 0.9333 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.7330 - val_f1_35: 0.9379 - val_f1_36: 0.1382 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.7778 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.8994 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.0748 - accuracy: 0.9802 - f1: 0.3347 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.9986 - f1_5: 0.6692 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.1461 - f1_9: 0.0000e+00 - f1_10: 0.7872 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.5541 - f1_14: 0.4501 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.7238 - f1_18: 0.7484 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.7562 - f1_26: 0.8461 - f1_27: 0.7987 - f1_28: 0.3948 - f1_29: 0.0000e+00 - f1_30: 0.9291 - f1_32: 0.9905 - f1_33: 0.0000e+00 - f1_34: 0.7387 - f1_35: 0.9395 - f1_36: 0.2165 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.7893 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.9078 - f1_44: 0.0026 - f1_45: 0.0000e+00 - val_loss: 0.0787 - val_accuracy: 0.9784 - val_f1: 0.3312 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 1.0000 - val_f1_5: 0.6309 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0975 - val_f1_9: 0.0000e+00 - val_f1_10: 0.7964 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.4832 - val_f1_14: 0.4152 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7102 - val_f1_18: 0.7054 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.7378 - val_f1_26: 0.7957 - val_f1_27: 0.7724 - val_f1_28: 0.5169 - val_f1_29: 0.0000e+00 - val_f1_30: 0.9280 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.7767 - val_f1_35: 0.9460 - val_f1_36: 0.2327 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.7980 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_43: 0.9023 - val_f1_44: 0.0057 - val_f1_45: 0.0000e+00\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.0711 - accuracy: 0.9811 - f1: 0.3457 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.9985 - f1_5: 0.6836 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.1577 - f1_9: 0.0000e+00 - f1_10: 0.8183 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.5740 - f1_14: 0.5498 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.7407 - f1_18: 0.7539 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.7738 - f1_26: 0.8462 - f1_27: 0.7947 - f1_28: 0.4717 - f1_29: 0.0000e+00 - f1_30: 0.9184 - f1_32: 0.9940 - f1_33: 0.0000e+00 - f1_34: 0.7740 - f1_35: 0.9475 - f1_36: 0.3045 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.7942 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_43: 0.9134 - f1_44: 0.0201 - f1_45: 0.0000e+00 - val_loss: 0.0751 - val_accuracy: 0.9794 - val_f1: 0.3413 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 1.0000 - val_f1_5: 0.6789 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.1662 - val_f1_9: 0.0000e+00 - val_f1_10: 0.8282 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.5126 - val_f1_14: 0.4737 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7179 - val_f1_18: 0.7228 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.7450 - val_f1_26: 0.8062 - val_f1_27: 0.7768 - val_f1_28: 0.5384 - val_f1_29: 0.0000e+00 - val_f1_30: 0.9286 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.7878 - val_f1_35: 0.9521 - val_f1_36: 0.2759 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.8055 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0148 - val_f1_43: 0.9054 - val_f1_44: 0.0187 - val_f1_45: 0.0000e+00\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0674 - accuracy: 0.9820 - f1: 0.3591 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.9986 - f1_5: 0.7149 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.2240 - f1_9: 0.0000e+00 - f1_10: 0.8212 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.5919 - f1_14: 0.6189 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.7601 - f1_18: 0.7620 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.7885 - f1_26: 0.8542 - f1_27: 0.8184 - f1_28: 0.5427 - f1_29: 0.0000e+00 - f1_30: 0.9324 - f1_32: 0.9948 - f1_33: 0.0000e+00 - f1_34: 0.8043 - f1_35: 0.9507 - f1_36: 0.3290 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.8142 - f1_40: 0.0000e+00 - f1_41: 0.0973 - f1_43: 0.9169 - f1_44: 0.0288 - f1_45: 0.0000e+00 - val_loss: 0.0718 - val_accuracy: 0.9800 - val_f1: 0.3502 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 1.0000 - val_f1_5: 0.6812 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.1561 - val_f1_9: 0.0000e+00 - val_f1_10: 0.8511 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.5298 - val_f1_14: 0.5910 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.6888 - val_f1_18: 0.7823 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.7615 - val_f1_26: 0.8135 - val_f1_27: 0.7700 - val_f1_28: 0.5433 - val_f1_29: 0.0000e+00 - val_f1_30: 0.9288 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8189 - val_f1_35: 0.9561 - val_f1_36: 0.2766 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.8126 - val_f1_40: 0.0000e+00 - val_f1_41: 0.1126 - val_f1_43: 0.9115 - val_f1_44: 0.0248 - val_f1_45: 0.0000e+00\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0642 - accuracy: 0.9828 - f1: 0.3722 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.9986 - f1_5: 0.7463 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.2917 - f1_9: 0.0000e+00 - f1_10: 0.8439 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.5990 - f1_14: 0.7178 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.7655 - f1_18: 0.7758 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.8064 - f1_26: 0.8678 - f1_27: 0.8139 - f1_28: 0.5357 - f1_29: 0.0000e+00 - f1_30: 0.9298 - f1_32: 0.9957 - f1_33: 0.0000e+00 - f1_34: 0.8293 - f1_35: 0.9568 - f1_36: 0.3857 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.8241 - f1_40: 0.0000e+00 - f1_41: 0.2303 - f1_43: 0.9213 - f1_44: 0.0535 - f1_45: 0.0000e+00 - val_loss: 0.0689 - val_accuracy: 0.9808 - val_f1: 0.3665 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 1.0000 - val_f1_5: 0.7062 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.1804 - val_f1_9: 0.0000e+00 - val_f1_10: 0.8567 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.5451 - val_f1_14: 0.6430 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7144 - val_f1_18: 0.7906 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.7691 - val_f1_26: 0.8271 - val_f1_27: 0.7720 - val_f1_28: 0.5721 - val_f1_29: 0.0000e+00 - val_f1_30: 0.9304 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8372 - val_f1_35: 0.9603 - val_f1_36: 0.3684 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.8264 - val_f1_40: 0.0000e+00 - val_f1_41: 0.3973 - val_f1_43: 0.9113 - val_f1_44: 0.0535 - val_f1_45: 0.0000e+00\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0617 - accuracy: 0.9836 - f1: 0.3893 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.9987 - f1_5: 0.7471 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.3314 - f1_9: 0.0000e+00 - f1_10: 0.8532 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.6180 - f1_14: 0.7754 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.7730 - f1_18: 0.7767 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.8122 - f1_26: 0.8853 - f1_27: 0.8170 - f1_28: 0.6001 - f1_29: 0.0000e+00 - f1_30: 0.9185 - f1_32: 0.9926 - f1_33: 0.0000e+00 - f1_34: 0.8574 - f1_35: 0.9599 - f1_36: 0.4443 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.8329 - f1_40: 0.0000e+00 - f1_41: 0.5591 - f1_43: 0.9252 - f1_44: 0.0928 - f1_45: 0.0000e+00 - val_loss: 0.0661 - val_accuracy: 0.9818 - val_f1: 0.3757 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 1.0000 - val_f1_5: 0.7089 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.2998 - val_f1_9: 0.0000e+00 - val_f1_10: 0.8611 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.5972 - val_f1_14: 0.6318 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7419 - val_f1_18: 0.7766 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.7900 - val_f1_26: 0.8294 - val_f1_27: 0.7738 - val_f1_28: 0.5987 - val_f1_29: 0.0000e+00 - val_f1_30: 0.9305 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8324 - val_f1_35: 0.9623 - val_f1_36: 0.4144 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.8454 - val_f1_40: 0.0000e+00 - val_f1_41: 0.3812 - val_f1_43: 0.9201 - val_f1_44: 0.1343 - val_f1_45: 0.0000e+00\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.0589 - accuracy: 0.9843 - f1: 0.4009 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.9986 - f1_5: 0.7592 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.3714 - f1_9: 0.0000e+00 - f1_10: 0.8657 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.6330 - f1_14: 0.7972 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.7931 - f1_18: 0.7962 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.8248 - f1_26: 0.8939 - f1_27: 0.8271 - f1_28: 0.6151 - f1_29: 0.0000e+00 - f1_30: 0.9228 - f1_32: 0.9946 - f1_33: 0.0000e+00 - f1_34: 0.8633 - f1_35: 0.9623 - f1_36: 0.4789 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.8472 - f1_40: 0.0000e+00 - f1_41: 0.7000 - f1_43: 0.9295 - f1_44: 0.1637 - f1_45: 0.0000e+00 - val_loss: 0.0643 - val_accuracy: 0.9824 - val_f1: 0.3928 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 1.0000 - val_f1_5: 0.7366 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.4256 - val_f1_9: 0.0000e+00 - val_f1_10: 0.8763 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.5933 - val_f1_14: 0.7025 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7432 - val_f1_18: 0.7638 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.7821 - val_f1_26: 0.8695 - val_f1_27: 0.7919 - val_f1_28: 0.6285 - val_f1_29: 0.0000e+00 - val_f1_30: 0.9253 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8536 - val_f1_35: 0.9693 - val_f1_36: 0.4394 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.8317 - val_f1_40: 0.0000e+00 - val_f1_41: 0.6510 - val_f1_43: 0.9171 - val_f1_44: 0.2121 - val_f1_45: 0.0000e+00\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - 74s 5s/step - loss: 0.0566 - accuracy: 0.9850 - f1: 0.4117 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.9985 - f1_5: 0.7784 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.4373 - f1_9: 0.0000e+00 - f1_10: 0.8769 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.6524 - f1_14: 0.8231 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.8061 - f1_18: 0.8020 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.8244 - f1_26: 0.9065 - f1_27: 0.8335 - f1_28: 0.6159 - f1_29: 0.0000e+00 - f1_30: 0.9333 - f1_32: 0.9942 - f1_33: 0.0000e+00 - f1_34: 0.8748 - f1_35: 0.9636 - f1_36: 0.4914 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.8443 - f1_40: 0.0000e+00 - f1_41: 0.8320 - f1_43: 0.9301 - f1_44: 0.2205 - f1_45: 0.0274 - val_loss: 0.0619 - val_accuracy: 0.9830 - val_f1: 0.4019 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 1.0000 - val_f1_5: 0.7306 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.4385 - val_f1_9: 0.0000e+00 - val_f1_10: 0.8886 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6038 - val_f1_14: 0.7311 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7492 - val_f1_18: 0.7791 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.8093 - val_f1_26: 0.9020 - val_f1_27: 0.7959 - val_f1_28: 0.6309 - val_f1_29: 0.0000e+00 - val_f1_30: 0.9388 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8884 - val_f1_35: 0.9713 - val_f1_36: 0.4901 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.8548 - val_f1_40: 0.0000e+00 - val_f1_41: 0.7070 - val_f1_43: 0.9202 - val_f1_44: 0.2469 - val_f1_45: 0.0000e+00\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.0544 - accuracy: 0.9855 - f1: 0.4187 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.9986 - f1_5: 0.7770 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.4490 - f1_9: 0.0000e+00 - f1_10: 0.8903 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.6624 - f1_14: 0.8325 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.8067 - f1_18: 0.8064 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.8336 - f1_26: 0.9198 - f1_27: 0.8435 - f1_28: 0.6502 - f1_29: 0.0000e+00 - f1_30: 0.9439 - f1_32: 0.9938 - f1_33: 0.0000e+00 - f1_34: 0.8956 - f1_35: 0.9656 - f1_36: 0.5333 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.8602 - f1_40: 0.0000e+00 - f1_41: 0.8530 - f1_43: 0.9332 - f1_44: 0.2714 - f1_45: 0.0288 - val_loss: 0.0599 - val_accuracy: 0.9835 - val_f1: 0.4099 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 1.0000 - val_f1_5: 0.7414 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.5052 - val_f1_9: 0.0000e+00 - val_f1_10: 0.8866 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6026 - val_f1_14: 0.7726 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7630 - val_f1_18: 0.7962 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.8016 - val_f1_26: 0.9227 - val_f1_27: 0.7968 - val_f1_28: 0.6178 - val_f1_29: 0.0000e+00 - val_f1_30: 0.9546 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8814 - val_f1_35: 0.9718 - val_f1_36: 0.5073 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.8658 - val_f1_40: 0.0000e+00 - val_f1_41: 0.8616 - val_f1_43: 0.9206 - val_f1_44: 0.2281 - val_f1_45: 0.0000e+00\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.0525 - accuracy: 0.9861 - f1: 0.4293 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.9986 - f1_5: 0.7856 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.5057 - f1_9: 0.0000e+00 - f1_10: 0.9024 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.6769 - f1_14: 0.8548 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.8265 - f1_18: 0.8120 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.8387 - f1_26: 0.9463 - f1_27: 0.8445 - f1_28: 0.6617 - f1_29: 0.0000e+00 - f1_30: 0.9539 - f1_32: 0.9944 - f1_33: 0.0000e+00 - f1_34: 0.9059 - f1_35: 0.9684 - f1_36: 0.5652 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.8670 - f1_40: 0.0000e+00 - f1_41: 0.8981 - f1_43: 0.9363 - f1_44: 0.3417 - f1_45: 0.0883 - val_loss: 0.0588 - val_accuracy: 0.9838 - val_f1: 0.4175 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 1.0000 - val_f1_5: 0.7725 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.4827 - val_f1_9: 0.0000e+00 - val_f1_10: 0.8943 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.5859 - val_f1_14: 0.8361 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7785 - val_f1_18: 0.7835 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.8140 - val_f1_26: 0.9262 - val_f1_27: 0.8043 - val_f1_28: 0.6624 - val_f1_29: 0.0000e+00 - val_f1_30: 0.9663 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9043 - val_f1_35: 0.9733 - val_f1_36: 0.4920 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.8585 - val_f1_40: 0.0000e+00 - val_f1_41: 0.8884 - val_f1_43: 0.9199 - val_f1_44: 0.2928 - val_f1_45: 0.0678\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - 71s 5s/step - loss: 0.0512 - accuracy: 0.9862 - f1: 0.4365 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.9985 - f1_5: 0.8006 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.5526 - f1_9: 0.0000e+00 - f1_10: 0.9115 - f1_11: 0.0179 - f1_12: 0.0000e+00 - f1_13: 0.6817 - f1_14: 0.8652 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.8291 - f1_18: 0.8104 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.8429 - f1_26: 0.9501 - f1_27: 0.8338 - f1_28: 0.6795 - f1_29: 0.0000e+00 - f1_30: 0.9628 - f1_32: 0.9934 - f1_33: 0.0000e+00 - f1_34: 0.9258 - f1_35: 0.9688 - f1_36: 0.5687 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.8751 - f1_40: 0.0000e+00 - f1_41: 0.8981 - f1_43: 0.9378 - f1_44: 0.4054 - f1_45: 0.1491 - val_loss: 0.0583 - val_accuracy: 0.9837 - val_f1: 0.4204 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 1.0000 - val_f1_5: 0.7680 - val_f1_6: 0.0364 - val_f1_7: 0.0000e+00 - val_f1_8: 0.5061 - val_f1_9: 0.0000e+00 - val_f1_10: 0.8957 - val_f1_11: 0.0182 - val_f1_12: 0.0000e+00 - val_f1_13: 0.5508 - val_f1_14: 0.8080 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7795 - val_f1_18: 0.7755 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.8007 - val_f1_26: 0.9346 - val_f1_27: 0.8021 - val_f1_28: 0.6491 - val_f1_29: 0.0000e+00 - val_f1_30: 0.9663 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9069 - val_f1_35: 0.9742 - val_f1_36: 0.5611 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.8649 - val_f1_40: 0.0000e+00 - val_f1_41: 0.8907 - val_f1_43: 0.9270 - val_f1_44: 0.3702 - val_f1_45: 0.0332\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.0494 - accuracy: 0.9868 - f1: 0.4421 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.9985 - f1_5: 0.8139 - f1_6: 0.0179 - f1_7: 0.0000e+00 - f1_8: 0.5445 - f1_9: 0.0000e+00 - f1_10: 0.9108 - f1_11: 0.0445 - f1_12: 0.0000e+00 - f1_13: 0.6998 - f1_14: 0.8851 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.8331 - f1_18: 0.8183 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.8495 - f1_26: 0.9564 - f1_27: 0.8490 - f1_28: 0.6926 - f1_29: 0.0000e+00 - f1_30: 0.9630 - f1_32: 0.9941 - f1_33: 0.0000e+00 - f1_34: 0.9315 - f1_35: 0.9704 - f1_36: 0.5893 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.8847 - f1_40: 0.0000e+00 - f1_41: 0.9236 - f1_43: 0.9418 - f1_44: 0.4149 - f1_45: 0.1567 - val_loss: 0.0558 - val_accuracy: 0.9845 - val_f1: 0.4314 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 1.0000 - val_f1_5: 0.7832 - val_f1_6: 0.1198 - val_f1_7: 0.0000e+00 - val_f1_8: 0.4437 - val_f1_9: 0.0000e+00 - val_f1_10: 0.8973 - val_f1_11: 0.0939 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6219 - val_f1_14: 0.8851 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7803 - val_f1_18: 0.8105 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.8331 - val_f1_26: 0.9349 - val_f1_27: 0.8050 - val_f1_28: 0.6514 - val_f1_29: 0.0000e+00 - val_f1_30: 0.9663 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9133 - val_f1_35: 0.9732 - val_f1_36: 0.5434 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.8671 - val_f1_40: 0.0000e+00 - val_f1_41: 0.8963 - val_f1_43: 0.9261 - val_f1_44: 0.3808 - val_f1_45: 0.1306\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.0477 - accuracy: 0.9873 - f1: 0.4560 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.9984 - f1_5: 0.8216 - f1_6: 0.1697 - f1_7: 0.0000e+00 - f1_8: 0.5728 - f1_9: 0.0000e+00 - f1_10: 0.9160 - f1_11: 0.0942 - f1_12: 0.0000e+00 - f1_13: 0.7030 - f1_14: 0.9070 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.8404 - f1_18: 0.8213 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.8547 - f1_26: 0.9652 - f1_27: 0.8539 - f1_28: 0.6913 - f1_29: 0.0000e+00 - f1_30: 0.9715 - f1_32: 0.9950 - f1_33: 0.0000e+00 - f1_34: 0.9329 - f1_35: 0.9704 - f1_36: 0.5979 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.8800 - f1_40: 0.0000e+00 - f1_41: 0.9363 - f1_43: 0.9432 - f1_44: 0.4752 - f1_45: 0.3294 - val_loss: 0.0543 - val_accuracy: 0.9850 - val_f1: 0.4403 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0057 - val_f1_3: 1.0000 - val_f1_5: 0.7627 - val_f1_6: 0.1788 - val_f1_7: 0.0000e+00 - val_f1_8: 0.5225 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9012 - val_f1_11: 0.1156 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6381 - val_f1_14: 0.8874 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.8063 - val_f1_18: 0.8165 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.8145 - val_f1_26: 0.9586 - val_f1_27: 0.8052 - val_f1_28: 0.6830 - val_f1_29: 0.0000e+00 - val_f1_30: 0.9663 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9140 - val_f1_35: 0.9736 - val_f1_36: 0.5649 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.8718 - val_f1_40: 0.0000e+00 - val_f1_41: 0.8976 - val_f1_43: 0.9279 - val_f1_44: 0.4263 - val_f1_45: 0.1778\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0462 - accuracy: 0.9876 - f1: 0.4674 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.9986 - f1_5: 0.8180 - f1_6: 0.2630 - f1_7: 0.0000e+00 - f1_8: 0.5994 - f1_9: 0.0000e+00 - f1_10: 0.9131 - f1_11: 0.1404 - f1_12: 0.0000e+00 - f1_13: 0.7164 - f1_14: 0.9213 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.8528 - f1_18: 0.8316 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.8594 - f1_26: 0.9698 - f1_27: 0.8580 - f1_28: 0.6983 - f1_29: 0.0000e+00 - f1_30: 0.9820 - f1_32: 0.9943 - f1_33: 0.0000e+00 - f1_34: 0.9413 - f1_35: 0.9709 - f1_36: 0.6163 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.8959 - f1_40: 0.0000e+00 - f1_41: 0.9399 - f1_43: 0.9467 - f1_44: 0.4778 - f1_45: 0.4908 - val_loss: 0.0532 - val_accuracy: 0.9855 - val_f1: 0.4573 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 1.0000 - val_f1_5: 0.7882 - val_f1_6: 0.3746 - val_f1_7: 0.0000e+00 - val_f1_8: 0.5421 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9034 - val_f1_11: 0.1416 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6418 - val_f1_14: 0.9323 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7889 - val_f1_18: 0.8161 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.8252 - val_f1_26: 0.9519 - val_f1_27: 0.8145 - val_f1_28: 0.6687 - val_f1_29: 0.0000e+00 - val_f1_30: 0.9772 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9241 - val_f1_35: 0.9752 - val_f1_36: 0.5967 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.8737 - val_f1_40: 0.0000e+00 - val_f1_41: 0.9202 - val_f1_43: 0.9250 - val_f1_44: 0.4258 - val_f1_45: 0.4862\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0449 - accuracy: 0.9880 - f1: 0.4786 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.9986 - f1_5: 0.8351 - f1_6: 0.5054 - f1_7: 0.0000e+00 - f1_8: 0.6129 - f1_9: 0.0000e+00 - f1_10: 0.9166 - f1_11: 0.1839 - f1_12: 0.0000e+00 - f1_13: 0.7248 - f1_14: 0.9153 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.8517 - f1_18: 0.8376 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.8575 - f1_26: 0.9721 - f1_27: 0.8614 - f1_28: 0.7151 - f1_29: 0.0000e+00 - f1_30: 0.9857 - f1_32: 0.9744 - f1_33: 0.0000e+00 - f1_34: 0.9444 - f1_35: 0.9740 - f1_36: 0.6439 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.8885 - f1_40: 0.0000e+00 - f1_41: 0.9504 - f1_43: 0.9452 - f1_44: 0.5142 - f1_45: 0.5368 - val_loss: 0.0521 - val_accuracy: 0.9857 - val_f1: 0.4629 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 1.0000 - val_f1_5: 0.7985 - val_f1_6: 0.2736 - val_f1_7: 0.0000e+00 - val_f1_8: 0.5472 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9053 - val_f1_11: 0.1156 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6250 - val_f1_14: 0.9373 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.8152 - val_f1_18: 0.8171 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.8170 - val_f1_26: 0.9644 - val_f1_27: 0.8168 - val_f1_28: 0.6666 - val_f1_29: 0.0000e+00 - val_f1_30: 0.9867 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9217 - val_f1_35: 0.9755 - val_f1_36: 0.6105 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.8876 - val_f1_40: 0.0000e+00 - val_f1_41: 0.9155 - val_f1_43: 0.9332 - val_f1_44: 0.5361 - val_f1_45: 0.6500\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - 74s 5s/step - loss: 0.0438 - accuracy: 0.9883 - f1: 0.4869 - f1_1: 0.0000e+00 - f1_2: 0.0074 - f1_3: 0.9986 - f1_5: 0.8370 - f1_6: 0.5124 - f1_7: 0.0000e+00 - f1_8: 0.6502 - f1_9: 0.0000e+00 - f1_10: 0.9244 - f1_11: 0.1785 - f1_12: 0.0000e+00 - f1_13: 0.7308 - f1_14: 0.9313 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.8596 - f1_18: 0.8376 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.8675 - f1_26: 0.9763 - f1_27: 0.8612 - f1_28: 0.7374 - f1_29: 0.0000e+00 - f1_30: 0.9827 - f1_32: 0.9942 - f1_33: 0.0000e+00 - f1_34: 0.9479 - f1_35: 0.9740 - f1_36: 0.6496 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.8961 - f1_40: 0.0000e+00 - f1_41: 0.9521 - f1_43: 0.9488 - f1_44: 0.5301 - f1_45: 0.6916 - val_loss: 0.0536 - val_accuracy: 0.9848 - val_f1: 0.4647 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 1.0000 - val_f1_5: 0.7985 - val_f1_6: 0.5419 - val_f1_7: 0.0000e+00 - val_f1_8: 0.5522 - val_f1_9: 0.0000e+00 - val_f1_10: 0.8985 - val_f1_11: 0.1682 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6097 - val_f1_14: 0.9038 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.8077 - val_f1_18: 0.7864 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.8327 - val_f1_26: 0.9558 - val_f1_27: 0.7978 - val_f1_28: 0.6936 - val_f1_29: 0.0000e+00 - val_f1_30: 0.9804 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9291 - val_f1_35: 0.9765 - val_f1_36: 0.5597 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.8569 - val_f1_40: 0.0000e+00 - val_f1_41: 0.9400 - val_f1_43: 0.9319 - val_f1_44: 0.5002 - val_f1_45: 0.5711\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.0431 - accuracy: 0.9883 - f1: 0.4917 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.9986 - f1_5: 0.8410 - f1_6: 0.5544 - f1_7: 0.0000e+00 - f1_8: 0.6593 - f1_9: 0.0000e+00 - f1_10: 0.9326 - f1_11: 0.2115 - f1_12: 0.0000e+00 - f1_13: 0.7180 - f1_14: 0.9352 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.8607 - f1_18: 0.8325 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.8750 - f1_26: 0.9767 - f1_27: 0.8618 - f1_28: 0.7483 - f1_29: 0.0000e+00 - f1_30: 0.9835 - f1_32: 0.9891 - f1_33: 0.0000e+00 - f1_34: 0.9477 - f1_35: 0.9741 - f1_36: 0.6618 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.8942 - f1_40: 0.0000e+00 - f1_41: 0.9491 - f1_43: 0.9518 - f1_44: 0.5544 - f1_45: 0.7571 - val_loss: 0.0532 - val_accuracy: 0.9847 - val_f1: 0.4701 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 1.0000 - val_f1_5: 0.8012 - val_f1_6: 0.5715 - val_f1_7: 0.0000e+00 - val_f1_8: 0.5705 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9034 - val_f1_11: 0.1876 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6033 - val_f1_14: 0.9028 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.8209 - val_f1_18: 0.7576 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.8380 - val_f1_26: 0.9497 - val_f1_27: 0.7913 - val_f1_28: 0.6887 - val_f1_29: 0.0000e+00 - val_f1_30: 0.9710 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9281 - val_f1_35: 0.9780 - val_f1_36: 0.6260 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.8843 - val_f1_40: 0.0000e+00 - val_f1_41: 0.9656 - val_f1_43: 0.9330 - val_f1_44: 0.4679 - val_f1_45: 0.6669\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.0425 - accuracy: 0.9885 - f1: 0.5019 - f1_1: 0.0000e+00 - f1_2: 0.0431 - f1_3: 0.9987 - f1_5: 0.8442 - f1_6: 0.6867 - f1_7: 0.0250 - f1_8: 0.6720 - f1_9: 0.0000e+00 - f1_10: 0.9293 - f1_11: 0.3006 - f1_12: 0.0000e+00 - f1_13: 0.7380 - f1_14: 0.9398 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.8651 - f1_18: 0.8395 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.8772 - f1_26: 0.9830 - f1_27: 0.8490 - f1_28: 0.7310 - f1_29: 0.0000e+00 - f1_30: 0.9850 - f1_32: 0.9943 - f1_33: 0.0000e+00 - f1_34: 0.9470 - f1_35: 0.9747 - f1_36: 0.6601 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9007 - f1_40: 0.0000e+00 - f1_41: 0.9576 - f1_43: 0.9537 - f1_44: 0.5880 - f1_45: 0.7935 - val_loss: 0.0501 - val_accuracy: 0.9861 - val_f1: 0.4804 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 1.0000 - val_f1_5: 0.8120 - val_f1_6: 0.5401 - val_f1_7: 0.0000e+00 - val_f1_8: 0.6214 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9110 - val_f1_11: 0.1876 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6643 - val_f1_14: 0.9246 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.8265 - val_f1_18: 0.7862 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.8393 - val_f1_26: 0.9640 - val_f1_27: 0.8229 - val_f1_28: 0.7080 - val_f1_29: 0.0000e+00 - val_f1_30: 0.9891 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9328 - val_f1_35: 0.9767 - val_f1_36: 0.6374 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.8884 - val_f1_40: 0.0000e+00 - val_f1_41: 0.9482 - val_f1_43: 0.9374 - val_f1_44: 0.5830 - val_f1_45: 0.7177\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - 74s 5s/step - loss: 0.0412 - accuracy: 0.9888 - f1: 0.5070 - f1_1: 0.0000e+00 - f1_2: 0.0234 - f1_3: 0.9986 - f1_5: 0.8563 - f1_6: 0.7173 - f1_7: 0.0458 - f1_8: 0.6819 - f1_9: 0.0000e+00 - f1_10: 0.9295 - f1_11: 0.3403 - f1_12: 0.0000e+00 - f1_13: 0.7443 - f1_14: 0.9443 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.8609 - f1_18: 0.8330 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.8861 - f1_26: 0.9818 - f1_27: 0.8656 - f1_28: 0.7338 - f1_29: 0.0000e+00 - f1_30: 0.9876 - f1_32: 0.9938 - f1_33: 0.0000e+00 - f1_34: 0.9582 - f1_35: 0.9762 - f1_36: 0.6892 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9056 - f1_40: 0.0000e+00 - f1_41: 0.9709 - f1_43: 0.9533 - f1_44: 0.5980 - f1_45: 0.8037 - val_loss: 0.0498 - val_accuracy: 0.9857 - val_f1: 0.4799 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0057 - val_f1_3: 1.0000 - val_f1_5: 0.8043 - val_f1_6: 0.6403 - val_f1_7: 0.0000e+00 - val_f1_8: 0.5569 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9034 - val_f1_11: 0.2201 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6464 - val_f1_14: 0.9296 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7785 - val_f1_18: 0.8306 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.8381 - val_f1_26: 0.9637 - val_f1_27: 0.8154 - val_f1_28: 0.6853 - val_f1_29: 0.0000e+00 - val_f1_30: 0.9826 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9324 - val_f1_35: 0.9788 - val_f1_36: 0.6219 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.8942 - val_f1_40: 0.0000e+00 - val_f1_41: 0.9590 - val_f1_43: 0.9392 - val_f1_44: 0.5212 - val_f1_45: 0.7496\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.0397 - accuracy: 0.9893 - f1: 0.5082 - f1_1: 0.0000e+00 - f1_2: 0.0264 - f1_3: 0.9985 - f1_5: 0.8532 - f1_6: 0.6762 - f1_7: 0.0000e+00 - f1_8: 0.6875 - f1_9: 0.0000e+00 - f1_10: 0.9325 - f1_11: 0.3465 - f1_12: 0.0000e+00 - f1_13: 0.7490 - f1_14: 0.9575 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.8667 - f1_18: 0.8486 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.8884 - f1_26: 0.9849 - f1_27: 0.8749 - f1_28: 0.7498 - f1_29: 0.0000e+00 - f1_30: 0.9949 - f1_32: 0.9930 - f1_33: 0.0000e+00 - f1_34: 0.9567 - f1_35: 0.9759 - f1_36: 0.6923 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9112 - f1_40: 0.0000e+00 - f1_41: 0.9702 - f1_43: 0.9550 - f1_44: 0.5906 - f1_45: 0.8489 - val_loss: 0.0482 - val_accuracy: 0.9865 - val_f1: 0.4909 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0208 - val_f1_3: 1.0000 - val_f1_5: 0.8026 - val_f1_6: 0.6100 - val_f1_7: 0.0000e+00 - val_f1_8: 0.6051 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9195 - val_f1_11: 0.2365 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6885 - val_f1_14: 0.9484 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.8451 - val_f1_18: 0.8203 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.8475 - val_f1_26: 0.9821 - val_f1_27: 0.8116 - val_f1_28: 0.7420 - val_f1_29: 0.0000e+00 - val_f1_30: 0.9921 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9341 - val_f1_35: 0.9789 - val_f1_36: 0.6296 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.8883 - val_f1_40: 0.0000e+00 - val_f1_41: 0.9474 - val_f1_43: 0.9402 - val_f1_44: 0.6084 - val_f1_45: 0.8403\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.0388 - accuracy: 0.9895 - f1: 0.5150 - f1_1: 0.0000e+00 - f1_2: 0.0227 - f1_3: 0.9984 - f1_5: 0.8619 - f1_6: 0.6944 - f1_7: 0.0281 - f1_8: 0.7020 - f1_9: 0.0000e+00 - f1_10: 0.9324 - f1_11: 0.4479 - f1_12: 0.0000e+00 - f1_13: 0.7613 - f1_14: 0.9440 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.8773 - f1_18: 0.8530 - f1_19: 0.0000e+00 - f1_20: 0.0179 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.8862 - f1_26: 0.9850 - f1_27: 0.8738 - f1_28: 0.7573 - f1_29: 0.0000e+00 - f1_30: 0.9911 - f1_32: 0.9958 - f1_33: 0.0000e+00 - f1_34: 0.9571 - f1_35: 0.9771 - f1_36: 0.6931 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9169 - f1_40: 0.0000e+00 - f1_41: 0.9671 - f1_43: 0.9547 - f1_44: 0.6269 - f1_45: 0.8780 - val_loss: 0.0473 - val_accuracy: 0.9867 - val_f1: 0.4894 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0057 - val_f1_3: 1.0000 - val_f1_5: 0.8106 - val_f1_6: 0.6219 - val_f1_7: 0.0000e+00 - val_f1_8: 0.6442 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9189 - val_f1_11: 0.2365 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6519 - val_f1_14: 0.9444 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.8455 - val_f1_18: 0.8250 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.8570 - val_f1_26: 0.9646 - val_f1_27: 0.8235 - val_f1_28: 0.7281 - val_f1_29: 0.0000e+00 - val_f1_30: 0.9921 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9357 - val_f1_35: 0.9788 - val_f1_36: 0.6472 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.8981 - val_f1_40: 0.0000e+00 - val_f1_41: 0.9614 - val_f1_43: 0.9421 - val_f1_44: 0.5531 - val_f1_45: 0.7910\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - 73s 5s/step - loss: 0.0379 - accuracy: 0.9896 - f1: 0.5193 - f1_1: 0.0000e+00 - f1_2: 0.0742 - f1_3: 0.9987 - f1_5: 0.8579 - f1_6: 0.7930 - f1_7: 0.0486 - f1_8: 0.7140 - f1_9: 0.0000e+00 - f1_10: 0.9377 - f1_11: 0.4338 - f1_12: 0.0000e+00 - f1_13: 0.7546 - f1_14: 0.9608 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.8805 - f1_18: 0.8545 - f1_19: 0.0000e+00 - f1_20: 0.0469 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.8994 - f1_26: 0.9861 - f1_27: 0.8788 - f1_28: 0.7486 - f1_29: 0.0000e+00 - f1_30: 0.9875 - f1_32: 0.9951 - f1_33: 0.0000e+00 - f1_34: 0.9563 - f1_35: 0.9778 - f1_36: 0.6972 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9199 - f1_40: 0.0000e+00 - f1_41: 0.9730 - f1_43: 0.9556 - f1_44: 0.6284 - f1_45: 0.8123 - val_loss: 0.0471 - val_accuracy: 0.9865 - val_f1: 0.4914 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0057 - val_f1_3: 1.0000 - val_f1_5: 0.8044 - val_f1_6: 0.6403 - val_f1_7: 0.0000e+00 - val_f1_8: 0.6156 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9156 - val_f1_11: 0.2305 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6585 - val_f1_14: 0.9401 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.8413 - val_f1_18: 0.8319 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.8483 - val_f1_26: 0.9802 - val_f1_27: 0.8258 - val_f1_28: 0.7476 - val_f1_29: 0.0000e+00 - val_f1_30: 0.9939 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9363 - val_f1_35: 0.9778 - val_f1_36: 0.6182 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.8924 - val_f1_40: 0.0000e+00 - val_f1_41: 0.9705 - val_f1_43: 0.9429 - val_f1_44: 0.6232 - val_f1_45: 0.8179\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - 72s 5s/step - loss: 0.0370 - accuracy: 0.9899 - f1: 0.5241 - f1_1: 0.0000e+00 - f1_2: 0.0498 - f1_3: 0.9985 - f1_5: 0.8628 - f1_6: 0.7597 - f1_7: 0.0880 - f1_8: 0.7246 - f1_9: 0.0000e+00 - f1_10: 0.9395 - f1_11: 0.4764 - f1_12: 0.0000e+00 - f1_13: 0.7687 - f1_14: 0.9538 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.8832 - f1_18: 0.8576 - f1_19: 0.0000e+00 - f1_20: 0.0521 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_25: 0.8994 - f1_26: 0.9847 - f1_27: 0.8806 - f1_28: 0.7709 - f1_29: 0.0156 - f1_30: 0.9920 - f1_32: 0.9944 - f1_33: 0.0000e+00 - f1_34: 0.9609 - f1_35: 0.9787 - f1_36: 0.7005 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.9186 - f1_40: 0.0000e+00 - f1_41: 0.9797 - f1_43: 0.9551 - f1_44: 0.6439 - f1_45: 0.8738 - val_loss: 0.0462 - val_accuracy: 0.9870 - val_f1: 0.4979 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0330 - val_f1_3: 1.0000 - val_f1_5: 0.8284 - val_f1_6: 0.6563 - val_f1_7: 0.0000e+00 - val_f1_8: 0.6807 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9145 - val_f1_11: 0.2467 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6971 - val_f1_14: 0.9510 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.8393 - val_f1_18: 0.8248 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_25: 0.8440 - val_f1_26: 0.9846 - val_f1_27: 0.8288 - val_f1_28: 0.7377 - val_f1_29: 0.0182 - val_f1_30: 0.9910 - val_f1_32: 0.9975 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9423 - val_f1_35: 0.9810 - val_f1_36: 0.6243 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.8957 - val_f1_40: 0.0000e+00 - val_f1_41: 0.9766 - val_f1_43: 0.9406 - val_f1_44: 0.6258 - val_f1_45: 0.8581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#History f1 for class"
      ],
      "metadata": {
        "id": "Q_XNg1aXonOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_val_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_val_list[i] = history.history['val_f1_{}'.format(i)][-1]\n",
        "f1_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_list[i] = history.history['f1_{}'.format(i)][-1]"
      ],
      "metadata": {
        "id": "DIeuPw8AorGd"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index2tag = {value : key for (key, value) in tag2index.items()}\n",
        "index2tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlQ8zRSMou1-",
        "outputId": "548a3194-8ca7-4521-89e6-c94ae3854517"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '-PAD-',\n",
              " 1: 'RBS',\n",
              " 2: 'JJR',\n",
              " 3: 'TO',\n",
              " 4: '.',\n",
              " 5: 'VBD',\n",
              " 6: 'WP',\n",
              " 7: 'RBR',\n",
              " 8: 'RB',\n",
              " 9: 'JJS',\n",
              " 10: 'VBZ',\n",
              " 11: 'RP',\n",
              " 12: 'WP$',\n",
              " 13: 'JJ',\n",
              " 14: 'MD',\n",
              " 15: '#',\n",
              " 16: 'NNPS',\n",
              " 17: 'NNS',\n",
              " 18: 'NN',\n",
              " 19: 'UH',\n",
              " 20: 'EX',\n",
              " 21: ':',\n",
              " 22: 'SYM',\n",
              " 23: 'PDT',\n",
              " 24: ',',\n",
              " 25: 'VB',\n",
              " 26: 'CC',\n",
              " 27: 'NNP',\n",
              " 28: 'VBP',\n",
              " 29: 'WRB',\n",
              " 30: 'POS',\n",
              " 31: '``',\n",
              " 32: '$',\n",
              " 33: '-RRB-',\n",
              " 34: 'PRP',\n",
              " 35: 'DT',\n",
              " 36: 'VBN',\n",
              " 37: 'FW',\n",
              " 38: '-LRB-',\n",
              " 39: 'CD',\n",
              " 40: 'LS',\n",
              " 41: 'PRP$',\n",
              " 42: \"''\",\n",
              " 43: 'IN',\n",
              " 44: 'VBG',\n",
              " 45: 'WDT'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- F1: {}'.format(index2tag[i], f1_list[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKsg3Ui-owlf",
        "outputId": "8667fdfe-ca59-4e94-c023-61b60b14e8dc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: RBS --- F1: 0.0\n",
            "Tag: JJR --- F1: 0.049825169146060944\n",
            "Tag: TO --- F1: 0.9985227584838867\n",
            "Tag: VBD --- F1: 0.8628365993499756\n",
            "Tag: WP --- F1: 0.7596665024757385\n",
            "Tag: RBR --- F1: 0.08799602836370468\n",
            "Tag: RB --- F1: 0.7246120572090149\n",
            "Tag: JJS --- F1: 0.0\n",
            "Tag: VBZ --- F1: 0.9394926428794861\n",
            "Tag: RP --- F1: 0.4763749837875366\n",
            "Tag: WP$ --- F1: 0.0\n",
            "Tag: JJ --- F1: 0.768677830696106\n",
            "Tag: MD --- F1: 0.9538385272026062\n",
            "Tag: # --- F1: 0.0\n",
            "Tag: NNPS --- F1: 0.0\n",
            "Tag: NNS --- F1: 0.8832181096076965\n",
            "Tag: NN --- F1: 0.8576164245605469\n",
            "Tag: UH --- F1: 0.0\n",
            "Tag: EX --- F1: 0.0520833283662796\n",
            "Tag: SYM --- F1: 0.0\n",
            "Tag: PDT --- F1: 0.0\n",
            "Tag: VB --- F1: 0.899385929107666\n",
            "Tag: CC --- F1: 0.9847109317779541\n",
            "Tag: NNP --- F1: 0.8805789351463318\n",
            "Tag: VBP --- F1: 0.7709144353866577\n",
            "Tag: WRB --- F1: 0.01562499813735485\n",
            "Tag: POS --- F1: 0.991973340511322\n",
            "Tag: $ --- F1: 0.9943763017654419\n",
            "Tag: -RRB- --- F1: 0.0\n",
            "Tag: PRP --- F1: 0.9608741998672485\n",
            "Tag: DT --- F1: 0.9787354469299316\n",
            "Tag: VBN --- F1: 0.7005327939987183\n",
            "Tag: FW --- F1: 0.0\n",
            "Tag: -LRB- --- F1: 0.0\n",
            "Tag: CD --- F1: 0.9186475872993469\n",
            "Tag: LS --- F1: 0.0\n",
            "Tag: PRP$ --- F1: 0.9797199368476868\n",
            "Tag: IN --- F1: 0.9551076292991638\n",
            "Tag: VBG --- F1: 0.6438502669334412\n",
            "Tag: WDT --- F1: 0.8738144636154175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- Val_F1: {}'.format(index2tag[i], f1_val_list[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9wM9cNloyzc",
        "outputId": "618d87e4-619d-466e-ef9a-59797c77766f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: RBS --- Val_F1: 0.0\n",
            "Tag: JJR --- Val_F1: 0.03295454382896423\n",
            "Tag: TO --- Val_F1: 1.0\n",
            "Tag: VBD --- Val_F1: 0.8283770084381104\n",
            "Tag: WP --- Val_F1: 0.6562771201133728\n",
            "Tag: RBR --- Val_F1: 0.0\n",
            "Tag: RB --- Val_F1: 0.6806783676147461\n",
            "Tag: JJS --- Val_F1: 0.0\n",
            "Tag: VBZ --- Val_F1: 0.9145107269287109\n",
            "Tag: RP --- Val_F1: 0.2466941922903061\n",
            "Tag: WP$ --- Val_F1: 0.0\n",
            "Tag: JJ --- Val_F1: 0.697091817855835\n",
            "Tag: MD --- Val_F1: 0.9509781002998352\n",
            "Tag: # --- Val_F1: 0.0\n",
            "Tag: NNPS --- Val_F1: 0.0\n",
            "Tag: NNS --- Val_F1: 0.839275598526001\n",
            "Tag: NN --- Val_F1: 0.8248283863067627\n",
            "Tag: UH --- Val_F1: 0.0\n",
            "Tag: EX --- Val_F1: 0.0\n",
            "Tag: SYM --- Val_F1: 0.0\n",
            "Tag: PDT --- Val_F1: 0.0\n",
            "Tag: VB --- Val_F1: 0.844004213809967\n",
            "Tag: CC --- Val_F1: 0.984649121761322\n",
            "Tag: NNP --- Val_F1: 0.8287506103515625\n",
            "Tag: VBP --- Val_F1: 0.7376811504364014\n",
            "Tag: WRB --- Val_F1: 0.0181818138808012\n",
            "Tag: POS --- Val_F1: 0.9909806847572327\n",
            "Tag: $ --- Val_F1: 0.9975429773330688\n",
            "Tag: -RRB- --- Val_F1: 0.0\n",
            "Tag: PRP --- Val_F1: 0.9422664642333984\n",
            "Tag: DT --- Val_F1: 0.9809825420379639\n",
            "Tag: VBN --- Val_F1: 0.6242933869361877\n",
            "Tag: FW --- Val_F1: 0.0\n",
            "Tag: -LRB- --- Val_F1: 0.0\n",
            "Tag: CD --- Val_F1: 0.8957306146621704\n",
            "Tag: LS --- Val_F1: 0.0\n",
            "Tag: PRP$ --- Val_F1: 0.9766481518745422\n",
            "Tag: IN --- Val_F1: 0.9406396746635437\n",
            "Tag: VBG --- Val_F1: 0.6258406043052673\n",
            "Tag: WDT --- Val_F1: 0.8581095337867737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Infos"
      ],
      "metadata": {
        "id": "1uv6mtuMo1Mh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#40 epochs, batch 128\n",
        "loss: 0.0370 - accuracy: 0.9899 - f1: 0.5241"
      ],
      "metadata": {
        "id": "Jot_8JtVo37_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_without_point = []\n",
        "\n",
        "for i in test:\n",
        "  if i[0] != '':\n",
        "    if i[1] != '.' and i[1] != ',' and i[1] != '``' and i[1] != \"''\":\n",
        "      test_without_point += [i]\n",
        "  else: test_without_point += ''\n",
        "\n",
        "#print(test_without_point)\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in test_without_point:\n",
        "  if i != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    test_sentences.append(s)\n",
        "    test_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "test_sentences_X,  test_tags_y = [], []\n",
        "\n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        "\n",
        "for s in test_tags:\n",
        "    test_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ],
      "metadata": {
        "id": "w9Y9N9RYoBNi"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import metrics \n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "y_val_pred = model.predict(test_sentences_X)\n",
        "\n",
        "for i in y_val_pred[0]:\n",
        "  i[np.amax(i)] = 1\n",
        "\n",
        "\n",
        "\n",
        "#f1_test = f1_score(test_tags_y, y_val_pred)\n",
        "#y_val_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFug9E5DoKWe",
        "outputId": "7211a14d-7e2d-4e63-c5c8-75820823c157"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8755726\n",
            "0.5822762\n",
            "0.9584586\n",
            "0.9932225\n",
            "0.9628572\n",
            "0.80483025\n",
            "0.9893037\n",
            "0.9252997\n",
            "0.9951781\n",
            "0.976174\n",
            "0.9216657\n",
            "0.53675264\n",
            "0.9236918\n",
            "0.99536127\n",
            "0.91346306\n",
            "0.9861491\n",
            "0.9159826\n",
            "0.75003856\n",
            "0.9977968\n",
            "0.9378483\n",
            "0.981656\n",
            "0.99700135\n",
            "0.85315394\n",
            "0.98625714\n",
            "0.9027753\n",
            "0.9968907\n",
            "0.98983943\n",
            "0.28140402\n",
            "0.94260186\n",
            "0.9932556\n",
            "0.9831476\n",
            "0.79242456\n",
            "0.9963541\n",
            "0.8921662\n",
            "0.9829884\n",
            "0.9155885\n",
            "0.98661476\n",
            "0.9979456\n",
            "0.977192\n",
            "0.9991805\n",
            "0.99223775\n",
            "0.9982601\n",
            "0.9993666\n",
            "0.9996822\n",
            "0.9998067\n",
            "0.9998708\n",
            "0.9999105\n",
            "0.99993587\n",
            "0.9999516\n",
            "0.99996173\n",
            "0.9999689\n",
            "0.99997425\n",
            "0.9999782\n",
            "0.99998105\n",
            "0.9999832\n",
            "0.9999846\n",
            "0.9999858\n",
            "0.99998665\n",
            "0.99998724\n",
            "0.9999877\n",
            "0.9999882\n",
            "0.99998844\n",
            "0.9999889\n",
            "0.9999896\n",
            "0.9999902\n",
            "0.9999907\n",
            "0.9999912\n",
            "0.99999154\n",
            "0.9999918\n",
            "0.999992\n",
            "0.99999225\n",
            "0.9999925\n",
            "0.9999925\n",
            "0.9999925\n",
            "0.9999926\n",
            "0.9999927\n",
            "0.9999927\n",
            "0.9999927\n",
            "0.9999927\n",
            "0.9999927\n",
            "0.9999927\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999285\n",
            "0.99999297\n",
            "0.99999297\n",
            "0.99999297\n",
            "0.99999297\n",
            "0.99999297\n",
            "0.99999297\n",
            "0.9999931\n",
            "0.9999931\n",
            "0.9999931\n",
            "0.9999931\n",
            "0.9999932\n",
            "0.9999933\n",
            "0.99999344\n",
            "0.99999344\n",
            "0.9999937\n",
            "0.9999937\n",
            "0.9999939\n",
            "0.9999939\n",
            "0.99999416\n",
            "0.9999944\n",
            "0.9999945\n",
            "0.99999475\n",
            "0.99999475\n",
            "0.9999944\n",
            "0.9999937\n",
            "0.9999912\n",
            "0.99998105\n",
            "0.9999199\n",
            "0.99946743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_pred[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxp2JTef7p9o",
        "outputId": "38bb5240-9f00-4402-b6ca-c44b66f87f58"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(249, 46)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    }
  ]
}