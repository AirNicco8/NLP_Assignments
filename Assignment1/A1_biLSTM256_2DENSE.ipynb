{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_biLSTM256_2DENSE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Due to**: 23/12/2021 (dd/mm/yyyy)\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Part-of Speech (POS) tagging as Sequence Labelling using Recurrent Neural Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4_wqPdlBcKS"
      },
      "source": [
        "# Intro\n",
        "\n",
        "In this assignment  we will ask you to perform POS tagging using neural architectures\n",
        "\n",
        "You are asked to follow these steps:\n",
        "*   Download the corpora and split it in training and test sets, structuring a dataframe.\n",
        "*   Embed the words using GloVe embeddings\n",
        "*   Create a baseline model, using a simple neural architecture\n",
        "*   Experiment doing small modifications to the baseline model, choose hyperparameters using the validation set\n",
        "*   Evaluate your two best model\n",
        "*   Analyze the errors of your model\n",
        "\n",
        "\n",
        "**Task**: given a corpus of documents, predict the POS tag for each word\n",
        "\n",
        "**Corpus**:\n",
        "Ignore the numeric value in the third column, use only the words/symbols and its label. \n",
        "The corpus is available at:\n",
        "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
        "\n",
        "**Splits**: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
        "\n",
        "\n",
        "**Features**: you MUST use GloVe embeddings as the only input features to the model.\n",
        "\n",
        "**Splitting**: you can decide to split documents into sentences or not, the choice is yours.\n",
        "\n",
        "**I/O structure**: The input data will have three dimensions: 1-documents/sentences, 2-token, 3-features; for the output there are 2 possibilities: if you use one-hot encoding it will be 1-documents/sentences, 2-token labels, 3-classes, if you use a single integer that indicates the number of the class it will be 1-documents/sentences, 2-token labels.\n",
        "\n",
        "**Baseline**: two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top; the choice of hyper-parameters is yours.\n",
        "\n",
        "**Architectures**: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and adding an additional dense layer; do not mix these variantions.\n",
        "\n",
        "\n",
        "**Training and Experiments**: all the experiments must involve only the training and validation sets.\n",
        "\n",
        "**Evaluation**: in the end, only the two best models of your choice (according to the validation set) must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech. DO NOT CONSIDER THE PUNCTUATION CLASSES.\n",
        "\n",
        "**Metrics**: the metric you must use to evaluate your final model is the F1-macro, WITHOUT considering punctuation/symbols classes; during the training process you can use accuracy because you can't use the F1 metric unless you use a single (gigantic) batch because there is no way to aggregate \"partial\" F1 scores computed on mini-batches.\n",
        "\n",
        "**Discussion and Error Analysis** : verify and discuss if the results on the test sets are coherent with those on the validation set; analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
        "\n",
        "**Report**: you are asked to deliver the code of your experiments and a small pdf report of about 2 pages; the pdf must begin with the names of the people of your team and a small abstract (4-5 lines) that sums up your findings.\n",
        "\n",
        "# Out Of Vocabulary (OOV) terms\n",
        "\n",
        "How to handle words that are not in GloVe vocabulary?\n",
        "You can handle them as you want (random embedding, placeholder, whatever!), but they must be STATIC embeddings (you cannot train them).\n",
        "\n",
        "But there is a very important caveat! As usual, the element of the test set must not influence the elements of the other splits!\n",
        "\n",
        "So, when you compute new embeddings for train+validation, you must forget about test documents.\n",
        "The motivation is to emulate a real-world scenario, where you select and train a model in the first stage, without knowing nothing about the testing environment.\n",
        "\n",
        "For implementation convenience, you CAN use a single vocabulary file/matrix/whatever. The principle of the previous point is that the embeddings inside that file/matrix must be generated independently for train and test splits.\n",
        "\n",
        "Basically in a real-world scenario, this is what would happen:\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Training of the model(s)\n",
        "5. Compute embeddings for terms OOV2 of the validation split \n",
        "6. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "7. Validation of the model(s)\n",
        "8. Compute embeddings for terms OOV3 of the test split \n",
        "9. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2+OOV3\n",
        "10. Testing of the final model\n",
        "\n",
        "In this case, where we already have all the documents, we can simplify the process a bit, but the procedure must remain rigorous.\n",
        "\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Compute embeddings for terms OOV2 of the validation split \n",
        "5. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "6. Compute embeddings for terms OOV3 of the test split \n",
        "7. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2\n",
        "8. Training of the model(s)\n",
        "9. Validation of the model(s)\n",
        "10. Testing of the final model\n",
        "\n",
        "Step 2 and step 6 must be completely independent of each other, for what concerns the method and the documents. But they can rely on the previous vocabulary (V1 for step 2 and V3 for step 6)\n",
        "THEREFORE if a word is present both in the training set and the test split and not in the starting vocabulary, its embedding is computed in step 2) and it is not considered OOV anymore in step 6).\n",
        "\n",
        "# Report\n",
        "The report must not be just a copy and paste of graphs and tables!\n",
        "\n",
        "The report must not be longer than 2 pages and must contain:\n",
        "* The names of the member of your team\n",
        "* A short abstract (4-5 lines) that sum ups everything\n",
        "* A general description of the task you have addressed and how you have addressed it\n",
        "* A short description of the models you have used\n",
        "* Some tables that sum up your findings in validation and test and a discussion of those results\n",
        "* The most relevant findings of your error analysis\n",
        "\n",
        "# Evaluation Criterion\n",
        "\n",
        "The goal of this assignment is not to prove you can find best model ever, but to face a common task, structure it correctly, and follow a correct and rigorous experimental procedure.\n",
        "In other words, we don't care if you final models are awful as long as you have followed the correct procedure and wrote a decent report.\n",
        "\n",
        "The score of the assignment will be computed roughly as follows\n",
        "* 1 point for the general setting of the problem\n",
        "* 1 point for the handling of OOV terms\n",
        "* 1 point for the models\n",
        "* 1 point for train-validation-test procedure\n",
        "* 2 point for the discussion of the results, error analysis, and report\n",
        "\n",
        "This distribution of scores is tentative and we may decide to alter it at any moment.\n",
        "We also reserve the right to assign a small bonus (0.5 points) to any assignment that is particularly worthy. Similarly, in case of grave errors, we may decide to assign an equivalent malus (-0.5 points).\n",
        "\n",
        "# Contacts\n",
        "\n",
        "In case of any doubt, question, issue, or help we highly recommend you to check the [course useful material](https://virtuale.unibo.it/pluginfile.php/1036039/mod_resource/content/2/NLP_Course_Useful_Material.pdf) for additional information, and to use the Virtuale forums to discuss with other students.\n",
        "\n",
        "You can always contact us at the following email addresses. To increase the probability of a prompt response, we reccomend you to write to both the teaching assistants.\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Andrea Galassi -> a.galassi@unibo.it\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it\n",
        "\n",
        "\n",
        "# FAQ\n",
        "* You can use a non-trainable Embedding layer to load the glove embeddings\n",
        "* You can use any library of your choice to implement the networks. Two options are tensorflow/keras or pythorch. Both these libraries have all the classes you need to implement these simple architectures and there are plenty of tutorials around, where you can learn how to use them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emdubW66fnlD"
      },
      "source": [
        "import os, shutil  #  file management\n",
        "import sys \n",
        "import pandas as pd  #  dataframe management\n",
        "import numpy as np  #  data manipulation\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request  #  download files\n",
        "import zipfile  #  unzip files\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B-w2pqyfpF1"
      },
      "source": [
        "from keras import metrics \n",
        "from tensorflow.python.keras.metrics import Metric\n",
        "from sklearn.metrics import f1_score\n",
        "import keras.backend as K"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFvajV2sni19",
        "outputId": "05ce4809-05b8-4325-931b-4e57c4e83f45"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3xn1MZPgA_v",
        "outputId": "434d5400-d1cd-48b3-c05c-476f8905a990"
      },
      "source": [
        "dataset_folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\")\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "    os.makedirs(dataset_folder)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
        "\n",
        "dataset_path = os.path.join(dataset_folder, \"treebank.zip\")\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    urllib.request.urlretrieve(url, dataset_path)\n",
        "    print(\"Successful download\")\n",
        "\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_folder)\n",
        "print(\"Successful extraction\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful extraction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre Processing\n",
        "- splitting"
      ],
      "metadata": {
        "id": "FiO1v6SJmm37"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKbUsrVHhGAG"
      },
      "source": [
        "dataset_name = \"dependency_treebank\"\n",
        "\n",
        "\n",
        "folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\", dataset_name)\n",
        "\n",
        "pre_train = []\n",
        "pre_valid = []\n",
        "pre_test = []\n",
        "i = 1\n",
        "\n",
        "file_list = sorted(os.listdir(folder))\n",
        "\n",
        "for filename in file_list:\n",
        "  file_path = os.path.join(folder, filename)\n",
        "  if os.path.isfile(file_path):\n",
        "    # open the file\n",
        "      text = []\n",
        "      with open(file_path, mode='r', encoding='utf-8') as text_file:\n",
        "        text = text_file.read()\n",
        "        if i <= 100:\n",
        "          pre_train.append(text)\n",
        "        elif i <= 150:\n",
        "          pre_valid.append(text)\n",
        "        else:\n",
        "          pre_test.append(text)  \n",
        "  i+=1\n",
        "\n",
        "tr = []\n",
        "val = []\n",
        "tes = []\n",
        "\n",
        "for paragraph in pre_train:\n",
        "   tr.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_valid:\n",
        "   val.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_test:\n",
        "   tes.append(paragraph.split('\\n'))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUgbTXi-F7Sa"
      },
      "source": [
        "train = []\n",
        "valid = []\n",
        "test = []\n",
        "\n",
        "for i in tr:\n",
        "  for j in i:\n",
        "    train.append(j.split('\\t'))\n",
        "\n",
        "for i in val:\n",
        "  for j in i:\n",
        "    valid.append(j.split('\\t'))\n",
        "\n",
        "for i in tes:\n",
        "  for j in i:\n",
        "    test.append(j.split('\\t'))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmzVaRuQFp0M"
      },
      "source": [
        "train_sentences = []\n",
        "valid_sentences = []\n",
        "test_sentences = []\n",
        "train_tags = []\n",
        "valid_tags = []\n",
        "test_tags = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in train:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    train_sentences.append(s)\n",
        "    train_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in valid:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    valid_sentences.append(s)\n",
        "    valid_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in test:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    test_sentences.append(s)\n",
        "    test_tags.append(t)\n",
        "    s = []\n",
        "    t = []"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB65cUFLIeEr"
      },
      "source": [
        "flat_train = [item for sublist in train_sentences for item in sublist]\n",
        "flat_valid = [item for sublist in valid_sentences for item in sublist]\n",
        "flat_test = [item for sublist in test_sentences for item in sublist]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "- vocabularies\n",
        "- GloVe download"
      ],
      "metadata": {
        "id": "d4IenRMamtLg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZO6ga1EGHdh"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "train_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "train_tokenizer.fit_on_texts(train_sentences)                    # fit tokeniser on data\n",
        "train_encoded = train_tokenizer.texts_to_sequences(train_sentences)\n",
        "\n",
        "valid_tokenizer = Tokenizer()       \n",
        "valid_tokenizer.fit_on_texts(valid_sentences)                  # instantiate tokeniser\n",
        "valid_encoded = valid_tokenizer.texts_to_sequences(valid_sentences)\n",
        "\n",
        "test_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "test_tokenizer.fit_on_texts(test_sentences)                    # fit tokeniser on data\n",
        "test_encoded = test_tokenizer.texts_to_sequences(test_sentences)  # use the tokeniser to encode input sequence"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(train_tags)\n",
        "tags_encoded = tag_tokenizer.texts_to_sequences(train_tags)"
      ],
      "metadata": {
        "id": "MSvhz4IvmziU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMsG0Yw1l50L",
        "outputId": "036df626-8e6b-46a2-ca3f-24b618622493"
      },
      "source": [
        "voc = list(set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(valid_tokenizer.word_index.keys()) - set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(test_tokenizer.word_index.keys()) - set(voc))\n",
        "print(len(voc))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7404\n",
            "9901\n",
            "10947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS9RwnOtJuas"
      },
      "source": [
        "word_index = dict(zip(voc, range(2, len(voc)+3)))\n",
        "word_index['-PAD-'] = 0\n",
        "word_index['-OOV-'] = 1\n",
        "\n",
        "tags = set([item for sublist in train_tags for item in sublist])\n",
        "\n",
        "tag2index = {t: i + 1 for i, t in enumerate(tags)}\n",
        "tag2index['-PAD-'] = 0 "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoj4Zx51J2MU",
        "outputId": "2a0b5110-7675-4776-c2d1-0d6d09ff2fbd"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-16 11:21:57--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-12-16 11:21:58--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-12-16 11:21:58--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  5.00MB/s    in 2m 41s  \n",
            "\n",
            "2021-12-16 11:24:40 (5.10 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc-T6FcLKtS9",
        "outputId": "042654e6-0014-4e87-9e62-0f8b36df7183"
      },
      "source": [
        "path_to_glove_file = os.path.join(os.getcwd(), \"glove.6B.100d.txt\")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding\n",
        "- matrix creation\n",
        "- padding\n",
        "- one hot encoding of tags\n",
        "- punctuation identification"
      ],
      "metadata": {
        "id": "E2IN3Mh-m85T"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0mOStEPMeXt",
        "outputId": "a8c6cfe8-183a-43ba-b50d-8263f6e56baa"
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        embedding_matrix += np.random.uniform(low=-0.05, high=0.05, size=embedding_dim)\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 10271 words (678 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpYrHs0RMtQJ"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8EZM31Js06-",
        "outputId": "0e769bc5-2729-4051-ba5b-6282e05b2bcb"
      },
      "source": [
        "MAX_LENGTH = len(max(train_sentences, key=len))\n",
        "print(MAX_LENGTH)\n",
        "MAX_LENGTH2 = len(max(valid_sentences, key=len))\n",
        "print(MAX_LENGTH2)\n",
        "MAX_LENGTH3 = len(max(test_sentences, key=len))\n",
        "print(MAX_LENGTH3)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "249\n",
            "81\n",
            "58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSRa9SwTuNrX"
      },
      "source": [
        "train_sentences_X, valid_sentences_X, train_tags_y, valid_tags_y, = [], [], [], []\n",
        " \n",
        "for s in train_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        "\n",
        "for s in valid_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    valid_sentences_X.append(s_int)\n",
        "\n",
        "\n",
        "for s in train_tags:\n",
        "    train_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "for s in valid_tags:\n",
        "    valid_tags_y.append([tag2index[t] for t in s])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf-XlSrStQx3"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_sentences_X = pad_sequences(valid_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_tags_y = pad_sequences(valid_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)"
      ],
      "metadata": {
        "id": "nUtevCQrnskt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "point = [tag2index['.']]\n",
        "virg = [tag2index[',']]\n",
        "weird_apex = [tag2index['``']]\n",
        "single_apex = [tag2index[\"''\"]]\n",
        "two_dots = [tag2index[':']]\n",
        "pad = [tag2index['-PAD-']]\n",
        "\n",
        "punct_cat_classes = to_categorical([point, virg, weird_apex, single_apex, two_dots, pad], len(tag2index))\n",
        "punct_cat_classes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT8PjDIynuHG",
        "outputId": "9afecef5-ffb4-483a-d4d1-dd780fe2692f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 1, 46)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cum_tags = np.zeros(46)\n",
        "for i in punct_cat_classes:\n",
        "  print(i[0])\n",
        "  cum_tags += i[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAqZ0Xkrnw_d",
        "outputId": "a6cf3d4a-3641-42d2-a18b-06b1dd2c3934"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "where_tags = np.where(np.logical_not(cum_tags))\n",
        "where_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzJNfc1xn7jA",
        "outputId": "03996f7c-298c-439f-c3b3-8ea714b62b81"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 12, 13, 14, 15, 16, 18, 19,\n",
              "        20, 21, 22, 23, 24, 25, 26, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38,\n",
              "        39, 41, 42, 43, 44, 45]),)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_punct_indexes = where_tags[0]\n",
        "n_classes = len(no_punct_indexes)"
      ],
      "metadata": {
        "id": "LrzwDP5Wn9TK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "cat_val_tags_y = to_categorical(valid_tags_y, len(tag2index))\n",
        "print(len(cat_train_tags_y), len(cat_val_tags_y))\n",
        "print(len(train_sentences_X), len(valid_sentences_X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik32ea9dn-5m",
        "outputId": "292544a6-e745-4e48-b05f-97f7f9cf4b9c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1963 1299\n",
            "1963 1299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mini Data Vizualization Inset"
      ],
      "metadata": {
        "id": "YbT_r953oBA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_tags = np.zeros(46)\n",
        "for i in cat_train_tags_y:\n",
        "  for t in i:\n",
        "    distribution_tags += t"
      ],
      "metadata": {
        "id": "YIyRpFGVoKlH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = sum(distribution_tags[1:])\n",
        "norm = [float(i)/s for i in distribution_tags[1:]]"
      ],
      "metadata": {
        "id": "OAbGJViloL0O"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt "
      ],
      "metadata": {
        "id": "sPiOzlwroPuJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(np.arange(len(norm)),norm)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "px2v7JyxoQrz",
        "outputId": "081fa555-6980-4674-c470-c7bacdf55ca6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#F1 Metric"
      ],
      "metadata": {
        "id": "l0m_PqWUoRsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric.\n",
        "\n",
        "    Only computes a batch-wise average of precision.\n",
        "\n",
        "    Computes the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric.\n",
        "\n",
        "    Only computes a batch-wise average of recall.\n",
        "\n",
        "    Computes the recall, a metric for multi-label classification of\n",
        "    how many relevant items are selected.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def f1_binary(y_true, y_pred):\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
        "\n",
        "def metric_wrapper(index):\n",
        "  def f1_for_class(y_true, y_pred):\n",
        "    #get only the desired class\n",
        "    true = y_true[:,:,index]\n",
        "    pred = y_pred[:,:,index]\n",
        "    #return dice per class\n",
        "    tmp = f1_binary(true,pred)\n",
        "    return tmp\n",
        "  f1_for_class.__name__ = 'f1_' + str(index)\n",
        "  return f1_for_class\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    result = 0.0\n",
        "    for class_id in no_punct_indexes:\n",
        "        y_true_single_class = y_true[:,:,class_id]\n",
        "        y_pred_single_class = y_pred[:,:,class_id]\n",
        "        f1_single = f1_binary(y_true_single_class, y_pred_single_class)\n",
        "        result += f1_single / float(n_classes)\n",
        "    return result"
      ],
      "metadata": {
        "id": "ZDaBrpk3oYXO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model compile and fit"
      ],
      "metadata": {
        "id": "sQz388M4oaCW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsVGByW_sdec",
        "outputId": "0f549467-a677-47b7-d15d-7edd177a8bf9"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(embedding_layer)\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(Dense(len(tag2index)))\n",
        "model.add(Dense(len(tag2index)))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=Adam(0.001),\n",
        "             metrics=['accuracy', f1, [metric_wrapper(i) for i in no_punct_indexes]])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 249, 100)          1094900   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 249, 512)         731136    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 249, 46)           23598     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 249, 46)           2162      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,851,796\n",
            "Trainable params: 756,896\n",
            "Non-trainable params: 1,094,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "history = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=40, validation_data=(valid_sentences_X, cat_val_tags_y), callbacks=[callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWIokkJwokrV",
        "outputId": "67a92bc0-1cfb-419a-cee7-a8aa8b7cbc3e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "16/16 [==============================] - 102s 6s/step - loss: 0.7091 - accuracy: 0.8473 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.3432 - val_accuracy: 0.9150 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.3172 - accuracy: 0.9142 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2987 - val_accuracy: 0.9181 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - 86s 5s/step - loss: 0.2893 - accuracy: 0.9255 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2787 - val_accuracy: 0.9292 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.2694 - accuracy: 0.9319 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2610 - val_accuracy: 0.9335 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.2516 - accuracy: 0.9356 - f1: 5.5669e-04 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0223 - f1_39: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2441 - val_accuracy: 0.9383 - val_f1: 7.2431e-04 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0290 - val_f1_39: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.2332 - accuracy: 0.9410 - f1: 0.0055 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.2207 - f1_39: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2248 - val_accuracy: 0.9447 - val_f1: 0.0045 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.1797 - val_f1_39: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - 78s 5s/step - loss: 0.2127 - accuracy: 0.9467 - f1: 0.0121 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0265 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0570 - f1_26: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.4018 - f1_39: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2034 - val_accuracy: 0.9478 - val_f1: 0.0190 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.1996 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.1814 - val_f1_26: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0040 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.3740 - val_f1_39: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - 80s 5s/step - loss: 0.1906 - accuracy: 0.9508 - f1: 0.0364 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0196 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.5421 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.3565 - f1_26: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0289 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.5090 - f1_39: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1821 - val_accuracy: 0.9522 - val_f1: 0.0527 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.1282 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.8673 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.5208 - val_f1_26: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.1120 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.4787 - val_f1_39: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.1695 - accuracy: 0.9563 - f1: 0.0603 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.1879 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0032 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.8831 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.6109 - f1_26: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.1523 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.5751 - f1_39: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1630 - val_accuracy: 0.9588 - val_f1: 0.0663 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.3239 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0090 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.8887 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.6825 - val_f1_26: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.2136 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0093 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.5251 - val_f1_39: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.1510 - accuracy: 0.9615 - f1: 0.0756 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0231 - f1_7: 0.0000e+00 - f1_8: 0.3262 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0455 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.8988 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.7396 - f1_26: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.3311 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0330 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.6260 - f1_39: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1458 - val_accuracy: 0.9625 - val_f1: 0.0829 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0930 - val_f1_7: 0.0000e+00 - val_f1_8: 0.4065 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0751 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9099 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.7471 - val_f1_26: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.4527 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0385 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.5936 - val_f1_39: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.1350 - accuracy: 0.9652 - f1: 0.1037 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.3057 - f1_7: 0.0000e+00 - f1_8: 0.4226 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.1762 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9091 - f1_23: 0.0054 - f1_24: 0.0000e+00 - f1_25: 0.8167 - f1_26: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.4682 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.1468 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.6656 - f1_39: 0.2280 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0035 - val_loss: 0.1317 - val_accuracy: 0.9652 - val_f1: 0.1207 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.4536 - val_f1_7: 0.0000e+00 - val_f1_8: 0.4839 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.1481 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9155 - val_f1_23: 0.0668 - val_f1_24: 0.0000e+00 - val_f1_25: 0.8036 - val_f1_26: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.6028 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.1544 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.6046 - val_f1_39: 0.5888 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0057\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.1217 - accuracy: 0.9686 - f1: 0.1466 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.6623 - f1_7: 0.0228 - f1_8: 0.5384 - f1_9: 0.0070 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0012 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.2837 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9157 - f1_23: 0.1185 - f1_24: 0.0000e+00 - f1_25: 0.8529 - f1_26: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.5614 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.3039 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.6846 - f1_39: 0.8014 - f1_41: 0.0000e+00 - f1_42: 0.0815 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0267 - val_loss: 0.1196 - val_accuracy: 0.9690 - val_f1: 0.1665 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0087 - val_f1_5: 0.0000e+00 - val_f1_6: 0.7038 - val_f1_7: 0.0751 - val_f1_8: 0.5957 - val_f1_9: 0.0092 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.3203 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9214 - val_f1_23: 0.2486 - val_f1_24: 0.0000e+00 - val_f1_25: 0.8528 - val_f1_26: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.5940 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.3878 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.6216 - val_f1_39: 0.9600 - val_f1_41: 0.0000e+00 - val_f1_42: 0.3349 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0249\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.1104 - accuracy: 0.9717 - f1: 0.1974 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0896 - f1_5: 0.0000e+00 - f1_6: 0.7958 - f1_7: 0.3108 - f1_8: 0.6206 - f1_9: 0.1018 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0118 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.3771 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9218 - f1_23: 0.3409 - f1_24: 0.0000e+00 - f1_25: 0.8696 - f1_26: 0.0000e+00 - f1_28: 0.0257 - f1_29: 0.5991 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.4573 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.7102 - f1_39: 0.9757 - f1_41: 0.0000e+00 - f1_42: 0.5676 - f1_43: 0.0000e+00 - f1_44: 0.0017 - f1_45: 0.1178 - val_loss: 0.1095 - val_accuracy: 0.9718 - val_f1: 0.2228 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.1818 - val_f1_5: 0.0000e+00 - val_f1_6: 0.7556 - val_f1_7: 0.5686 - val_f1_8: 0.6601 - val_f1_9: 0.2075 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0139 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.3841 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9287 - val_f1_23: 0.4530 - val_f1_24: 0.0000e+00 - val_f1_25: 0.8687 - val_f1_26: 0.0000e+00 - val_f1_28: 0.2286 - val_f1_29: 0.6126 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.4586 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.6522 - val_f1_39: 0.9891 - val_f1_41: 0.0000e+00 - val_f1_42: 0.7903 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0060 - val_f1_45: 0.1535\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.1009 - accuracy: 0.9741 - f1: 0.2471 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.2989 - f1_5: 0.0000e+00 - f1_6: 0.8233 - f1_7: 0.6081 - f1_8: 0.6681 - f1_9: 0.2850 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0335 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.4243 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9258 - f1_23: 0.4672 - f1_24: 0.0000e+00 - f1_25: 0.8829 - f1_26: 0.0000e+00 - f1_28: 0.4097 - f1_29: 0.6339 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.5394 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.7275 - f1_39: 0.9902 - f1_41: 0.0000e+00 - f1_42: 0.8349 - f1_43: 0.0000e+00 - f1_44: 0.0276 - f1_45: 0.3021 - val_loss: 0.1008 - val_accuracy: 0.9733 - val_f1: 0.2622 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.3414 - val_f1_5: 0.0000e+00 - val_f1_6: 0.7745 - val_f1_7: 0.7106 - val_f1_8: 0.7059 - val_f1_9: 0.4872 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0388 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.3886 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9328 - val_f1_23: 0.5880 - val_f1_24: 0.0000e+00 - val_f1_25: 0.8782 - val_f1_26: 0.0000e+00 - val_f1_28: 0.5784 - val_f1_29: 0.6698 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.5047 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.6715 - val_f1_39: 0.9957 - val_f1_41: 0.0000e+00 - val_f1_42: 0.8575 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0193 - val_f1_45: 0.3443\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0927 - accuracy: 0.9764 - f1: 0.2845 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.4441 - f1_5: 0.0000e+00 - f1_6: 0.8245 - f1_7: 0.7301 - f1_8: 0.7088 - f1_9: 0.4901 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0741 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.4703 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9302 - f1_23: 0.5941 - f1_24: 0.0000e+00 - f1_25: 0.8909 - f1_26: 0.0000e+00 - f1_28: 0.7734 - f1_29: 0.6654 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.5993 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.7350 - f1_39: 0.9952 - f1_41: 0.0000e+00 - f1_42: 0.9332 - f1_43: 0.0000e+00 - f1_44: 0.0681 - f1_45: 0.4528 - val_loss: 0.0939 - val_accuracy: 0.9755 - val_f1: 0.2918 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.4394 - val_f1_5: 0.0000e+00 - val_f1_6: 0.7754 - val_f1_7: 0.7347 - val_f1_8: 0.7269 - val_f1_9: 0.5896 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0540 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.4691 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9367 - val_f1_23: 0.7057 - val_f1_24: 0.0203 - val_f1_25: 0.8919 - val_f1_26: 0.0000e+00 - val_f1_28: 0.8270 - val_f1_29: 0.7017 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.5837 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.6595 - val_f1_39: 0.9989 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9287 - val_f1_43: 0.0000e+00 - val_f1_44: 0.1349 - val_f1_45: 0.4951\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - 74s 5s/step - loss: 0.0861 - accuracy: 0.9781 - f1: 0.3120 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.5599 - f1_5: 0.0000e+00 - f1_6: 0.8403 - f1_7: 0.7702 - f1_8: 0.7389 - f1_9: 0.6060 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.1388 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.4982 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9325 - f1_23: 0.7177 - f1_24: 0.0571 - f1_25: 0.8969 - f1_26: 0.0000e+00 - f1_28: 0.8911 - f1_29: 0.6938 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0238 - f1_35: 0.6489 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.7527 - f1_39: 0.9976 - f1_41: 0.0000e+00 - f1_42: 0.9790 - f1_43: 0.0000e+00 - f1_44: 0.1878 - f1_45: 0.5486 - val_loss: 0.0883 - val_accuracy: 0.9767 - val_f1: 0.3069 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.4328 - val_f1_5: 0.0000e+00 - val_f1_6: 0.8143 - val_f1_7: 0.7467 - val_f1_8: 0.7450 - val_f1_9: 0.5934 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.1168 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.4727 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9388 - val_f1_23: 0.7774 - val_f1_24: 0.0569 - val_f1_25: 0.8893 - val_f1_26: 0.0000e+00 - val_f1_28: 0.8945 - val_f1_29: 0.6909 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.5668 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.7179 - val_f1_39: 1.0000 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9838 - val_f1_43: 0.0000e+00 - val_f1_44: 0.2680 - val_f1_45: 0.5703\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0803 - accuracy: 0.9794 - f1: 0.3346 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.6352 - f1_5: 0.0000e+00 - f1_6: 0.8425 - f1_7: 0.8015 - f1_8: 0.7732 - f1_9: 0.6875 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.1840 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.5218 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9360 - f1_23: 0.7807 - f1_24: 0.1672 - f1_25: 0.9005 - f1_26: 0.0000e+00 - f1_28: 0.9159 - f1_29: 0.7151 - f1_31: 0.0119 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.1539 - f1_35: 0.6791 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.7694 - f1_39: 0.9977 - f1_41: 0.0000e+00 - f1_42: 0.9863 - f1_43: 0.0000e+00 - f1_44: 0.3010 - f1_45: 0.6231 - val_loss: 0.0825 - val_accuracy: 0.9783 - val_f1: 0.3397 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.7726 - val_f1_5: 0.0000e+00 - val_f1_6: 0.7898 - val_f1_7: 0.7882 - val_f1_8: 0.7817 - val_f1_9: 0.7164 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.2056 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.5172 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9439 - val_f1_23: 0.7983 - val_f1_24: 0.2874 - val_f1_25: 0.9010 - val_f1_26: 0.0000e+00 - val_f1_28: 0.9268 - val_f1_29: 0.7364 - val_f1_31: 0.0036 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.1672 - val_f1_35: 0.6463 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.6844 - val_f1_39: 1.0000 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9892 - val_f1_43: 0.0000e+00 - val_f1_44: 0.3451 - val_f1_45: 0.5873\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0754 - accuracy: 0.9806 - f1: 0.3575 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.7757 - f1_5: 0.0000e+00 - f1_6: 0.8569 - f1_7: 0.8381 - f1_8: 0.8003 - f1_9: 0.7419 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.2399 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.5533 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9401 - f1_23: 0.7989 - f1_24: 0.3243 - f1_25: 0.9053 - f1_26: 0.0000e+00 - f1_28: 0.9209 - f1_29: 0.7312 - f1_31: 0.0196 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.3261 - f1_35: 0.7138 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.7761 - f1_39: 0.9987 - f1_41: 0.0000e+00 - f1_42: 0.9903 - f1_43: 0.0000e+00 - f1_44: 0.3931 - f1_45: 0.6573 - val_loss: 0.0781 - val_accuracy: 0.9795 - val_f1: 0.3608 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.8107 - val_f1_5: 0.0000e+00 - val_f1_6: 0.8253 - val_f1_7: 0.7867 - val_f1_8: 0.7912 - val_f1_9: 0.7304 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.2256 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.5596 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9452 - val_f1_23: 0.8288 - val_f1_24: 0.3769 - val_f1_25: 0.9082 - val_f1_26: 0.0000e+00 - val_f1_28: 0.9327 - val_f1_29: 0.7147 - val_f1_31: 0.0421 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.4817 - val_f1_35: 0.6751 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.7330 - val_f1_39: 1.0000 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9975 - val_f1_43: 0.0000e+00 - val_f1_44: 0.4639 - val_f1_45: 0.6014\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0712 - accuracy: 0.9816 - f1: 0.3760 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.8226 - f1_5: 0.0000e+00 - f1_6: 0.8679 - f1_7: 0.8534 - f1_8: 0.8102 - f1_9: 0.7668 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.2897 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.5688 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9419 - f1_23: 0.8234 - f1_24: 0.4301 - f1_25: 0.9092 - f1_26: 0.0000e+00 - f1_28: 0.9156 - f1_29: 0.7449 - f1_31: 0.0605 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.5584 - f1_35: 0.7311 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.7907 - f1_39: 0.9986 - f1_41: 0.0000e+00 - f1_42: 0.9935 - f1_43: 0.0000e+00 - f1_44: 0.4616 - f1_45: 0.7021 - val_loss: 0.0744 - val_accuracy: 0.9803 - val_f1: 0.3714 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.8285 - val_f1_5: 0.0000e+00 - val_f1_6: 0.8382 - val_f1_7: 0.7984 - val_f1_8: 0.7872 - val_f1_9: 0.7539 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.3016 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.5751 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9470 - val_f1_23: 0.8479 - val_f1_24: 0.4814 - val_f1_25: 0.9067 - val_f1_26: 0.0000e+00 - val_f1_28: 0.9302 - val_f1_29: 0.7729 - val_f1_31: 0.0530 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.5255 - val_f1_35: 0.6823 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.7242 - val_f1_39: 1.0000 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9975 - val_f1_43: 0.0000e+00 - val_f1_44: 0.4808 - val_f1_45: 0.6223\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - 78s 5s/step - loss: 0.0673 - accuracy: 0.9826 - f1: 0.3887 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.8313 - f1_5: 0.0000e+00 - f1_6: 0.8720 - f1_7: 0.8737 - f1_8: 0.8316 - f1_9: 0.7921 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.3125 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.5995 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9431 - f1_23: 0.8362 - f1_24: 0.4841 - f1_25: 0.9148 - f1_26: 0.0000e+00 - f1_28: 0.9263 - f1_29: 0.7580 - f1_31: 0.1373 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.6796 - f1_35: 0.7618 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.7998 - f1_39: 0.9986 - f1_41: 0.0000e+00 - f1_42: 0.9956 - f1_43: 0.0000e+00 - f1_44: 0.4749 - f1_45: 0.7240 - val_loss: 0.0710 - val_accuracy: 0.9812 - val_f1: 0.3853 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.8312 - val_f1_5: 0.0000e+00 - val_f1_6: 0.8596 - val_f1_7: 0.8749 - val_f1_8: 0.8024 - val_f1_9: 0.7950 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.2973 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.5670 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9476 - val_f1_23: 0.8760 - val_f1_24: 0.5454 - val_f1_25: 0.9092 - val_f1_26: 0.0000e+00 - val_f1_28: 0.9339 - val_f1_29: 0.7499 - val_f1_31: 0.0999 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.6570 - val_f1_35: 0.7076 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.7572 - val_f1_39: 1.0000 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9975 - val_f1_43: 0.0000e+00 - val_f1_44: 0.5096 - val_f1_45: 0.6931\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - 80s 5s/step - loss: 0.0641 - accuracy: 0.9833 - f1: 0.3998 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.8687 - f1_5: 0.0000e+00 - f1_6: 0.8857 - f1_7: 0.8965 - f1_8: 0.8413 - f1_9: 0.8160 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.3712 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.6208 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9465 - f1_23: 0.8569 - f1_24: 0.5261 - f1_25: 0.9189 - f1_26: 0.0000e+00 - f1_28: 0.9181 - f1_29: 0.7667 - f1_31: 0.1941 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.7151 - f1_35: 0.7697 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.8060 - f1_39: 0.9987 - f1_41: 0.0000e+00 - f1_42: 0.9936 - f1_43: 0.0000e+00 - f1_44: 0.5264 - f1_45: 0.7548 - val_loss: 0.0679 - val_accuracy: 0.9817 - val_f1: 0.3947 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.8662 - val_f1_5: 0.0000e+00 - val_f1_6: 0.8668 - val_f1_7: 0.8951 - val_f1_8: 0.8373 - val_f1_9: 0.7800 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.3381 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.5900 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9503 - val_f1_23: 0.8623 - val_f1_24: 0.5292 - val_f1_25: 0.9193 - val_f1_26: 0.0000e+00 - val_f1_28: 0.9307 - val_f1_29: 0.7753 - val_f1_31: 0.1426 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.7677 - val_f1_35: 0.7210 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.7697 - val_f1_39: 1.0000 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9975 - val_f1_43: 0.0000e+00 - val_f1_44: 0.5544 - val_f1_45: 0.6929\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - 80s 5s/step - loss: 0.0611 - accuracy: 0.9840 - f1: 0.4103 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.8916 - f1_5: 0.0000e+00 - f1_6: 0.8971 - f1_7: 0.9136 - f1_8: 0.8625 - f1_9: 0.8184 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.4083 - f1_14: 0.0244 - f1_15: 0.0000e+00 - f1_16: 0.6408 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9494 - f1_23: 0.8649 - f1_24: 0.5658 - f1_25: 0.9218 - f1_26: 0.0000e+00 - f1_28: 0.9171 - f1_29: 0.7766 - f1_31: 0.2467 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.8142 - f1_35: 0.7808 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.8154 - f1_39: 0.9986 - f1_41: 0.0000e+00 - f1_42: 0.9947 - f1_43: 0.0000e+00 - f1_44: 0.5524 - f1_45: 0.7588 - val_loss: 0.0658 - val_accuracy: 0.9824 - val_f1: 0.4034 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.8829 - val_f1_5: 0.0000e+00 - val_f1_6: 0.8733 - val_f1_7: 0.8730 - val_f1_8: 0.8441 - val_f1_9: 0.8030 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.3912 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.5587 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9510 - val_f1_23: 0.8736 - val_f1_24: 0.6376 - val_f1_25: 0.9149 - val_f1_26: 0.0000e+00 - val_f1_28: 0.9288 - val_f1_29: 0.7752 - val_f1_31: 0.2065 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.7907 - val_f1_35: 0.7501 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.7737 - val_f1_39: 1.0000 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9975 - val_f1_43: 0.0000e+00 - val_f1_44: 0.5866 - val_f1_45: 0.7225\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - 78s 5s/step - loss: 0.0588 - accuracy: 0.9845 - f1: 0.4169 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.9114 - f1_5: 0.0000e+00 - f1_6: 0.9084 - f1_7: 0.9134 - f1_8: 0.8774 - f1_9: 0.8242 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.4315 - f1_14: 0.0173 - f1_15: 0.0000e+00 - f1_16: 0.6402 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9509 - f1_23: 0.8822 - f1_24: 0.5746 - f1_25: 0.9227 - f1_26: 0.0000e+00 - f1_28: 0.9212 - f1_29: 0.7896 - f1_31: 0.3251 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.8146 - f1_35: 0.7989 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.8198 - f1_39: 0.9986 - f1_41: 0.0000e+00 - f1_42: 0.9934 - f1_43: 0.0000e+00 - f1_44: 0.5924 - f1_45: 0.7690 - val_loss: 0.0636 - val_accuracy: 0.9829 - val_f1: 0.4118 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.8978 - val_f1_5: 0.0000e+00 - val_f1_6: 0.8896 - val_f1_7: 0.8941 - val_f1_8: 0.8583 - val_f1_9: 0.8155 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.4504 - val_f1_14: 0.0091 - val_f1_15: 0.0000e+00 - val_f1_16: 0.5946 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9520 - val_f1_23: 0.8882 - val_f1_24: 0.6644 - val_f1_25: 0.9186 - val_f1_26: 0.0000e+00 - val_f1_28: 0.9340 - val_f1_29: 0.7433 - val_f1_31: 0.2851 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.7998 - val_f1_35: 0.7826 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.7902 - val_f1_39: 1.0000 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9975 - val_f1_43: 0.0000e+00 - val_f1_44: 0.5789 - val_f1_45: 0.7273\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0566 - accuracy: 0.9851 - f1: 0.4246 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.9330 - f1_5: 0.0000e+00 - f1_6: 0.9105 - f1_7: 0.9255 - f1_8: 0.8854 - f1_9: 0.8235 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.4838 - f1_14: 0.0612 - f1_15: 0.0000e+00 - f1_16: 0.6621 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9527 - f1_23: 0.8934 - f1_24: 0.6010 - f1_25: 0.9303 - f1_26: 0.0000e+00 - f1_28: 0.9418 - f1_29: 0.7863 - f1_31: 0.3604 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.8522 - f1_35: 0.8015 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.8218 - f1_39: 0.9987 - f1_41: 0.0000e+00 - f1_42: 0.9924 - f1_43: 0.0000e+00 - f1_44: 0.5784 - f1_45: 0.7887 - val_loss: 0.0616 - val_accuracy: 0.9831 - val_f1: 0.4149 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.9191 - val_f1_5: 0.0000e+00 - val_f1_6: 0.8853 - val_f1_7: 0.9174 - val_f1_8: 0.8583 - val_f1_9: 0.8195 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.4073 - val_f1_14: 0.0468 - val_f1_15: 0.0000e+00 - val_f1_16: 0.6061 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9546 - val_f1_23: 0.8869 - val_f1_24: 0.6773 - val_f1_25: 0.9224 - val_f1_26: 0.0000e+00 - val_f1_28: 0.9408 - val_f1_29: 0.8067 - val_f1_31: 0.2828 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8629 - val_f1_35: 0.7867 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.7503 - val_f1_39: 1.0000 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9975 - val_f1_43: 0.0000e+00 - val_f1_44: 0.5214 - val_f1_45: 0.7439\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - 78s 5s/step - loss: 0.0546 - accuracy: 0.9855 - f1: 0.4342 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.9481 - f1_5: 0.0000e+00 - f1_6: 0.9238 - f1_7: 0.9260 - f1_8: 0.8898 - f1_9: 0.8453 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.5103 - f1_14: 0.1745 - f1_15: 0.0000e+00 - f1_16: 0.6702 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9546 - f1_23: 0.9014 - f1_24: 0.6166 - f1_25: 0.9282 - f1_26: 0.0000e+00 - f1_28: 0.9463 - f1_29: 0.7911 - f1_31: 0.4152 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.8904 - f1_35: 0.8199 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.8262 - f1_39: 0.9987 - f1_41: 0.0000e+00 - f1_42: 0.9938 - f1_43: 0.0000e+00 - f1_44: 0.5970 - f1_45: 0.7987 - val_loss: 0.0598 - val_accuracy: 0.9838 - val_f1: 0.4240 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.9221 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9008 - val_f1_7: 0.9263 - val_f1_8: 0.8745 - val_f1_9: 0.8138 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.4954 - val_f1_14: 0.1137 - val_f1_15: 0.0000e+00 - val_f1_16: 0.6216 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9554 - val_f1_23: 0.8995 - val_f1_24: 0.6611 - val_f1_25: 0.9244 - val_f1_26: 0.0000e+00 - val_f1_28: 0.9498 - val_f1_29: 0.7805 - val_f1_31: 0.3325 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8643 - val_f1_35: 0.7610 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.7964 - val_f1_39: 1.0000 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9975 - val_f1_43: 0.0000e+00 - val_f1_44: 0.6301 - val_f1_45: 0.7391\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0524 - accuracy: 0.9861 - f1: 0.4410 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.9492 - f1_5: 0.0000e+00 - f1_6: 0.9296 - f1_7: 0.9396 - f1_8: 0.9002 - f1_9: 0.8417 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.5411 - f1_14: 0.1968 - f1_15: 0.0000e+00 - f1_16: 0.6784 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9562 - f1_23: 0.9070 - f1_24: 0.6567 - f1_25: 0.9322 - f1_26: 0.0000e+00 - f1_28: 0.9515 - f1_29: 0.8073 - f1_31: 0.4600 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.8960 - f1_35: 0.8242 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.8339 - f1_39: 0.9986 - f1_41: 0.0114 - f1_42: 0.9924 - f1_43: 0.0000e+00 - f1_44: 0.6325 - f1_45: 0.8046 - val_loss: 0.0578 - val_accuracy: 0.9843 - val_f1: 0.4339 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.9267 - val_f1_5: 0.0000e+00 - val_f1_6: 0.8907 - val_f1_7: 0.9298 - val_f1_8: 0.8855 - val_f1_9: 0.8252 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.5588 - val_f1_14: 0.3626 - val_f1_15: 0.0000e+00 - val_f1_16: 0.6326 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9586 - val_f1_23: 0.9177 - val_f1_24: 0.6259 - val_f1_25: 0.9244 - val_f1_26: 0.0000e+00 - val_f1_28: 0.9586 - val_f1_29: 0.8145 - val_f1_31: 0.3273 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9207 - val_f1_35: 0.7704 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.7793 - val_f1_39: 1.0000 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9975 - val_f1_43: 0.0000e+00 - val_f1_44: 0.5939 - val_f1_45: 0.7569\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0505 - accuracy: 0.9866 - f1: 0.4529 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.9586 - f1_5: 0.0000e+00 - f1_6: 0.9315 - f1_7: 0.9418 - f1_8: 0.9063 - f1_9: 0.8520 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.5946 - f1_14: 0.3851 - f1_15: 0.0312 - f1_16: 0.6957 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9578 - f1_23: 0.9167 - f1_24: 0.6611 - f1_25: 0.9331 - f1_26: 0.0000e+00 - f1_28: 0.9555 - f1_29: 0.8130 - f1_31: 0.5022 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9063 - f1_35: 0.8342 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.8428 - f1_39: 0.9986 - f1_41: 0.0351 - f1_42: 0.9947 - f1_43: 0.0000e+00 - f1_44: 0.6516 - f1_45: 0.8148 - val_loss: 0.0563 - val_accuracy: 0.9845 - val_f1: 0.4363 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.9392 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9213 - val_f1_7: 0.9302 - val_f1_8: 0.8787 - val_f1_9: 0.8348 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.5555 - val_f1_14: 0.3154 - val_f1_15: 0.0000e+00 - val_f1_16: 0.6424 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9609 - val_f1_23: 0.9219 - val_f1_24: 0.6820 - val_f1_25: 0.9251 - val_f1_26: 0.0000e+00 - val_f1_28: 0.9603 - val_f1_29: 0.8156 - val_f1_31: 0.3223 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9214 - val_f1_35: 0.7770 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.7937 - val_f1_39: 1.0000 - val_f1_41: 0.0000e+00 - val_f1_42: 0.9975 - val_f1_43: 0.0000e+00 - val_f1_44: 0.5938 - val_f1_45: 0.7613\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0492 - accuracy: 0.9868 - f1: 0.4608 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.9631 - f1_5: 0.0000e+00 - f1_6: 0.9367 - f1_7: 0.9423 - f1_8: 0.9075 - f1_9: 0.8575 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.5923 - f1_14: 0.5456 - f1_15: 0.0379 - f1_16: 0.6951 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9625 - f1_23: 0.9209 - f1_24: 0.6696 - f1_25: 0.9326 - f1_26: 0.0000e+00 - f1_28: 0.9644 - f1_29: 0.8161 - f1_31: 0.5073 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9381 - f1_35: 0.8342 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.8360 - f1_39: 0.9985 - f1_41: 0.0982 - f1_42: 0.9932 - f1_43: 0.0000e+00 - f1_44: 0.6618 - f1_45: 0.8221 - val_loss: 0.0549 - val_accuracy: 0.9850 - val_f1: 0.4460 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.9462 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9395 - val_f1_7: 0.9314 - val_f1_8: 0.8856 - val_f1_9: 0.8421 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6032 - val_f1_14: 0.4380 - val_f1_15: 0.0303 - val_f1_16: 0.6388 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9603 - val_f1_23: 0.9321 - val_f1_24: 0.7091 - val_f1_25: 0.9261 - val_f1_26: 0.0000e+00 - val_f1_28: 0.9596 - val_f1_29: 0.8128 - val_f1_31: 0.3470 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9335 - val_f1_35: 0.8182 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.7925 - val_f1_39: 1.0000 - val_f1_41: 0.0442 - val_f1_42: 0.9975 - val_f1_43: 0.0000e+00 - val_f1_44: 0.6057 - val_f1_45: 0.7460\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0476 - accuracy: 0.9873 - f1: 0.4649 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.9669 - f1_5: 0.0000e+00 - f1_6: 0.9390 - f1_7: 0.9466 - f1_8: 0.9063 - f1_9: 0.8655 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.6083 - f1_14: 0.5053 - f1_15: 0.1390 - f1_16: 0.7018 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9636 - f1_23: 0.9186 - f1_24: 0.6935 - f1_25: 0.9376 - f1_26: 0.0000e+00 - f1_28: 0.9633 - f1_29: 0.8238 - f1_31: 0.5045 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9464 - f1_35: 0.8410 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.8423 - f1_39: 0.9986 - f1_41: 0.1004 - f1_42: 0.9935 - f1_43: 0.0000e+00 - f1_44: 0.6631 - f1_45: 0.8264 - val_loss: 0.0535 - val_accuracy: 0.9854 - val_f1: 0.4591 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.9537 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9410 - val_f1_7: 0.9330 - val_f1_8: 0.8760 - val_f1_9: 0.8394 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.5634 - val_f1_14: 0.5582 - val_f1_15: 0.2744 - val_f1_16: 0.6605 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9629 - val_f1_23: 0.9298 - val_f1_24: 0.7343 - val_f1_25: 0.9317 - val_f1_26: 0.0000e+00 - val_f1_28: 0.9625 - val_f1_29: 0.8016 - val_f1_31: 0.3916 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9469 - val_f1_35: 0.8276 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.7988 - val_f1_39: 1.0000 - val_f1_41: 0.0745 - val_f1_42: 0.9975 - val_f1_43: 0.0000e+00 - val_f1_44: 0.6347 - val_f1_45: 0.7696\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0460 - accuracy: 0.9876 - f1: 0.4750 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.9730 - f1_5: 0.0000e+00 - f1_6: 0.9425 - f1_7: 0.9469 - f1_8: 0.9128 - f1_9: 0.8751 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.6251 - f1_14: 0.5969 - f1_15: 0.2637 - f1_16: 0.7192 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9656 - f1_23: 0.9271 - f1_24: 0.7071 - f1_25: 0.9412 - f1_26: 0.0000e+00 - f1_28: 0.9709 - f1_29: 0.8265 - f1_31: 0.5404 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9413 - f1_35: 0.8498 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.8501 - f1_39: 0.9975 - f1_41: 0.1356 - f1_42: 0.9898 - f1_43: 0.0000e+00 - f1_44: 0.6678 - f1_45: 0.8333 - val_loss: 0.0528 - val_accuracy: 0.9854 - val_f1: 0.4665 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.9643 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9240 - val_f1_7: 0.9370 - val_f1_8: 0.8984 - val_f1_9: 0.8408 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.5375 - val_f1_14: 0.7533 - val_f1_15: 0.2925 - val_f1_16: 0.6661 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9705 - val_f1_23: 0.9276 - val_f1_24: 0.7166 - val_f1_25: 0.9322 - val_f1_26: 0.0000e+00 - val_f1_28: 0.9601 - val_f1_29: 0.8203 - val_f1_31: 0.4680 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9456 - val_f1_35: 0.8170 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.7743 - val_f1_39: 1.0000 - val_f1_41: 0.1091 - val_f1_42: 0.9952 - val_f1_43: 0.0000e+00 - val_f1_44: 0.6312 - val_f1_45: 0.7782\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0448 - accuracy: 0.9880 - f1: 0.4865 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.9718 - f1_5: 0.0000e+00 - f1_6: 0.9466 - f1_7: 0.9497 - f1_8: 0.9065 - f1_9: 0.8579 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.6510 - f1_14: 0.7043 - f1_15: 0.4365 - f1_16: 0.7193 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9676 - f1_23: 0.9277 - f1_24: 0.7150 - f1_25: 0.9413 - f1_26: 0.0000e+00 - f1_28: 0.9639 - f1_29: 0.8270 - f1_31: 0.5601 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9526 - f1_35: 0.8546 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.8507 - f1_39: 0.9986 - f1_41: 0.2457 - f1_42: 0.9944 - f1_43: 0.0000e+00 - f1_44: 0.6776 - f1_45: 0.8415 - val_loss: 0.0518 - val_accuracy: 0.9856 - val_f1: 0.4716 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.9491 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9296 - val_f1_7: 0.9430 - val_f1_8: 0.8846 - val_f1_9: 0.8471 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6199 - val_f1_14: 0.7505 - val_f1_15: 0.3106 - val_f1_16: 0.6158 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9698 - val_f1_23: 0.9386 - val_f1_24: 0.7350 - val_f1_25: 0.9370 - val_f1_26: 0.0000e+00 - val_f1_28: 0.9658 - val_f1_29: 0.8221 - val_f1_31: 0.4712 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9569 - val_f1_35: 0.8064 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.8008 - val_f1_39: 1.0000 - val_f1_41: 0.1697 - val_f1_42: 0.9952 - val_f1_43: 0.0000e+00 - val_f1_44: 0.6635 - val_f1_45: 0.7832\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0437 - accuracy: 0.9882 - f1: 0.4925 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.9710 - f1_5: 0.0000e+00 - f1_6: 0.9522 - f1_7: 0.9533 - f1_8: 0.9131 - f1_9: 0.8755 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.6498 - f1_14: 0.7388 - f1_15: 0.4845 - f1_16: 0.7232 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9693 - f1_23: 0.9323 - f1_24: 0.7367 - f1_25: 0.9441 - f1_26: 0.0000e+00 - f1_28: 0.9746 - f1_29: 0.8330 - f1_31: 0.5653 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9619 - f1_35: 0.8598 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.8530 - f1_39: 0.9986 - f1_41: 0.2671 - f1_42: 0.9930 - f1_43: 0.0000e+00 - f1_44: 0.7000 - f1_45: 0.8504 - val_loss: 0.0511 - val_accuracy: 0.9858 - val_f1: 0.4775 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.9736 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9418 - val_f1_7: 0.9460 - val_f1_8: 0.8745 - val_f1_9: 0.8583 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6263 - val_f1_14: 0.7472 - val_f1_15: 0.4949 - val_f1_16: 0.6841 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9726 - val_f1_23: 0.9369 - val_f1_24: 0.7315 - val_f1_25: 0.9366 - val_f1_26: 0.0000e+00 - val_f1_28: 0.9718 - val_f1_29: 0.8203 - val_f1_31: 0.4883 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9595 - val_f1_35: 0.8294 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.7678 - val_f1_39: 1.0000 - val_f1_41: 0.1242 - val_f1_42: 0.9975 - val_f1_43: 0.0000e+00 - val_f1_44: 0.6443 - val_f1_45: 0.7716\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0426 - accuracy: 0.9885 - f1: 0.4997 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.9710 - f1_5: 0.0000e+00 - f1_6: 0.9562 - f1_7: 0.9549 - f1_8: 0.9179 - f1_9: 0.8804 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.6739 - f1_14: 0.7838 - f1_15: 0.5879 - f1_16: 0.7359 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9714 - f1_23: 0.9344 - f1_24: 0.7369 - f1_25: 0.9443 - f1_26: 0.0000e+00 - f1_28: 0.9815 - f1_29: 0.8399 - f1_31: 0.5998 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9652 - f1_35: 0.8624 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.8557 - f1_39: 0.9987 - f1_41: 0.2897 - f1_42: 0.9923 - f1_43: 0.0000e+00 - f1_44: 0.7031 - f1_45: 0.8527 - val_loss: 0.0498 - val_accuracy: 0.9860 - val_f1: 0.4836 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.9705 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9533 - val_f1_7: 0.9427 - val_f1_8: 0.8937 - val_f1_9: 0.8551 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6263 - val_f1_14: 0.7395 - val_f1_15: 0.5646 - val_f1_16: 0.6965 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9754 - val_f1_23: 0.9316 - val_f1_24: 0.7477 - val_f1_25: 0.9434 - val_f1_26: 0.0000e+00 - val_f1_28: 0.9644 - val_f1_29: 0.8230 - val_f1_31: 0.5146 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9584 - val_f1_35: 0.8393 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.7783 - val_f1_39: 1.0000 - val_f1_41: 0.2061 - val_f1_42: 0.9975 - val_f1_43: 0.0000e+00 - val_f1_44: 0.6397 - val_f1_45: 0.7806\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0413 - accuracy: 0.9888 - f1: 0.5041 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.9738 - f1_5: 0.0000e+00 - f1_6: 0.9597 - f1_7: 0.9589 - f1_8: 0.9208 - f1_9: 0.8839 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.6900 - f1_14: 0.7844 - f1_15: 0.6573 - f1_16: 0.7437 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9722 - f1_23: 0.9301 - f1_24: 0.7533 - f1_25: 0.9468 - f1_26: 0.0000e+00 - f1_28: 0.9818 - f1_29: 0.8417 - f1_31: 0.6184 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9615 - f1_35: 0.8678 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.8574 - f1_39: 0.9985 - f1_41: 0.3070 - f1_42: 0.9927 - f1_43: 0.0000e+00 - f1_44: 0.7088 - f1_45: 0.8555 - val_loss: 0.0486 - val_accuracy: 0.9864 - val_f1: 0.4894 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.9758 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9645 - val_f1_7: 0.9526 - val_f1_8: 0.8966 - val_f1_9: 0.8637 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6662 - val_f1_14: 0.7380 - val_f1_15: 0.5756 - val_f1_16: 0.6860 - val_f1_18: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9743 - val_f1_23: 0.9372 - val_f1_24: 0.7441 - val_f1_25: 0.9428 - val_f1_26: 0.0000e+00 - val_f1_28: 0.9718 - val_f1_29: 0.8264 - val_f1_31: 0.4886 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9621 - val_f1_35: 0.8399 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.8062 - val_f1_39: 1.0000 - val_f1_41: 0.3432 - val_f1_42: 0.9952 - val_f1_43: 0.0000e+00 - val_f1_44: 0.6430 - val_f1_45: 0.7805\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0404 - accuracy: 0.9890 - f1: 0.5095 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.9711 - f1_5: 0.0000e+00 - f1_6: 0.9701 - f1_7: 0.9567 - f1_8: 0.9179 - f1_9: 0.8873 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.6978 - f1_14: 0.7856 - f1_15: 0.7246 - f1_16: 0.7466 - f1_18: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.9742 - f1_23: 0.9385 - f1_24: 0.7587 - f1_25: 0.9469 - f1_26: 0.0000e+00 - f1_28: 0.9855 - f1_29: 0.8385 - f1_31: 0.6236 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9678 - f1_35: 0.8732 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.8625 - f1_39: 0.9980 - f1_41: 0.3822 - f1_42: 0.9942 - f1_43: 0.0000e+00 - f1_44: 0.7150 - f1_45: 0.8619 - val_loss: 0.0491 - val_accuracy: 0.9859 - val_f1: 0.4888 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.9680 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9563 - val_f1_7: 0.9501 - val_f1_8: 0.9017 - val_f1_9: 0.8505 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6402 - val_f1_14: 0.7849 - val_f1_15: 0.5747 - val_f1_16: 0.6670 - val_f1_18: 0.0083 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9782 - val_f1_23: 0.9253 - val_f1_24: 0.7541 - val_f1_25: 0.9460 - val_f1_26: 0.0000e+00 - val_f1_28: 0.9592 - val_f1_29: 0.8233 - val_f1_31: 0.5189 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9643 - val_f1_35: 0.8385 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.7727 - val_f1_39: 1.0000 - val_f1_41: 0.3352 - val_f1_42: 0.9975 - val_f1_43: 0.0000e+00 - val_f1_44: 0.6549 - val_f1_45: 0.7827\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - 78s 5s/step - loss: 0.0395 - accuracy: 0.9893 - f1: 0.5145 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.9783 - f1_5: 0.0000e+00 - f1_6: 0.9680 - f1_7: 0.9648 - f1_8: 0.9195 - f1_9: 0.8893 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.7208 - f1_14: 0.8503 - f1_15: 0.7433 - f1_16: 0.7499 - f1_18: 0.0264 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0292 - f1_22: 0.9741 - f1_23: 0.9386 - f1_24: 0.7689 - f1_25: 0.9497 - f1_26: 0.0000e+00 - f1_28: 0.9855 - f1_29: 0.8418 - f1_31: 0.6425 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9691 - f1_35: 0.8764 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.8642 - f1_39: 0.9985 - f1_41: 0.3447 - f1_42: 0.9920 - f1_43: 0.0000e+00 - f1_44: 0.7325 - f1_45: 0.8598 - val_loss: 0.0472 - val_accuracy: 0.9866 - val_f1: 0.4920 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.9786 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9630 - val_f1_7: 0.9572 - val_f1_8: 0.9028 - val_f1_9: 0.8719 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6546 - val_f1_14: 0.7368 - val_f1_15: 0.5970 - val_f1_16: 0.7061 - val_f1_18: 0.0158 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9784 - val_f1_23: 0.9364 - val_f1_24: 0.7467 - val_f1_25: 0.9454 - val_f1_26: 0.0000e+00 - val_f1_28: 0.9718 - val_f1_29: 0.8293 - val_f1_31: 0.4870 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9619 - val_f1_35: 0.8381 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.8036 - val_f1_39: 1.0000 - val_f1_41: 0.3391 - val_f1_42: 0.9975 - val_f1_43: 0.0000e+00 - val_f1_44: 0.6673 - val_f1_45: 0.7918\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0386 - accuracy: 0.9894 - f1: 0.5174 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.9796 - f1_5: 0.0000e+00 - f1_6: 0.9728 - f1_7: 0.9676 - f1_8: 0.9199 - f1_9: 0.8929 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.7200 - f1_14: 0.8275 - f1_15: 0.7925 - f1_16: 0.7538 - f1_18: 0.0277 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0125 - f1_22: 0.9746 - f1_23: 0.9396 - f1_24: 0.7730 - f1_25: 0.9501 - f1_26: 0.0000e+00 - f1_28: 0.9818 - f1_29: 0.8446 - f1_31: 0.6256 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9690 - f1_35: 0.8789 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.8696 - f1_39: 0.9987 - f1_41: 0.4305 - f1_42: 0.9949 - f1_43: 0.0000e+00 - f1_44: 0.7290 - f1_45: 0.8703 - val_loss: 0.0469 - val_accuracy: 0.9865 - val_f1: 0.4933 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.9792 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9644 - val_f1_7: 0.9563 - val_f1_8: 0.9060 - val_f1_9: 0.8579 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6645 - val_f1_14: 0.7270 - val_f1_15: 0.6218 - val_f1_16: 0.7159 - val_f1_18: 0.0083 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9785 - val_f1_23: 0.9402 - val_f1_24: 0.7519 - val_f1_25: 0.9492 - val_f1_26: 0.0000e+00 - val_f1_28: 0.9808 - val_f1_29: 0.8324 - val_f1_31: 0.5550 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9606 - val_f1_35: 0.8374 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.7917 - val_f1_39: 1.0000 - val_f1_41: 0.3085 - val_f1_42: 0.9975 - val_f1_43: 0.0000e+00 - val_f1_44: 0.6501 - val_f1_45: 0.7963\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - 75s 5s/step - loss: 0.0377 - accuracy: 0.9897 - f1: 0.5212 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.9764 - f1_5: 0.0000e+00 - f1_6: 0.9747 - f1_7: 0.9655 - f1_8: 0.9294 - f1_9: 0.8991 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.7223 - f1_14: 0.8481 - f1_15: 0.7878 - f1_16: 0.7682 - f1_18: 0.0347 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0335 - f1_22: 0.9748 - f1_23: 0.9441 - f1_24: 0.7806 - f1_25: 0.9524 - f1_26: 0.0000e+00 - f1_28: 0.9867 - f1_29: 0.8532 - f1_31: 0.6503 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9689 - f1_35: 0.8876 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.8732 - f1_39: 0.9982 - f1_41: 0.4340 - f1_42: 0.9924 - f1_43: 0.0000e+00 - f1_44: 0.7347 - f1_45: 0.8766 - val_loss: 0.0458 - val_accuracy: 0.9869 - val_f1: 0.4976 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.9867 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9665 - val_f1_7: 0.9608 - val_f1_8: 0.8965 - val_f1_9: 0.8680 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6905 - val_f1_14: 0.8106 - val_f1_15: 0.6218 - val_f1_16: 0.7115 - val_f1_18: 0.0336 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9776 - val_f1_23: 0.9406 - val_f1_24: 0.7661 - val_f1_25: 0.9481 - val_f1_26: 0.0000e+00 - val_f1_28: 0.9695 - val_f1_29: 0.8315 - val_f1_31: 0.4774 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9719 - val_f1_35: 0.8423 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.8166 - val_f1_39: 1.0000 - val_f1_41: 0.3430 - val_f1_42: 0.9975 - val_f1_43: 0.0000e+00 - val_f1_44: 0.6712 - val_f1_45: 0.8049\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - 83s 5s/step - loss: 0.0370 - accuracy: 0.9898 - f1: 0.5238 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.9733 - f1_5: 0.0000e+00 - f1_6: 0.9767 - f1_7: 0.9654 - f1_8: 0.9289 - f1_9: 0.9024 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.7336 - f1_14: 0.8591 - f1_15: 0.8286 - f1_16: 0.7717 - f1_18: 0.0665 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0139 - f1_22: 0.9758 - f1_23: 0.9454 - f1_24: 0.7823 - f1_25: 0.9531 - f1_26: 0.0000e+00 - f1_28: 0.9840 - f1_29: 0.8481 - f1_31: 0.6637 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9725 - f1_35: 0.8826 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.8713 - f1_39: 0.9987 - f1_41: 0.4466 - f1_42: 0.9929 - f1_43: 0.0000e+00 - f1_44: 0.7389 - f1_45: 0.8754 - val_loss: 0.0462 - val_accuracy: 0.9869 - val_f1: 0.4994 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.9935 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9697 - val_f1_7: 0.9683 - val_f1_8: 0.8983 - val_f1_9: 0.8660 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6844 - val_f1_14: 0.8146 - val_f1_15: 0.6218 - val_f1_16: 0.7162 - val_f1_18: 0.1003 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9769 - val_f1_23: 0.9454 - val_f1_24: 0.7802 - val_f1_25: 0.9372 - val_f1_26: 0.0000e+00 - val_f1_28: 0.9803 - val_f1_29: 0.8220 - val_f1_31: 0.4637 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9718 - val_f1_35: 0.8315 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.8133 - val_f1_39: 1.0000 - val_f1_41: 0.3429 - val_f1_42: 0.9975 - val_f1_43: 0.0000e+00 - val_f1_44: 0.6687 - val_f1_45: 0.8118\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0365 - accuracy: 0.9900 - f1: 0.5256 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.9802 - f1_5: 0.0000e+00 - f1_6: 0.9818 - f1_7: 0.9714 - f1_8: 0.9316 - f1_9: 0.9033 - f1_10: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.7454 - f1_14: 0.8794 - f1_15: 0.8079 - f1_16: 0.7697 - f1_18: 0.0838 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0114 - f1_22: 0.9769 - f1_23: 0.9459 - f1_24: 0.7931 - f1_25: 0.9533 - f1_26: 0.0000e+00 - f1_28: 0.9867 - f1_29: 0.8525 - f1_31: 0.6517 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.9721 - f1_35: 0.8800 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.8754 - f1_39: 0.9988 - f1_41: 0.4523 - f1_42: 0.9929 - f1_43: 0.0000e+00 - f1_44: 0.7406 - f1_45: 0.8839 - val_loss: 0.0450 - val_accuracy: 0.9870 - val_f1: 0.5004 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.9775 - val_f1_5: 0.0000e+00 - val_f1_6: 0.9735 - val_f1_7: 0.9561 - val_f1_8: 0.9041 - val_f1_9: 0.8572 - val_f1_10: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6519 - val_f1_14: 0.8053 - val_f1_15: 0.6412 - val_f1_16: 0.6980 - val_f1_18: 0.0978 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.9813 - val_f1_23: 0.9335 - val_f1_24: 0.7871 - val_f1_25: 0.9494 - val_f1_26: 0.0000e+00 - val_f1_28: 0.9798 - val_f1_29: 0.8296 - val_f1_31: 0.5510 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.9668 - val_f1_35: 0.8436 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.8238 - val_f1_39: 1.0000 - val_f1_41: 0.3313 - val_f1_42: 0.9952 - val_f1_43: 0.0000e+00 - val_f1_44: 0.6684 - val_f1_45: 0.8125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#History f1 for class"
      ],
      "metadata": {
        "id": "Q_XNg1aXonOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_val_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_val_list[i] = history.history['val_f1_{}'.format(i)][-1]\n",
        "f1_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_list[i] = history.history['f1_{}'.format(i)][-1]"
      ],
      "metadata": {
        "id": "DIeuPw8AorGd"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index2tag = {value : key for (key, value) in tag2index.items()}\n",
        "index2tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlQ8zRSMou1-",
        "outputId": "b95ae987-6f31-422c-8fda-120d91b33ad8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '-PAD-',\n",
              " 1: '-LRB-',\n",
              " 2: 'LS',\n",
              " 3: '-RRB-',\n",
              " 4: 'PRP$',\n",
              " 5: 'EX',\n",
              " 6: 'CC',\n",
              " 7: 'PRP',\n",
              " 8: 'CD',\n",
              " 9: 'VB',\n",
              " 10: 'NNPS',\n",
              " 11: ':',\n",
              " 12: 'SYM',\n",
              " 13: 'RB',\n",
              " 14: 'WDT',\n",
              " 15: 'WP',\n",
              " 16: 'JJ',\n",
              " 17: \"''\",\n",
              " 18: 'JJR',\n",
              " 19: 'RBS',\n",
              " 20: 'PDT',\n",
              " 21: 'RBR',\n",
              " 22: 'DT',\n",
              " 23: 'VBZ',\n",
              " 24: 'VBP',\n",
              " 25: 'IN',\n",
              " 26: 'JJS',\n",
              " 27: ',',\n",
              " 28: 'POS',\n",
              " 29: 'NN',\n",
              " 30: '``',\n",
              " 31: 'VBG',\n",
              " 32: 'WRB',\n",
              " 33: 'WP$',\n",
              " 34: 'MD',\n",
              " 35: 'NNS',\n",
              " 36: 'FW',\n",
              " 37: '#',\n",
              " 38: 'NNP',\n",
              " 39: 'TO',\n",
              " 40: '.',\n",
              " 41: 'RP',\n",
              " 42: '$',\n",
              " 43: 'UH',\n",
              " 44: 'VBN',\n",
              " 45: 'VBD'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- F1: {}'.format(index2tag[i], f1_list[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKsg3Ui-owlf",
        "outputId": "b4bc4a46-e925-40b7-d2fe-c38b6c0ca6e9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: -LRB- --- F1: 0.0\n",
            "Tag: LS --- F1: 0.0\n",
            "Tag: -RRB- --- F1: 0.0\n",
            "Tag: PRP$ --- F1: 0.9801980257034302\n",
            "Tag: EX --- F1: 0.0\n",
            "Tag: CC --- F1: 0.9818464517593384\n",
            "Tag: PRP --- F1: 0.9713720679283142\n",
            "Tag: CD --- F1: 0.9316425919532776\n",
            "Tag: VB --- F1: 0.9033249616622925\n",
            "Tag: NNPS --- F1: 0.0\n",
            "Tag: SYM --- F1: 0.0\n",
            "Tag: RB --- F1: 0.7454392910003662\n",
            "Tag: WDT --- F1: 0.8794039487838745\n",
            "Tag: WP --- F1: 0.8078557848930359\n",
            "Tag: JJ --- F1: 0.7697327136993408\n",
            "Tag: JJR --- F1: 0.0837932825088501\n",
            "Tag: RBS --- F1: 0.0\n",
            "Tag: PDT --- F1: 0.0\n",
            "Tag: RBR --- F1: 0.011363633908331394\n",
            "Tag: DT --- F1: 0.9769250154495239\n",
            "Tag: VBZ --- F1: 0.9459187984466553\n",
            "Tag: VBP --- F1: 0.7931026220321655\n",
            "Tag: IN --- F1: 0.9533044099807739\n",
            "Tag: JJS --- F1: 0.0\n",
            "Tag: POS --- F1: 0.986731767654419\n",
            "Tag: NN --- F1: 0.8525465130805969\n",
            "Tag: VBG --- F1: 0.6516727805137634\n",
            "Tag: WRB --- F1: 0.0\n",
            "Tag: WP$ --- F1: 0.0\n",
            "Tag: MD --- F1: 0.9721231460571289\n",
            "Tag: NNS --- F1: 0.8799801468849182\n",
            "Tag: FW --- F1: 0.0\n",
            "Tag: # --- F1: 0.0\n",
            "Tag: NNP --- F1: 0.8754019737243652\n",
            "Tag: TO --- F1: 0.9987851977348328\n",
            "Tag: RP --- F1: 0.4522727429866791\n",
            "Tag: $ --- F1: 0.99288010597229\n",
            "Tag: UH --- F1: 0.0\n",
            "Tag: VBN --- F1: 0.7406455278396606\n",
            "Tag: VBD --- F1: 0.8839006423950195\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- Val_F1: {}'.format(index2tag[i], f1_val_list[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9wM9cNloyzc",
        "outputId": "80e0bd6f-3d9d-4288-e6ff-2092dfa0b43f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: -LRB- --- Val_F1: 0.0\n",
            "Tag: LS --- Val_F1: 0.0\n",
            "Tag: -RRB- --- Val_F1: 0.0\n",
            "Tag: PRP$ --- Val_F1: 0.9775190949440002\n",
            "Tag: EX --- Val_F1: 0.0\n",
            "Tag: CC --- Val_F1: 0.9734578728675842\n",
            "Tag: PRP --- Val_F1: 0.9561285376548767\n",
            "Tag: CD --- Val_F1: 0.9041489958763123\n",
            "Tag: VB --- Val_F1: 0.8572497963905334\n",
            "Tag: NNPS --- Val_F1: 0.0\n",
            "Tag: SYM --- Val_F1: 0.0\n",
            "Tag: RB --- Val_F1: 0.6518694162368774\n",
            "Tag: WDT --- Val_F1: 0.8053422570228577\n",
            "Tag: WP --- Val_F1: 0.6412449479103088\n",
            "Tag: JJ --- Val_F1: 0.6980189681053162\n",
            "Tag: JJR --- Val_F1: 0.09775470942258835\n",
            "Tag: RBS --- Val_F1: 0.0\n",
            "Tag: PDT --- Val_F1: 0.0\n",
            "Tag: RBR --- Val_F1: 0.0\n",
            "Tag: DT --- Val_F1: 0.98126620054245\n",
            "Tag: VBZ --- Val_F1: 0.933537483215332\n",
            "Tag: VBP --- Val_F1: 0.7870826125144958\n",
            "Tag: IN --- Val_F1: 0.9494487643241882\n",
            "Tag: JJS --- Val_F1: 0.0\n",
            "Tag: POS --- Val_F1: 0.9797810912132263\n",
            "Tag: NN --- Val_F1: 0.8295570015907288\n",
            "Tag: VBG --- Val_F1: 0.550996720790863\n",
            "Tag: WRB --- Val_F1: 0.0\n",
            "Tag: WP$ --- Val_F1: 0.0\n",
            "Tag: MD --- Val_F1: 0.9667774438858032\n",
            "Tag: NNS --- Val_F1: 0.8435878157615662\n",
            "Tag: FW --- Val_F1: 0.0\n",
            "Tag: # --- Val_F1: 0.0\n",
            "Tag: NNP --- Val_F1: 0.8237524032592773\n",
            "Tag: TO --- Val_F1: 1.0\n",
            "Tag: RP --- Val_F1: 0.3313186466693878\n",
            "Tag: $ --- Val_F1: 0.9952153563499451\n",
            "Tag: UH --- Val_F1: 0.0\n",
            "Tag: VBN --- Val_F1: 0.6683931350708008\n",
            "Tag: VBD --- Val_F1: 0.8124809861183167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Infos"
      ],
      "metadata": {
        "id": "1uv6mtuMo1Mh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#40 epochs, batch 128\n",
        "loss: 0.0370 - accuracy: 0.9899 - f1: 0.5241\n",
        "\n",
        "loss: 0.0373 - accuracy: 0.9896 - f1: 0.5214"
      ],
      "metadata": {
        "id": "Jot_8JtVo37_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Evaluation"
      ],
      "metadata": {
        "id": "KOQVTC5DSMvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_without_point = []\n",
        "\n",
        "for i in test:\n",
        "  if i[0] != '':\n",
        "    if i[1] != '.' and i[1] != ',' and i[1] != '``' and i[1] != \"''\":\n",
        "      test_without_point += [i]\n",
        "  else: test_without_point += ''\n",
        "\n",
        "#print(test_without_point)\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in test_without_point:\n",
        "  if i != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    test_sentences.append(s)\n",
        "    test_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "test_sentences_X,  test_tags_y = [], []\n",
        "\n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        "\n",
        "for s in test_tags:\n",
        "    test_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ],
      "metadata": {
        "id": "w9Y9N9RYoBNi"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "f1 function written for the test evaluation"
      ],
      "metadata": {
        "id": "ElKKPcGLSUHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_score_single(y_true, y_pred):\n",
        "    y_true = set(y_true)\n",
        "    y_pred = set(y_pred)\n",
        "    cross_size = len(y_true & y_pred)\n",
        "    if cross_size == 0: return 0.\n",
        "    p = 1. * cross_size / len(y_pred)\n",
        "    r = 1. * cross_size / len(y_true)\n",
        "    return 2 * p * r / (p + r)\n",
        "    \n",
        "def f1_test(y_true, y_pred):\n",
        "    return np.mean([f1_score_single(x, y) for x, y in zip(y_true, y_pred)])"
      ],
      "metadata": {
        "id": "YPt7KavO9GAr"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_pred = model.predict(test_sentences_X)\n",
        "\n",
        "y_pred = np.zeros((y_val_pred.shape[0], y_val_pred.shape[1]), dtype=int)\n",
        "for i in range(len(y_val_pred)):\n",
        "  for j in range(len(y_val_pred[i])):\n",
        "    y_pred[i][j] = np.argmax(y_val_pred[i][j])\n",
        "\n",
        "\n",
        "f1_val = f1_test(test_tags_y, y_pred)"
      ],
      "metadata": {
        "id": "bFug9E5DoKWe"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZwSHKWC9glW",
        "outputId": "a1bc653e-c983-42ce-b682-d360c225cc68"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.93235382300883"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the f1_score from sklearn"
      ],
      "metadata": {
        "id": "Trdg6NLXSdGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "tags_flat = test_tags_y.flatten()\n",
        "pred_flat = y_pred.flatten()\n",
        "\n",
        "print(f1_score(tags_flat, pred_flat, average='weighted'))\n",
        "print(f1_score(tags_flat, pred_flat, average='macro'))\n",
        "print(f1_score(tags_flat, pred_flat, average='micro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bsytk51_PLmR",
        "outputId": "48503da9-a2be-458c-bc82-078cba90a6ff"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9866750199883636\n",
            "0.6706219753130207\n",
            "0.9871880158671495\n"
          ]
        }
      ]
    }
  ]
}