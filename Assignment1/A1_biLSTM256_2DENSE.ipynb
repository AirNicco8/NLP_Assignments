{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AirNicco8/NLP_Assignments/blob/main/Assignment1/A1_biLSTM256_2DENSE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Due to**: 23/12/2021 (dd/mm/yyyy)\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Part-of Speech (POS) tagging as Sequence Labelling using Recurrent Neural Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4_wqPdlBcKS"
      },
      "source": [
        "# Intro\n",
        "\n",
        "In this assignment  we will ask you to perform POS tagging using neural architectures\n",
        "\n",
        "You are asked to follow these steps:\n",
        "*   Download the corpora and split it in training and test sets, structuring a dataframe.\n",
        "*   Embed the words using GloVe embeddings\n",
        "*   Create a baseline model, using a simple neural architecture\n",
        "*   Experiment doing small modifications to the baseline model, choose hyperparameters using the validation set\n",
        "*   Evaluate your two best model\n",
        "*   Analyze the errors of your model\n",
        "\n",
        "\n",
        "**Task**: given a corpus of documents, predict the POS tag for each word\n",
        "\n",
        "**Corpus**:\n",
        "Ignore the numeric value in the third column, use only the words/symbols and its label. \n",
        "The corpus is available at:\n",
        "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
        "\n",
        "**Splits**: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
        "\n",
        "\n",
        "**Features**: you MUST use GloVe embeddings as the only input features to the model.\n",
        "\n",
        "**Splitting**: you can decide to split documents into sentences or not, the choice is yours.\n",
        "\n",
        "**I/O structure**: The input data will have three dimensions: 1-documents/sentences, 2-token, 3-features; for the output there are 2 possibilities: if you use one-hot encoding it will be 1-documents/sentences, 2-token labels, 3-classes, if you use a single integer that indicates the number of the class it will be 1-documents/sentences, 2-token labels.\n",
        "\n",
        "**Baseline**: two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top; the choice of hyper-parameters is yours.\n",
        "\n",
        "**Architectures**: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and adding an additional dense layer; do not mix these variantions.\n",
        "\n",
        "\n",
        "**Training and Experiments**: all the experiments must involve only the training and validation sets.\n",
        "\n",
        "**Evaluation**: in the end, only the two best models of your choice (according to the validation set) must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech. DO NOT CONSIDER THE PUNCTUATION CLASSES.\n",
        "\n",
        "**Metrics**: the metric you must use to evaluate your final model is the F1-macro, WITHOUT considering punctuation/symbols classes; during the training process you can use accuracy because you can't use the F1 metric unless you use a single (gigantic) batch because there is no way to aggregate \"partial\" F1 scores computed on mini-batches.\n",
        "\n",
        "**Discussion and Error Analysis** : verify and discuss if the results on the test sets are coherent with those on the validation set; analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
        "\n",
        "**Report**: you are asked to deliver the code of your experiments and a small pdf report of about 2 pages; the pdf must begin with the names of the people of your team and a small abstract (4-5 lines) that sums up your findings.\n",
        "\n",
        "# Out Of Vocabulary (OOV) terms\n",
        "\n",
        "How to handle words that are not in GloVe vocabulary?\n",
        "You can handle them as you want (random embedding, placeholder, whatever!), but they must be STATIC embeddings (you cannot train them).\n",
        "\n",
        "But there is a very important caveat! As usual, the element of the test set must not influence the elements of the other splits!\n",
        "\n",
        "So, when you compute new embeddings for train+validation, you must forget about test documents.\n",
        "The motivation is to emulate a real-world scenario, where you select and train a model in the first stage, without knowing nothing about the testing environment.\n",
        "\n",
        "For implementation convenience, you CAN use a single vocabulary file/matrix/whatever. The principle of the previous point is that the embeddings inside that file/matrix must be generated independently for train and test splits.\n",
        "\n",
        "Basically in a real-world scenario, this is what would happen:\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Training of the model(s)\n",
        "5. Compute embeddings for terms OOV2 of the validation split \n",
        "6. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "7. Validation of the model(s)\n",
        "8. Compute embeddings for terms OOV3 of the test split \n",
        "9. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2+OOV3\n",
        "10. Testing of the final model\n",
        "\n",
        "In this case, where we already have all the documents, we can simplify the process a bit, but the procedure must remain rigorous.\n",
        "\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Compute embeddings for terms OOV2 of the validation split \n",
        "5. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "6. Compute embeddings for terms OOV3 of the test split \n",
        "7. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2\n",
        "8. Training of the model(s)\n",
        "9. Validation of the model(s)\n",
        "10. Testing of the final model\n",
        "\n",
        "Step 2 and step 6 must be completely independent of each other, for what concerns the method and the documents. But they can rely on the previous vocabulary (V1 for step 2 and V3 for step 6)\n",
        "THEREFORE if a word is present both in the training set and the test split and not in the starting vocabulary, its embedding is computed in step 2) and it is not considered OOV anymore in step 6).\n",
        "\n",
        "# Report\n",
        "The report must not be just a copy and paste of graphs and tables!\n",
        "\n",
        "The report must not be longer than 2 pages and must contain:\n",
        "* The names of the member of your team\n",
        "* A short abstract (4-5 lines) that sum ups everything\n",
        "* A general description of the task you have addressed and how you have addressed it\n",
        "* A short description of the models you have used\n",
        "* Some tables that sum up your findings in validation and test and a discussion of those results\n",
        "* The most relevant findings of your error analysis\n",
        "\n",
        "# Evaluation Criterion\n",
        "\n",
        "The goal of this assignment is not to prove you can find best model ever, but to face a common task, structure it correctly, and follow a correct and rigorous experimental procedure.\n",
        "In other words, we don't care if you final models are awful as long as you have followed the correct procedure and wrote a decent report.\n",
        "\n",
        "The score of the assignment will be computed roughly as follows\n",
        "* 1 point for the general setting of the problem\n",
        "* 1 point for the handling of OOV terms\n",
        "* 1 point for the models\n",
        "* 1 point for train-validation-test procedure\n",
        "* 2 point for the discussion of the results, error analysis, and report\n",
        "\n",
        "This distribution of scores is tentative and we may decide to alter it at any moment.\n",
        "We also reserve the right to assign a small bonus (0.5 points) to any assignment that is particularly worthy. Similarly, in case of grave errors, we may decide to assign an equivalent malus (-0.5 points).\n",
        "\n",
        "# Contacts\n",
        "\n",
        "In case of any doubt, question, issue, or help we highly recommend you to check the [course useful material](https://virtuale.unibo.it/pluginfile.php/1036039/mod_resource/content/2/NLP_Course_Useful_Material.pdf) for additional information, and to use the Virtuale forums to discuss with other students.\n",
        "\n",
        "You can always contact us at the following email addresses. To increase the probability of a prompt response, we reccomend you to write to both the teaching assistants.\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Andrea Galassi -> a.galassi@unibo.it\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it\n",
        "\n",
        "\n",
        "# FAQ\n",
        "* You can use a non-trainable Embedding layer to load the glove embeddings\n",
        "* You can use any library of your choice to implement the networks. Two options are tensorflow/keras or pythorch. Both these libraries have all the classes you need to implement these simple architectures and there are plenty of tutorials around, where you can learn how to use them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usG1eafJvaQ4"
      },
      "source": [
        "# Preparation of the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iUsOu-UvaQ4"
      },
      "source": [
        "## Import the basic libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emdubW66fnlD"
      },
      "outputs": [],
      "source": [
        "import os, shutil  #  file management\n",
        "import sys \n",
        "import pandas as pd  #  dataframe management\n",
        "import numpy as np  #  data manipulation\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request  #  download files\n",
        "import zipfile  #  unzip files\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6B-w2pqyfpF1"
      },
      "outputs": [],
      "source": [
        "from keras import metrics \n",
        "from tensorflow.python.keras.metrics import Metric\n",
        "from sklearn.metrics import f1_score\n",
        "import keras.backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFvajV2sni19",
        "outputId": "af4e2966-dbbe-4f81-fc66-021f6fdeaee3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3qoNG45vaQ6"
      },
      "source": [
        "## Download the dataset\n",
        "Use the Dependency Treebank corpora from nltk. Download and extract it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3xn1MZPgA_v",
        "outputId": "6d7883e7-8c5f-4c56-9186-30af2a28b6c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful download\n",
            "Successful extraction\n"
          ]
        }
      ],
      "source": [
        "dataset_folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\")\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "    os.makedirs(dataset_folder)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
        "\n",
        "dataset_path = os.path.join(dataset_folder, \"treebank.zip\")\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    urllib.request.urlretrieve(url, dataset_path)\n",
        "    print(\"Successful download\")\n",
        "\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_folder)\n",
        "print(\"Successful extraction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiO1v6SJmm37"
      },
      "source": [
        "# Pre Processing\n",
        "Reorder files and split train, validation and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKbUsrVHhGAG"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"dependency_treebank\"\n",
        "\n",
        "\n",
        "folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\", dataset_name)\n",
        "\n",
        "pre_train = []\n",
        "pre_valid = []\n",
        "pre_test = []\n",
        "i = 1\n",
        "\n",
        "file_list = sorted(os.listdir(folder))\n",
        "\n",
        "for filename in file_list:\n",
        "  file_path = os.path.join(folder, filename)\n",
        "  if os.path.isfile(file_path):\n",
        "    # open the file\n",
        "      text = []\n",
        "      with open(file_path, mode='r', encoding='utf-8') as text_file:\n",
        "        text = text_file.read()\n",
        "        if i <= 100: # from 1 to 100 train set\n",
        "          pre_train.append(text)\n",
        "        elif i <= 150: # from 101 to 150 validation set\n",
        "          pre_valid.append(text)\n",
        "        else: # remainings are test set\n",
        "          pre_test.append(text)  \n",
        "  i+=1\n",
        "\n",
        "tmp_train = []\n",
        "tmp_valid = []\n",
        "tmp_test = []\n",
        "\n",
        "# split the contents of the files for the character '\\n'\n",
        "for paragraph in pre_train:\n",
        "   tmp_train.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_valid:\n",
        "   tmp_valid.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_test:\n",
        "   tmp_test.append(paragraph.split('\\n'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split each word with its tag"
      ],
      "metadata": {
        "id": "AR4rlQo5wL-H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUgbTXi-F7Sa"
      },
      "outputs": [],
      "source": [
        "train = []\n",
        "valid = []\n",
        "test = []\n",
        "\n",
        "for i in tmp_train:\n",
        "  for j in i:\n",
        "    train.append(j.split('\\t'))\n",
        "\n",
        "for i in tmp_valid:\n",
        "  for j in i:\n",
        "    valid.append(j.split('\\t'))\n",
        "\n",
        "for i in tmp_test:\n",
        "  for j in i:\n",
        "    test.append(j.split('\\t'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the sentences by their respective tags by creating two lists of lists.\n",
        "\n",
        "One contains lists of words (which form sentences) and one contains lists of tags that refer to individual words"
      ],
      "metadata": {
        "id": "KB50HMDn18NX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmzVaRuQFp0M"
      },
      "outputs": [],
      "source": [
        "train_sentences = []\n",
        "valid_sentences = []\n",
        "test_sentences = []\n",
        "\n",
        "train_tags = []\n",
        "valid_tags = []\n",
        "test_tags = []\n",
        "\n",
        "tmp_sentences = []\n",
        "tmp_tags = []\n",
        "for i in train:\n",
        "  if i[0] != '':\n",
        "    tmp_sentences.append(i[0])\n",
        "    tmp_tags.append(i[1])\n",
        "  else:\n",
        "    train_sentences.append(tmp_sentences)\n",
        "    train_tags.append(tmp_tags)\n",
        "    tmp_sentences = []\n",
        "    tmp_tags = []\n",
        "\n",
        "tmp_sentences = []\n",
        "tmp_tags = []\n",
        "for i in valid:\n",
        "  if i[0] != '':\n",
        "    tmp_sentences.append(i[0])\n",
        "    tmp_tags.append(i[1])\n",
        "  else:\n",
        "    valid_sentences.append(tmp_sentences)\n",
        "    valid_tags.append(tmp_tags)\n",
        "    tmp_sentences = []\n",
        "    tmp_tags = []\n",
        "\n",
        "tmp_sentences = []\n",
        "tmp_tags = []\n",
        "for i in test:\n",
        "  if i[0] != '':\n",
        "    tmp_sentences.append(i[0])\n",
        "    tmp_tags.append(i[1])\n",
        "  else:\n",
        "    test_sentences.append(tmp_sentences)\n",
        "    test_tags.append(tmp_tags)\n",
        "    tmp_sentences = []\n",
        "    tmp_tags = []"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We flatten the lists so that we have a 3 simple lists with the words in order"
      ],
      "metadata": {
        "id": "f8hhGpCK2yS7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tB65cUFLIeEr"
      },
      "outputs": [],
      "source": [
        "flat_train = [item for sublist in train_sentences for item in sublist]\n",
        "flat_valid = [item for sublist in valid_sentences for item in sublist]\n",
        "flat_test = [item for sublist in test_sentences for item in sublist]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4IenRMamtLg"
      },
      "source": [
        "# Tokenization\n",
        "We create the vocabulary index based on word frequency and then we take each word in the text and replaces it with its corresponding integer value from the dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZO6ga1EGHdh"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "train_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "train_tokenizer.fit_on_texts(train_sentences)                    # fit tokeniser on data\n",
        "train_encoded = train_tokenizer.texts_to_sequences(train_sentences)\n",
        "\n",
        "valid_tokenizer = Tokenizer()       \n",
        "valid_tokenizer.fit_on_texts(valid_sentences)                  # instantiate tokeniser\n",
        "valid_encoded = valid_tokenizer.texts_to_sequences(valid_sentences)\n",
        "\n",
        "test_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "test_tokenizer.fit_on_texts(test_sentences)                    # fit tokeniser on data\n",
        "test_encoded = test_tokenizer.texts_to_sequences(test_sentences)  # use the tokeniser to encode input sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSvhz4IvmziU"
      },
      "outputs": [],
      "source": [
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(train_tags)\n",
        "tags_encoded = tag_tokenizer.texts_to_sequences(train_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We pick the differences between the 3 vocabularies and we concatenate them"
      ],
      "metadata": {
        "id": "IYaZwQfqWoDq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMsG0Yw1l50L"
      },
      "outputs": [],
      "source": [
        "vocabulary = list(set(train_tokenizer.word_index.keys()))\n",
        "vocabulary += list(set(valid_tokenizer.word_index.keys()) - \n",
        "                   set(train_tokenizer.word_index.keys()))\n",
        "vocabulary += list(set(test_tokenizer.word_index.keys()) - set(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dS9RwnOtJuas"
      },
      "outputs": [],
      "source": [
        "word_index = dict(zip(vocabulary, range(2, len(vocabulary)+3)))\n",
        "word_index['-PAD-'] = 0\n",
        "word_index['-OOV-'] = 1\n",
        "\n",
        "tags = set([item for sublist in train_tags for item in sublist])\n",
        "\n",
        "tag2index = {t: i + 1 for i, t in enumerate(tags)}\n",
        "tag2index['-PAD-'] = 0 "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and setup the GloVe Model"
      ],
      "metadata": {
        "id": "it-cE6dfR6Lj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoj4Zx51J2MU",
        "outputId": "3af57ce3-4b1a-41b8-e209-058afd6d4877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-20 15:51:51--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-12-20 15:51:51--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-12-20 15:51:51--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  5.08MB/s    in 2m 40s  \n",
            "\n",
            "2021-12-20 15:54:31 (5.15 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc-T6FcLKtS9",
        "outputId": "775f7637-4a81-401d-fab5-bebd5fb56951"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "path_to_glove_file = os.path.join(os.getcwd(), \"glove.6B.100d.txt\")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2IN3Mh-m85T"
      },
      "source": [
        "# Embedding\n",
        "- padding\n",
        "- one hot encoding of tags\n",
        "- punctuation identification\n",
        "\n",
        "Create the embedding matrix which maps each word index to its GloVe embedding (100 dimensions).\n",
        "\n",
        "We had to consider what to do with the words that were not present in the downloaded embeddings: we chose to map them randomly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0mOStEPMeXt",
        "outputId": "88840aed-c85c-4949-98d4-2eb795130537"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 10271 words (678 misses)\n"
          ]
        }
      ],
      "source": [
        "num_tokens = len(vocabulary) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        embedding_matrix += np.random.uniform(low=-0.05,\n",
        "                                              high=0.05,\n",
        "                                              size=embedding_dim)\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup the embedding layer and the maximum length based on the sentences of each set"
      ],
      "metadata": {
        "id": "8aVXEf0skeQa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpYrHs0RMtQJ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8EZM31Js06-"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = len(max(train_sentences, key=len))\n",
        "MAX_LENGTH2 = len(max(valid_sentences, key=len))\n",
        "MAX_LENGTH3 = len(max(test_sentences, key=len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSRa9SwTuNrX"
      },
      "outputs": [],
      "source": [
        "train_sentences_X, valid_sentences_X, train_tags_y, valid_tags_y, = [], [], [], []\n",
        " \n",
        "for sentence in train_sentences:\n",
        "    sentence_int = []\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            sentence_int.append(word_index[word.lower()])\n",
        "        except KeyError:\n",
        "            sentence_int.append(word_index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(sentence_int)\n",
        "\n",
        "for sentence in valid_sentences:\n",
        "    sentence_int = []\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            sentence_int.append(word_index[word.lower()])\n",
        "        except KeyError:\n",
        "            sentence_int.append(word_index['-OOV-'])\n",
        " \n",
        "    valid_sentences_X.append(sentence_int)\n",
        "\n",
        "\n",
        "for sentence in train_tags:\n",
        "    train_tags_y.append([tag2index[tag] for tag in sentence])\n",
        "\n",
        "for sentence in valid_tags:\n",
        "    valid_tags_y.append([tag2index[tag] for tag in sentence])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padded the sentences to the length of the longest one present in the train split (MAX_LENGTH)"
      ],
      "metadata": {
        "id": "kzvPLh_Snn4V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf-XlSrStQx3"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_sentences_X = pad_sequences(valid_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_tags_y = pad_sequences(valid_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the method to categorize and map tags into integer indexes"
      ],
      "metadata": {
        "id": "IxzClSXao14z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUtevCQrnskt"
      },
      "outputs": [],
      "source": [
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define categories for punctuation, symbols and padding"
      ],
      "metadata": {
        "id": "cYoq3Ui5pT9N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT8PjDIynuHG"
      },
      "outputs": [],
      "source": [
        "point = [tag2index['.']]\n",
        "virg = [tag2index[',']]\n",
        "weird_apex = [tag2index['``']]\n",
        "single_apex = [tag2index[\"''\"]]\n",
        "two_dots = [tag2index[':']]\n",
        "pad = [tag2index['-PAD-']]\n",
        "\n",
        "punct_cat_classes = to_categorical([point, virg, weird_apex, single_apex, two_dots, pad], len(tag2index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAqZ0Xkrnw_d",
        "outputId": "d58cf31a-ef92-4dd5-a407-20e0e279eb2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "cumulative_tags = np.zeros(46)\n",
        "for i in punct_cat_classes:\n",
        "  print(i[0])\n",
        "  cumulative_tags += i[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzJNfc1xn7jA"
      },
      "outputs": [],
      "source": [
        "where_tags = np.where(np.logical_not(cumulative_tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrzwDP5Wn9TK"
      },
      "outputs": [],
      "source": [
        "no_punct_indexes = where_tags[0]\n",
        "n_classes = len(no_punct_indexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik32ea9dn-5m"
      },
      "outputs": [],
      "source": [
        "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "cat_val_tags_y = to_categorical(valid_tags_y, len(tag2index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbT_r953oBA5"
      },
      "source": [
        "#Mini Data Vizualization Inset\n",
        "Observe the distribution of the tags and their frequency\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPiOzlwroPuJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIyRpFGVoKlH"
      },
      "outputs": [],
      "source": [
        "distribution_tags = np.zeros(46)\n",
        "for i in cat_train_tags_y:\n",
        "  for t in i:\n",
        "    distribution_tags += t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAbGJViloL0O"
      },
      "outputs": [],
      "source": [
        "s = sum(distribution_tags[1:])\n",
        "norm = [float(i)/s for i in distribution_tags[1:]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "px2v7JyxoQrz",
        "outputId": "c2ed0df8-f8a7-4986-9655-0689620b8681"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.bar(np.arange(len(norm)),norm)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0m_PqWUoRsV"
      },
      "source": [
        "#F1 Metric\n",
        "Definition of the methods to calculate the F1 Metric\n",
        "\n",
        "##Precision metric.\n",
        "```\n",
        "Only computes a batch-wise average of precision.\n",
        "Computes the precision, a metric for multi-label classification of\n",
        "how many selected items are relevant.\n",
        "```\n",
        "\n",
        "##Recall metric.\n",
        "\n",
        "```\n",
        "Only computes a batch-wise average of recall.\n",
        "Computes the recall, a metric for multi-label classification of\n",
        "how many relevant items are selected.\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDaBrpk3oYXO"
      },
      "outputs": [],
      "source": [
        "def precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def f1_binary(y_true, y_pred):\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
        "\n",
        "def metric_wrapper(index):\n",
        "  def f1_for_class(y_true, y_pred):\n",
        "    #get only the desired class\n",
        "    true = y_true[:,:,index]\n",
        "    pred = y_pred[:,:,index]\n",
        "    #return dice per class\n",
        "    tmp = f1_binary(true,pred)\n",
        "    return tmp\n",
        "  f1_for_class.__name__ = 'f1_' + str(index)\n",
        "  return f1_for_class\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    result = 0.0\n",
        "    for class_id in no_punct_indexes:\n",
        "        y_true_single_class = y_true[:,:,class_id]\n",
        "        y_pred_single_class = y_pred[:,:,class_id]\n",
        "        f1_single = f1_binary(y_true_single_class, y_pred_single_class)\n",
        "        result += f1_single / float(n_classes)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQz388M4oaCW"
      },
      "source": [
        "#Model compile and fit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of the model, in this case we're using:\n",
        "\n",
        "*   Input Layer\n",
        "*   Embedding Layer\n",
        "*   Bidirectional LSTM with 256 units\n",
        "*   Two dense layer\n",
        "*   Activation layer with softmax activation function\n",
        "\n",
        "The model is compiled using *categorical_crossentropy* as loss,\n",
        "*Adam(0.001)* optimizer, *accuracy* and *macro_f1* as metrics.\n",
        "\n",
        "Then the trainingis done for 40 epochs on a batch size of 128.\n"
      ],
      "metadata": {
        "id": "sfli4cpdt1Qh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsVGByW_sdec",
        "outputId": "9fc478f3-0c69-4290-cc9f-60ff6187277e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 249, 100)          1094900   \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 249, 512)         731136    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 249, 46)           23598     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 249, 46)           2162      \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,851,796\n",
            "Trainable params: 756,896\n",
            "Non-trainable params: 1,094,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        " \n",
        "\n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(embedding_layer)\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(Dense(len(tag2index)))\n",
        "model.add(Dense(len(tag2index)))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=Adam(0.001),\n",
        "             metrics=['accuracy', f1, [metric_wrapper(i) for i in no_punct_indexes]])\n",
        " \n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWIokkJwokrV",
        "outputId": "7e360e2e-46cd-4d77-b0b0-75f17cf5065d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "16/16 [==============================] - 88s 5s/step - loss: 0.7744 - accuracy: 0.8467 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - val_loss: 0.3482 - val_accuracy: 0.9167 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - 68s 4s/step - loss: 0.3196 - accuracy: 0.9130 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - val_loss: 0.3008 - val_accuracy: 0.9182 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - 67s 4s/step - loss: 0.2916 - accuracy: 0.9214 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - val_loss: 0.2807 - val_accuracy: 0.9276 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - 68s 4s/step - loss: 0.2716 - accuracy: 0.9311 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - val_loss: 0.2622 - val_accuracy: 0.9378 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - 67s 4s/step - loss: 0.2525 - accuracy: 0.9392 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - val_loss: 0.2431 - val_accuracy: 0.9416 - val_f1: 3.0102e-05 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0012 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - 68s 4s/step - loss: 0.2320 - accuracy: 0.9434 - f1: 0.0018 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0712 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - val_loss: 0.2221 - val_accuracy: 0.9443 - val_f1: 0.0024 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0879 - val_f1_10: 0.0071 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - 68s 4s/step - loss: 0.2100 - accuracy: 0.9460 - f1: 0.0156 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.3143 - f1_10: 0.3015 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0071 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0015 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - val_loss: 0.2003 - val_accuracy: 0.9459 - val_f1: 0.0274 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.2609 - val_f1_10: 0.7930 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0328 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0085 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - 67s 4s/step - loss: 0.1873 - accuracy: 0.9497 - f1: 0.0413 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.4709 - f1_10: 0.8398 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.3323 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0098 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - val_loss: 0.1781 - val_accuracy: 0.9517 - val_f1: 0.0488 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0110 - val_f1_9: 0.4363 - val_f1_10: 0.8938 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.5355 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0750 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - 68s 4s/step - loss: 0.1660 - accuracy: 0.9550 - f1: 0.0577 - f1_1: 0.0000e+00 - f1_2: 0.0217 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0646 - f1_9: 0.5600 - f1_10: 0.8988 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0126 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.6348 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.1139 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - val_loss: 0.1589 - val_accuracy: 0.9579 - val_f1: 0.0710 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0913 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.2493 - val_f1_9: 0.5117 - val_f1_10: 0.8964 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.2400 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.6562 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.1965 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - 68s 4s/step - loss: 0.1474 - accuracy: 0.9610 - f1: 0.0927 - f1_1: 0.0000e+00 - f1_2: 0.1466 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.3369 - f1_9: 0.6248 - f1_10: 0.8975 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.6075 - f1_14: 0.0000e+00 - f1_15: 0.0053 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.7419 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.2940 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0540 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - val_loss: 0.1428 - val_accuracy: 0.9625 - val_f1: 0.1070 - val_f1_1: 0.0000e+00 - val_f1_2: 0.1877 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.4823 - val_f1_9: 0.5988 - val_f1_10: 0.9010 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.7797 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0181 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.7708 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.3695 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.1715 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - 69s 4s/step - loss: 0.1319 - accuracy: 0.9657 - f1: 0.1304 - f1_1: 0.0000e+00 - f1_2: 0.3303 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.4887 - f1_9: 0.6610 - f1_10: 0.9057 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9327 - f1_14: 0.0000e+00 - f1_15: 0.0721 - f1_16: 0.0046 - f1_17: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.8036 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.4280 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.5894 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - val_loss: 0.1293 - val_accuracy: 0.9662 - val_f1: 0.1373 - val_f1_1: 0.0000e+00 - val_f1_2: 0.3172 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.6133 - val_f1_9: 0.6398 - val_f1_10: 0.9085 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.9554 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0706 - val_f1_16: 0.0287 - val_f1_17: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.8320 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.4582 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.6642 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0059 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - 67s 4s/step - loss: 0.1192 - accuracy: 0.9694 - f1: 0.1606 - f1_1: 0.0000e+00 - f1_2: 0.4458 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.5781 - f1_9: 0.6872 - f1_10: 0.9113 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9906 - f1_14: 0.0000e+00 - f1_15: 0.1791 - f1_16: 0.4165 - f1_17: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.8455 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.5096 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.7892 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0250 - f1_43: 0.0000e+00 - f1_44: 0.0447 - val_loss: 0.1181 - val_accuracy: 0.9684 - val_f1: 0.1749 - val_f1_1: 0.0000e+00 - val_f1_2: 0.3552 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.6570 - val_f1_9: 0.6543 - val_f1_10: 0.9121 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.9988 - val_f1_14: 0.0000e+00 - val_f1_15: 0.1626 - val_f1_16: 0.7910 - val_f1_17: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.8552 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.5782 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.1225 - val_f1_38: 0.0000e+00 - val_f1_39: 0.7681 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0459 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0941\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - 67s 4s/step - loss: 0.1085 - accuracy: 0.9719 - f1: 0.2012 - f1_1: 0.0000e+00 - f1_2: 0.5235 - f1_3: 0.0000e+00 - f1_4: 0.0266 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.6427 - f1_9: 0.7130 - f1_10: 0.9119 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9986 - f1_14: 0.0000e+00 - f1_15: 0.2937 - f1_16: 0.9004 - f1_17: 0.0000e+00 - f1_19: 0.0062 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.8675 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.5777 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.4306 - f1_38: 0.0000e+00 - f1_39: 0.8264 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.1278 - f1_43: 0.0000e+00 - f1_44: 0.2012 - val_loss: 0.1087 - val_accuracy: 0.9706 - val_f1: 0.2171 - val_f1_1: 0.0000e+00 - val_f1_2: 0.4676 - val_f1_3: 0.0023 - val_f1_4: 0.0934 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.6762 - val_f1_9: 0.6761 - val_f1_10: 0.9155 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.3306 - val_f1_16: 0.9952 - val_f1_17: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.8717 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.6217 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.8275 - val_f1_38: 0.0000e+00 - val_f1_39: 0.7752 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.2026 - val_f1_43: 0.0000e+00 - val_f1_44: 0.2295\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - 69s 4s/step - loss: 0.0995 - accuracy: 0.9739 - f1: 0.2338 - f1_1: 0.0000e+00 - f1_2: 0.6062 - f1_3: 0.0037 - f1_4: 0.1689 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.6938 - f1_9: 0.7329 - f1_10: 0.9156 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9987 - f1_14: 0.0000e+00 - f1_15: 0.3809 - f1_16: 0.9832 - f1_17: 0.0000e+00 - f1_19: 0.0178 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.8831 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.6244 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.8116 - f1_38: 0.0000e+00 - f1_39: 0.8324 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.3324 - f1_43: 0.0000e+00 - f1_44: 0.3675 - val_loss: 0.1003 - val_accuracy: 0.9729 - val_f1: 0.2407 - val_f1_1: 0.0000e+00 - val_f1_2: 0.5512 - val_f1_3: 0.0494 - val_f1_4: 0.3452 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.7262 - val_f1_9: 0.6793 - val_f1_10: 0.9160 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.3443 - val_f1_16: 0.9952 - val_f1_17: 0.0000e+00 - val_f1_19: 0.0477 - val_f1_20: 0.0038 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.8845 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.6341 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.8852 - val_f1_38: 0.0119 - val_f1_39: 0.7762 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.3486 - val_f1_43: 0.0000e+00 - val_f1_44: 0.4298\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - 67s 4s/step - loss: 0.0920 - accuracy: 0.9758 - f1: 0.2629 - f1_1: 0.0000e+00 - f1_2: 0.6501 - f1_3: 0.0558 - f1_4: 0.4087 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.7272 - f1_9: 0.7538 - f1_10: 0.9194 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9987 - f1_14: 0.0000e+00 - f1_15: 0.4451 - f1_16: 0.9934 - f1_17: 0.0000e+00 - f1_19: 0.1274 - f1_20: 0.0040 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.8916 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.6612 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.8902 - f1_38: 0.1671 - f1_39: 0.8339 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.4651 - f1_43: 0.0000e+00 - f1_44: 0.5217 - val_loss: 0.0941 - val_accuracy: 0.9743 - val_f1: 0.2747 - val_f1_1: 0.0000e+00 - val_f1_2: 0.5455 - val_f1_3: 0.1343 - val_f1_4: 0.6191 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.7343 - val_f1_9: 0.6869 - val_f1_10: 0.9183 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.3953 - val_f1_16: 0.9975 - val_f1_17: 0.0017 - val_f1_19: 0.3543 - val_f1_20: 0.0430 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.8891 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0557 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.6821 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.8926 - val_f1_38: 0.2046 - val_f1_39: 0.7862 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.5099 - val_f1_43: 0.0000e+00 - val_f1_44: 0.5384\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - 68s 4s/step - loss: 0.0857 - accuracy: 0.9776 - f1: 0.2926 - f1_1: 0.0000e+00 - f1_2: 0.6838 - f1_3: 0.1242 - f1_4: 0.6155 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.7432 - f1_9: 0.7658 - f1_10: 0.9260 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9980 - f1_14: 0.0000e+00 - f1_15: 0.4678 - f1_16: 0.9942 - f1_17: 0.0012 - f1_19: 0.3536 - f1_20: 0.0933 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.8981 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.1558 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.6920 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9048 - f1_38: 0.2757 - f1_39: 0.8446 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.5553 - f1_43: 0.0000e+00 - f1_44: 0.6121 - val_loss: 0.0883 - val_accuracy: 0.9760 - val_f1: 0.3032 - val_f1_1: 0.0000e+00 - val_f1_2: 0.6421 - val_f1_3: 0.2515 - val_f1_4: 0.6406 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.7611 - val_f1_9: 0.7153 - val_f1_10: 0.9256 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.4508 - val_f1_16: 0.9975 - val_f1_17: 0.0121 - val_f1_19: 0.6224 - val_f1_20: 0.1688 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.8909 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.1769 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.6545 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9075 - val_f1_38: 0.3519 - val_f1_39: 0.8089 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.5237 - val_f1_43: 0.0000e+00 - val_f1_44: 0.6270\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - 67s 4s/step - loss: 0.0806 - accuracy: 0.9789 - f1: 0.3226 - f1_1: 0.0000e+00 - f1_2: 0.7050 - f1_3: 0.2203 - f1_4: 0.7196 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.7723 - f1_9: 0.7766 - f1_10: 0.9300 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9985 - f1_14: 0.0000e+00 - f1_15: 0.4989 - f1_16: 0.9955 - f1_17: 0.0077 - f1_19: 0.5964 - f1_20: 0.2608 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.9070 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.4140 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.7052 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9052 - f1_38: 0.3736 - f1_39: 0.8588 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.5863 - f1_43: 0.0000e+00 - f1_44: 0.6736 - val_loss: 0.0832 - val_accuracy: 0.9774 - val_f1: 0.3267 - val_f1_1: 0.0000e+00 - val_f1_2: 0.6277 - val_f1_3: 0.2771 - val_f1_4: 0.7399 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.7720 - val_f1_9: 0.7097 - val_f1_10: 0.9311 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.4949 - val_f1_16: 0.9975 - val_f1_17: 0.0234 - val_f1_19: 0.6175 - val_f1_20: 0.2428 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.9100 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.5638 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.7212 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9148 - val_f1_38: 0.4676 - val_f1_39: 0.8299 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.5836 - val_f1_43: 0.0000e+00 - val_f1_44: 0.6453\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - 67s 4s/step - loss: 0.0756 - accuracy: 0.9802 - f1: 0.3441 - f1_1: 0.0000e+00 - f1_2: 0.7405 - f1_3: 0.2966 - f1_4: 0.7748 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.7809 - f1_9: 0.7861 - f1_10: 0.9324 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9978 - f1_14: 0.0000e+00 - f1_15: 0.5359 - f1_16: 0.9939 - f1_17: 0.0413 - f1_19: 0.6807 - f1_20: 0.3365 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.9119 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.7052 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.7248 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9050 - f1_38: 0.4009 - f1_39: 0.8667 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.6342 - f1_43: 0.0000e+00 - f1_44: 0.7165 - val_loss: 0.0785 - val_accuracy: 0.9786 - val_f1: 0.3429 - val_f1_1: 0.0000e+00 - val_f1_2: 0.6680 - val_f1_3: 0.3635 - val_f1_4: 0.7817 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.8068 - val_f1_9: 0.7281 - val_f1_10: 0.9383 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.4904 - val_f1_16: 0.9975 - val_f1_17: 0.0816 - val_f1_19: 0.7048 - val_f1_20: 0.3027 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.9155 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.6681 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.7287 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9254 - val_f1_38: 0.4711 - val_f1_39: 0.8436 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.6034 - val_f1_43: 0.0000e+00 - val_f1_44: 0.6978\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - 67s 4s/step - loss: 0.0714 - accuracy: 0.9811 - f1: 0.3609 - f1_1: 0.0000e+00 - f1_2: 0.7531 - f1_3: 0.3743 - f1_4: 0.8024 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.8077 - f1_9: 0.7992 - f1_10: 0.9383 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9986 - f1_14: 0.0000e+00 - f1_15: 0.5581 - f1_16: 0.9915 - f1_17: 0.0878 - f1_19: 0.7416 - f1_20: 0.4296 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.9185 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.8447 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.7481 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9141 - f1_38: 0.4482 - f1_39: 0.8778 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.6525 - f1_43: 0.0000e+00 - f1_44: 0.7510 - val_loss: 0.0750 - val_accuracy: 0.9797 - val_f1: 0.3630 - val_f1_1: 0.0213 - val_f1_2: 0.6872 - val_f1_3: 0.4540 - val_f1_4: 0.8227 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.8142 - val_f1_9: 0.7203 - val_f1_10: 0.9424 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.5583 - val_f1_16: 0.9975 - val_f1_17: 0.1198 - val_f1_19: 0.7152 - val_f1_20: 0.4887 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.9199 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.8260 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.7371 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9356 - val_f1_38: 0.5603 - val_f1_39: 0.8534 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.6184 - val_f1_43: 0.0000e+00 - val_f1_44: 0.7273\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - 67s 4s/step - loss: 0.0676 - accuracy: 0.9821 - f1: 0.3754 - f1_1: 0.0163 - f1_2: 0.7642 - f1_3: 0.4371 - f1_4: 0.8376 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.8230 - f1_9: 0.8059 - f1_10: 0.9430 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9986 - f1_14: 0.0000e+00 - f1_15: 0.5849 - f1_16: 0.9906 - f1_17: 0.1529 - f1_19: 0.7620 - f1_20: 0.5533 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.9256 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.9004 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.7587 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9136 - f1_38: 0.4968 - f1_39: 0.8810 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.6965 - f1_43: 0.0000e+00 - f1_44: 0.7758 - val_loss: 0.0715 - val_accuracy: 0.9805 - val_f1: 0.3731 - val_f1_1: 0.0521 - val_f1_2: 0.7073 - val_f1_3: 0.4863 - val_f1_4: 0.8355 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.8299 - val_f1_9: 0.7362 - val_f1_10: 0.9465 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.5429 - val_f1_16: 0.9975 - val_f1_17: 0.1251 - val_f1_19: 0.7594 - val_f1_20: 0.5879 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.9237 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.8680 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.7682 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9300 - val_f1_38: 0.5975 - val_f1_39: 0.8601 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.6283 - val_f1_43: 0.0000e+00 - val_f1_44: 0.7405\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - 68s 4s/step - loss: 0.0645 - accuracy: 0.9828 - f1: 0.3858 - f1_1: 0.0373 - f1_2: 0.7807 - f1_3: 0.4755 - f1_4: 0.8488 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.8430 - f1_9: 0.8116 - f1_10: 0.9468 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9986 - f1_14: 0.0000e+00 - f1_15: 0.5970 - f1_16: 0.9923 - f1_17: 0.2196 - f1_19: 0.7975 - f1_20: 0.6080 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.9290 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.9289 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.7710 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9228 - f1_38: 0.5169 - f1_39: 0.8969 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.7160 - f1_43: 0.0000e+00 - f1_44: 0.7918 - val_loss: 0.0685 - val_accuracy: 0.9815 - val_f1: 0.3843 - val_f1_1: 0.1122 - val_f1_2: 0.7109 - val_f1_3: 0.4925 - val_f1_4: 0.8576 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.8335 - val_f1_9: 0.7534 - val_f1_10: 0.9496 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.5735 - val_f1_16: 0.9975 - val_f1_17: 0.1994 - val_f1_19: 0.7933 - val_f1_20: 0.6349 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.9269 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.9068 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.7730 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9321 - val_f1_38: 0.6090 - val_f1_39: 0.8817 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.6876 - val_f1_43: 0.0000e+00 - val_f1_44: 0.7482\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - 67s 4s/step - loss: 0.0617 - accuracy: 0.9835 - f1: 0.3974 - f1_1: 0.0996 - f1_2: 0.7964 - f1_3: 0.5090 - f1_4: 0.8638 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.8471 - f1_9: 0.8134 - f1_10: 0.9500 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9980 - f1_14: 0.0000e+00 - f1_15: 0.6131 - f1_16: 0.9927 - f1_17: 0.2750 - f1_19: 0.8343 - f1_20: 0.7254 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.9333 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.9330 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.7827 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9290 - f1_38: 0.5375 - f1_39: 0.9140 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.7452 - f1_43: 0.0000e+00 - f1_44: 0.8036 - val_loss: 0.0663 - val_accuracy: 0.9821 - val_f1: 0.3928 - val_f1_1: 0.1552 - val_f1_2: 0.7084 - val_f1_3: 0.5643 - val_f1_4: 0.8677 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.8553 - val_f1_9: 0.7404 - val_f1_10: 0.9544 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6010 - val_f1_16: 0.9975 - val_f1_17: 0.2444 - val_f1_19: 0.8363 - val_f1_20: 0.6544 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.9311 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.9088 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.7809 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9468 - val_f1_38: 0.6055 - val_f1_39: 0.8874 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.7051 - val_f1_43: 0.0000e+00 - val_f1_44: 0.7681\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - 68s 4s/step - loss: 0.0593 - accuracy: 0.9842 - f1: 0.4061 - f1_1: 0.1310 - f1_2: 0.7952 - f1_3: 0.5673 - f1_4: 0.8798 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.8629 - f1_9: 0.8140 - f1_10: 0.9544 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9986 - f1_14: 0.0000e+00 - f1_15: 0.6370 - f1_16: 0.9940 - f1_17: 0.3281 - f1_19: 0.8426 - f1_20: 0.7852 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.9352 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.9389 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.7882 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9364 - f1_38: 0.5645 - f1_39: 0.9180 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.7530 - f1_43: 0.0000e+00 - f1_44: 0.8200 - val_loss: 0.0634 - val_accuracy: 0.9827 - val_f1: 0.3990 - val_f1_1: 0.2161 - val_f1_2: 0.7372 - val_f1_3: 0.5132 - val_f1_4: 0.8639 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.8466 - val_f1_9: 0.7745 - val_f1_10: 0.9562 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.5585 - val_f1_16: 0.9975 - val_f1_17: 0.2388 - val_f1_19: 0.8807 - val_f1_20: 0.6814 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.9341 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.9528 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.7893 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9458 - val_f1_38: 0.6366 - val_f1_39: 0.8958 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.7413 - val_f1_43: 0.0000e+00 - val_f1_44: 0.7998\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - 68s 4s/step - loss: 0.0568 - accuracy: 0.9849 - f1: 0.4148 - f1_1: 0.1999 - f1_2: 0.8096 - f1_3: 0.5852 - f1_4: 0.8883 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.8646 - f1_9: 0.8277 - f1_10: 0.9596 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9985 - f1_14: 0.0000e+00 - f1_15: 0.6360 - f1_16: 0.9953 - f1_17: 0.3459 - f1_19: 0.8778 - f1_20: 0.8337 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.9404 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.9529 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.7961 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9472 - f1_38: 0.6007 - f1_39: 0.9345 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.7678 - f1_43: 0.0000e+00 - f1_44: 0.8289 - val_loss: 0.0618 - val_accuracy: 0.9830 - val_f1: 0.4088 - val_f1_1: 0.2390 - val_f1_2: 0.7713 - val_f1_3: 0.5224 - val_f1_4: 0.8793 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.8566 - val_f1_9: 0.7878 - val_f1_10: 0.9557 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.5324 - val_f1_16: 0.9975 - val_f1_17: 0.4390 - val_f1_19: 0.8947 - val_f1_20: 0.7405 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.9359 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.9576 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.7766 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9542 - val_f1_38: 0.6305 - val_f1_39: 0.9347 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.7593 - val_f1_43: 0.0000e+00 - val_f1_44: 0.7882\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - 67s 4s/step - loss: 0.0547 - accuracy: 0.9853 - f1: 0.4228 - f1_1: 0.2624 - f1_2: 0.8240 - f1_3: 0.6117 - f1_4: 0.8946 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.8730 - f1_9: 0.8243 - f1_10: 0.9600 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9986 - f1_14: 0.0000e+00 - f1_15: 0.6548 - f1_16: 0.9940 - f1_17: 0.4211 - f1_19: 0.9012 - f1_20: 0.8377 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.9413 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.9639 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.8001 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9585 - f1_38: 0.6197 - f1_39: 0.9475 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.7866 - f1_43: 0.0000e+00 - f1_44: 0.8362 - val_loss: 0.0595 - val_accuracy: 0.9839 - val_f1: 0.4153 - val_f1_1: 0.2890 - val_f1_2: 0.7511 - val_f1_3: 0.5516 - val_f1_4: 0.8895 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.8944 - val_f1_9: 0.7576 - val_f1_10: 0.9601 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6241 - val_f1_16: 0.9975 - val_f1_17: 0.4046 - val_f1_19: 0.8876 - val_f1_20: 0.7850 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.9397 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.9648 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8023 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9599 - val_f1_38: 0.6501 - val_f1_39: 0.9420 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.7521 - val_f1_43: 0.0000e+00 - val_f1_44: 0.8103\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - 67s 4s/step - loss: 0.0527 - accuracy: 0.9859 - f1: 0.4297 - f1_1: 0.2895 - f1_2: 0.8297 - f1_3: 0.6267 - f1_4: 0.9029 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.8729 - f1_9: 0.8387 - f1_10: 0.9611 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9985 - f1_14: 0.0000e+00 - f1_15: 0.6596 - f1_16: 0.9933 - f1_17: 0.4544 - f1_19: 0.9011 - f1_20: 0.9053 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.9409 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.9615 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.8100 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9787 - f1_38: 0.6514 - f1_39: 0.9615 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.8004 - f1_43: 0.0000e+00 - f1_44: 0.8478 - val_loss: 0.0578 - val_accuracy: 0.9841 - val_f1: 0.4210 - val_f1_1: 0.3079 - val_f1_2: 0.7692 - val_f1_3: 0.5721 - val_f1_4: 0.8974 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.8859 - val_f1_9: 0.7786 - val_f1_10: 0.9615 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.5908 - val_f1_16: 0.9975 - val_f1_17: 0.4331 - val_f1_19: 0.9166 - val_f1_20: 0.8062 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.9403 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.9685 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8152 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9824 - val_f1_38: 0.6708 - val_f1_39: 0.9639 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.7776 - val_f1_43: 0.0000e+00 - val_f1_44: 0.8028\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - 71s 4s/step - loss: 0.0506 - accuracy: 0.9865 - f1: 0.4356 - f1_1: 0.3564 - f1_2: 0.8426 - f1_3: 0.6239 - f1_4: 0.9091 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.8814 - f1_9: 0.8397 - f1_10: 0.9620 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9985 - f1_14: 0.0000e+00 - f1_15: 0.6786 - f1_16: 0.9951 - f1_17: 0.4871 - f1_19: 0.9198 - f1_20: 0.9067 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0616 - f1_26: 0.9481 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.9655 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.8143 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9683 - f1_38: 0.6482 - f1_39: 0.9621 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.8026 - f1_43: 0.0000e+00 - f1_44: 0.8506 - val_loss: 0.0561 - val_accuracy: 0.9846 - val_f1: 0.4316 - val_f1_1: 0.3863 - val_f1_2: 0.7622 - val_f1_3: 0.6275 - val_f1_4: 0.9019 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.8927 - val_f1_9: 0.7948 - val_f1_10: 0.9624 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6014 - val_f1_16: 0.9975 - val_f1_17: 0.4538 - val_f1_19: 0.9176 - val_f1_20: 0.8557 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.1681 - val_f1_26: 0.9434 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.9705 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8114 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9926 - val_f1_38: 0.6750 - val_f1_39: 0.9491 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.7810 - val_f1_43: 0.0000e+00 - val_f1_44: 0.8185\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - 69s 4s/step - loss: 0.0489 - accuracy: 0.9869 - f1: 0.4476 - f1_1: 0.4013 - f1_2: 0.8424 - f1_3: 0.6590 - f1_4: 0.9096 - f1_5: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.8879 - f1_9: 0.8486 - f1_10: 0.9642 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9986 - f1_14: 0.0000e+00 - f1_15: 0.6914 - f1_16: 0.9947 - f1_17: 0.5267 - f1_19: 0.9322 - f1_20: 0.9290 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.1508 - f1_26: 0.9477 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.9707 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.8227 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9841 - f1_38: 0.6685 - f1_39: 0.9676 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.8150 - f1_43: 0.1404 - f1_44: 0.8513 - val_loss: 0.0552 - val_accuracy: 0.9851 - val_f1: 0.4415 - val_f1_1: 0.4485 - val_f1_2: 0.7939 - val_f1_3: 0.6572 - val_f1_4: 0.9096 - val_f1_5: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.9029 - val_f1_9: 0.7591 - val_f1_10: 0.9647 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6568 - val_f1_16: 0.9975 - val_f1_17: 0.5256 - val_f1_19: 0.9162 - val_f1_20: 0.9125 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.1786 - val_f1_26: 0.9452 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.9771 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8158 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9901 - val_f1_38: 0.6979 - val_f1_39: 0.9691 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.7660 - val_f1_43: 0.0667 - val_f1_44: 0.8097\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - 68s 4s/step - loss: 0.0475 - accuracy: 0.9873 - f1: 0.4594 - f1_1: 0.4474 - f1_2: 0.8490 - f1_3: 0.6752 - f1_4: 0.9185 - f1_5: 0.0535 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.8955 - f1_9: 0.8460 - f1_10: 0.9654 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9985 - f1_14: 0.0000e+00 - f1_15: 0.6924 - f1_16: 0.9927 - f1_17: 0.5474 - f1_19: 0.9362 - f1_20: 0.9276 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.2909 - f1_26: 0.9493 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.9713 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.8243 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9839 - f1_38: 0.7046 - f1_39: 0.9700 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.8246 - f1_43: 0.2509 - f1_44: 0.8602 - val_loss: 0.0538 - val_accuracy: 0.9852 - val_f1: 0.4516 - val_f1_1: 0.4855 - val_f1_2: 0.7777 - val_f1_3: 0.6351 - val_f1_4: 0.9040 - val_f1_5: 0.0442 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.9029 - val_f1_9: 0.7735 - val_f1_10: 0.9668 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6166 - val_f1_16: 0.9975 - val_f1_17: 0.4588 - val_f1_19: 0.9437 - val_f1_20: 0.9252 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.4200 - val_f1_26: 0.9478 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.9746 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8224 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9894 - val_f1_38: 0.7019 - val_f1_39: 0.9700 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.7993 - val_f1_43: 0.1651 - val_f1_44: 0.8417\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - 69s 4s/step - loss: 0.0463 - accuracy: 0.9875 - f1: 0.4711 - f1_1: 0.4654 - f1_2: 0.8561 - f1_3: 0.6728 - f1_4: 0.9215 - f1_5: 0.0156 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.8995 - f1_9: 0.8491 - f1_10: 0.9657 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9986 - f1_14: 0.0000e+00 - f1_15: 0.7112 - f1_16: 0.9940 - f1_17: 0.5958 - f1_19: 0.9400 - f1_20: 0.9489 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.4715 - f1_26: 0.9521 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.9755 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.8270 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9817 - f1_38: 0.7136 - f1_39: 0.9810 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.8418 - f1_43: 0.3976 - f1_44: 0.8676 - val_loss: 0.0522 - val_accuracy: 0.9858 - val_f1: 0.4553 - val_f1_1: 0.5082 - val_f1_2: 0.7865 - val_f1_3: 0.6689 - val_f1_4: 0.9092 - val_f1_5: 0.0260 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.9027 - val_f1_9: 0.7992 - val_f1_10: 0.9668 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6746 - val_f1_16: 0.9975 - val_f1_17: 0.5645 - val_f1_19: 0.9472 - val_f1_20: 0.8859 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.4275 - val_f1_26: 0.9446 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.9820 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8239 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9944 - val_f1_38: 0.7035 - val_f1_39: 0.9620 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.7889 - val_f1_43: 0.1110 - val_f1_44: 0.8370\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - 67s 4s/step - loss: 0.0447 - accuracy: 0.9880 - f1: 0.4807 - f1_1: 0.5213 - f1_2: 0.8562 - f1_3: 0.6896 - f1_4: 0.9244 - f1_5: 0.0814 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.9015 - f1_9: 0.8554 - f1_10: 0.9683 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9986 - f1_14: 0.0000e+00 - f1_15: 0.7126 - f1_16: 0.9934 - f1_17: 0.6127 - f1_19: 0.9484 - f1_20: 0.9544 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.4971 - f1_26: 0.9511 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.9728 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.8366 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9828 - f1_38: 0.7254 - f1_39: 0.9807 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.8417 - f1_43: 0.5526 - f1_44: 0.8674 - val_loss: 0.0511 - val_accuracy: 0.9859 - val_f1: 0.4679 - val_f1_1: 0.5126 - val_f1_2: 0.8142 - val_f1_3: 0.6363 - val_f1_4: 0.9119 - val_f1_5: 0.0896 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.9023 - val_f1_9: 0.8162 - val_f1_10: 0.9685 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6357 - val_f1_16: 0.9975 - val_f1_17: 0.5200 - val_f1_19: 0.9516 - val_f1_20: 0.9358 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.5154 - val_f1_26: 0.9505 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.9776 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8086 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9911 - val_f1_38: 0.7076 - val_f1_39: 0.9686 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.8025 - val_f1_43: 0.4517 - val_f1_44: 0.8484\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - 68s 4s/step - loss: 0.0433 - accuracy: 0.9884 - f1: 0.4872 - f1_1: 0.5332 - f1_2: 0.8691 - f1_3: 0.7044 - f1_4: 0.9236 - f1_5: 0.0729 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.9044 - f1_9: 0.8588 - f1_10: 0.9678 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9986 - f1_14: 0.0000e+00 - f1_15: 0.7247 - f1_16: 0.9940 - f1_17: 0.6437 - f1_19: 0.9526 - f1_20: 0.9655 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.6181 - f1_26: 0.9537 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.9740 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.8381 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9809 - f1_38: 0.7305 - f1_39: 0.9823 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.8420 - f1_43: 0.5756 - f1_44: 0.8798 - val_loss: 0.0500 - val_accuracy: 0.9864 - val_f1: 0.4752 - val_f1_1: 0.5792 - val_f1_2: 0.8032 - val_f1_3: 0.6568 - val_f1_4: 0.9110 - val_f1_5: 0.0864 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.9055 - val_f1_9: 0.8105 - val_f1_10: 0.9698 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6699 - val_f1_16: 0.9975 - val_f1_17: 0.5969 - val_f1_19: 0.9530 - val_f1_20: 0.9328 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.5461 - val_f1_26: 0.9510 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.9809 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8256 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9911 - val_f1_38: 0.7133 - val_f1_39: 0.9707 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.8089 - val_f1_43: 0.5181 - val_f1_44: 0.8304\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - 67s 4s/step - loss: 0.0421 - accuracy: 0.9886 - f1: 0.4945 - f1_1: 0.5516 - f1_2: 0.8672 - f1_3: 0.6973 - f1_4: 0.9315 - f1_5: 0.1391 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.9054 - f1_9: 0.8636 - f1_10: 0.9708 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9986 - f1_14: 0.0000e+00 - f1_15: 0.7309 - f1_16: 0.9939 - f1_17: 0.6564 - f1_19: 0.9529 - f1_20: 0.9678 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.6538 - f1_26: 0.9550 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.9778 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.8386 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9814 - f1_38: 0.7417 - f1_39: 0.9833 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.8582 - f1_43: 0.6807 - f1_44: 0.8832 - val_loss: 0.0511 - val_accuracy: 0.9857 - val_f1: 0.4790 - val_f1_1: 0.5507 - val_f1_2: 0.7919 - val_f1_3: 0.6809 - val_f1_4: 0.9137 - val_f1_5: 0.1167 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.9066 - val_f1_9: 0.7640 - val_f1_10: 0.9704 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6603 - val_f1_16: 0.9975 - val_f1_17: 0.5953 - val_f1_19: 0.9566 - val_f1_20: 0.9298 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.6354 - val_f1_26: 0.9515 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.9865 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8306 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9929 - val_f1_38: 0.7441 - val_f1_39: 0.9771 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.7981 - val_f1_43: 0.5944 - val_f1_44: 0.8139\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - 67s 4s/step - loss: 0.0415 - accuracy: 0.9886 - f1: 0.4978 - f1_1: 0.5613 - f1_2: 0.8690 - f1_3: 0.6938 - f1_4: 0.9309 - f1_5: 0.1681 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.9071 - f1_9: 0.8609 - f1_10: 0.9711 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9987 - f1_14: 0.0000e+00 - f1_15: 0.7366 - f1_16: 0.9935 - f1_17: 0.6728 - f1_19: 0.9539 - f1_20: 0.9740 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.6845 - f1_26: 0.9547 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.9734 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.8387 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9831 - f1_38: 0.7543 - f1_39: 0.9842 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.8577 - f1_43: 0.7065 - f1_44: 0.8849 - val_loss: 0.0484 - val_accuracy: 0.9868 - val_f1: 0.4866 - val_f1_1: 0.6123 - val_f1_2: 0.8260 - val_f1_3: 0.6662 - val_f1_4: 0.9129 - val_f1_5: 0.1896 - val_f1_6: 0.0096 - val_f1_7: 0.0000e+00 - val_f1_8: 0.9136 - val_f1_9: 0.7998 - val_f1_10: 0.9693 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6768 - val_f1_16: 0.9975 - val_f1_17: 0.5809 - val_f1_19: 0.9565 - val_f1_20: 0.9474 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.6337 - val_f1_26: 0.9541 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.9837 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8298 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9911 - val_f1_38: 0.7603 - val_f1_39: 0.9694 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.7962 - val_f1_43: 0.6250 - val_f1_44: 0.8610\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - 67s 4s/step - loss: 0.0400 - accuracy: 0.9892 - f1: 0.5048 - f1_1: 0.6158 - f1_2: 0.8728 - f1_3: 0.7121 - f1_4: 0.9344 - f1_5: 0.2686 - f1_6: 0.0240 - f1_7: 0.0000e+00 - f1_8: 0.9154 - f1_9: 0.8634 - f1_10: 0.9710 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9973 - f1_14: 0.0000e+00 - f1_15: 0.7433 - f1_16: 0.9927 - f1_17: 0.6891 - f1_19: 0.9579 - f1_20: 0.9764 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.6928 - f1_26: 0.9543 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.9776 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.8516 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9871 - f1_38: 0.7523 - f1_39: 0.9843 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.8528 - f1_43: 0.7201 - f1_44: 0.8843 - val_loss: 0.0483 - val_accuracy: 0.9866 - val_f1: 0.4874 - val_f1_1: 0.5429 - val_f1_2: 0.7956 - val_f1_3: 0.6617 - val_f1_4: 0.9146 - val_f1_5: 0.1722 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.9037 - val_f1_9: 0.8194 - val_f1_10: 0.9723 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6467 - val_f1_16: 0.9975 - val_f1_17: 0.6892 - val_f1_19: 0.9643 - val_f1_20: 0.9423 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.7430 - val_f1_26: 0.9487 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.9797 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8314 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9911 - val_f1_38: 0.7377 - val_f1_39: 0.9709 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.8169 - val_f1_43: 0.6064 - val_f1_44: 0.8477\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - 67s 4s/step - loss: 0.0397 - accuracy: 0.9892 - f1: 0.5082 - f1_1: 0.5853 - f1_2: 0.8708 - f1_3: 0.6982 - f1_4: 0.9359 - f1_5: 0.3302 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.9102 - f1_9: 0.8611 - f1_10: 0.9759 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9989 - f1_14: 0.0000e+00 - f1_15: 0.7458 - f1_16: 0.9938 - f1_17: 0.7111 - f1_19: 0.9564 - f1_20: 0.9758 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.7122 - f1_26: 0.9568 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.9772 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.8473 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9901 - f1_38: 0.7591 - f1_39: 0.9859 - f1_40: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.8598 - f1_43: 0.7994 - f1_44: 0.8908 - val_loss: 0.0471 - val_accuracy: 0.9869 - val_f1: 0.4937 - val_f1_1: 0.6232 - val_f1_2: 0.8224 - val_f1_3: 0.6741 - val_f1_4: 0.9172 - val_f1_5: 0.2782 - val_f1_6: 0.0454 - val_f1_7: 0.0000e+00 - val_f1_8: 0.9137 - val_f1_9: 0.7832 - val_f1_10: 0.9741 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6870 - val_f1_16: 0.9975 - val_f1_17: 0.6175 - val_f1_19: 0.9587 - val_f1_20: 0.9521 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.6884 - val_f1_26: 0.9547 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.9854 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8404 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9929 - val_f1_38: 0.7578 - val_f1_39: 0.9768 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.8190 - val_f1_43: 0.6285 - val_f1_44: 0.8581\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - 68s 4s/step - loss: 0.0383 - accuracy: 0.9895 - f1: 0.5165 - f1_1: 0.6227 - f1_2: 0.8795 - f1_3: 0.7269 - f1_4: 0.9378 - f1_5: 0.4293 - f1_6: 0.0548 - f1_7: 0.0000e+00 - f1_8: 0.9164 - f1_9: 0.8743 - f1_10: 0.9748 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9987 - f1_14: 0.0000e+00 - f1_15: 0.7546 - f1_16: 0.9947 - f1_17: 0.7198 - f1_19: 0.9611 - f1_20: 0.9722 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.7644 - f1_26: 0.9579 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.9791 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.8538 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9884 - f1_38: 0.7832 - f1_39: 0.9858 - f1_40: 0.0000e+00 - f1_41: 0.0312 - f1_42: 0.8716 - f1_43: 0.7369 - f1_44: 0.8912 - val_loss: 0.0460 - val_accuracy: 0.9875 - val_f1: 0.4967 - val_f1_1: 0.6269 - val_f1_2: 0.8270 - val_f1_3: 0.6913 - val_f1_4: 0.9265 - val_f1_5: 0.3188 - val_f1_6: 0.0096 - val_f1_7: 0.0000e+00 - val_f1_8: 0.9105 - val_f1_9: 0.8276 - val_f1_10: 0.9738 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6992 - val_f1_16: 0.9975 - val_f1_17: 0.6621 - val_f1_19: 0.9619 - val_f1_20: 0.9520 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.6937 - val_f1_26: 0.9565 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.9854 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8309 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9929 - val_f1_38: 0.7416 - val_f1_39: 0.9736 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.8263 - val_f1_43: 0.6185 - val_f1_44: 0.8631\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - 68s 4s/step - loss: 0.0372 - accuracy: 0.9899 - f1: 0.5196 - f1_1: 0.6315 - f1_2: 0.8809 - f1_3: 0.7329 - f1_4: 0.9357 - f1_5: 0.3962 - f1_6: 0.0841 - f1_7: 0.0000e+00 - f1_8: 0.9246 - f1_9: 0.8788 - f1_10: 0.9747 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9986 - f1_14: 0.0000e+00 - f1_15: 0.7647 - f1_16: 0.9952 - f1_17: 0.7395 - f1_19: 0.9654 - f1_20: 0.9798 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.7894 - f1_26: 0.9597 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.9837 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.8570 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9840 - f1_38: 0.7793 - f1_39: 0.9850 - f1_40: 0.0000e+00 - f1_41: 0.0179 - f1_42: 0.8649 - f1_43: 0.7763 - f1_44: 0.9037 - val_loss: 0.0459 - val_accuracy: 0.9873 - val_f1: 0.4964 - val_f1_1: 0.6170 - val_f1_2: 0.8219 - val_f1_3: 0.6738 - val_f1_4: 0.9169 - val_f1_5: 0.2573 - val_f1_6: 0.0247 - val_f1_7: 0.0000e+00 - val_f1_8: 0.9092 - val_f1_9: 0.8292 - val_f1_10: 0.9757 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6978 - val_f1_16: 0.9975 - val_f1_17: 0.6809 - val_f1_19: 0.9657 - val_f1_20: 0.9305 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.7246 - val_f1_26: 0.9578 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.9854 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8307 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9895 - val_f1_38: 0.7772 - val_f1_39: 0.9772 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.8290 - val_f1_43: 0.6386 - val_f1_44: 0.8481\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - 67s 4s/step - loss: 0.0362 - accuracy: 0.9901 - f1: 0.5254 - f1_1: 0.6229 - f1_2: 0.8927 - f1_3: 0.7436 - f1_4: 0.9433 - f1_5: 0.4763 - f1_6: 0.0907 - f1_7: 0.0000e+00 - f1_8: 0.9246 - f1_9: 0.8838 - f1_10: 0.9759 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9986 - f1_14: 0.0000e+00 - f1_15: 0.7692 - f1_16: 0.9912 - f1_17: 0.7468 - f1_19: 0.9665 - f1_20: 0.9817 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.8020 - f1_26: 0.9585 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.9804 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.8630 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9928 - f1_38: 0.7984 - f1_39: 0.9874 - f1_40: 0.0000e+00 - f1_41: 0.0521 - f1_42: 0.8792 - f1_43: 0.7907 - f1_44: 0.9033 - val_loss: 0.0450 - val_accuracy: 0.9878 - val_f1: 0.5059 - val_f1_1: 0.6206 - val_f1_2: 0.8520 - val_f1_3: 0.6814 - val_f1_4: 0.9314 - val_f1_5: 0.3460 - val_f1_6: 0.0878 - val_f1_7: 0.0000e+00 - val_f1_8: 0.9113 - val_f1_9: 0.8217 - val_f1_10: 0.9750 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.7059 - val_f1_16: 0.9975 - val_f1_17: 0.6755 - val_f1_19: 0.9650 - val_f1_20: 0.9586 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.7719 - val_f1_26: 0.9569 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.9854 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8258 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9929 - val_f1_38: 0.7844 - val_f1_39: 0.9777 - val_f1_40: 0.0000e+00 - val_f1_41: 0.0818 - val_f1_42: 0.8266 - val_f1_43: 0.6386 - val_f1_44: 0.8626\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - 68s 4s/step - loss: 0.0355 - accuracy: 0.9904 - f1: 0.5322 - f1_1: 0.6403 - f1_2: 0.8916 - f1_3: 0.7382 - f1_4: 0.9426 - f1_5: 0.5074 - f1_6: 0.1548 - f1_7: 0.0000e+00 - f1_8: 0.9230 - f1_9: 0.8831 - f1_10: 0.9765 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.9986 - f1_14: 0.0000e+00 - f1_15: 0.7725 - f1_16: 0.9945 - f1_17: 0.7592 - f1_19: 0.9647 - f1_20: 0.9789 - f1_21: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.8375 - f1_26: 0.9599 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.9864 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.8627 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.9892 - f1_38: 0.8014 - f1_39: 0.9866 - f1_40: 0.0000e+00 - f1_41: 0.1656 - f1_42: 0.8795 - f1_43: 0.7856 - f1_44: 0.9061 - val_loss: 0.0444 - val_accuracy: 0.9877 - val_f1: 0.5083 - val_f1_1: 0.6330 - val_f1_2: 0.8528 - val_f1_3: 0.6920 - val_f1_4: 0.9205 - val_f1_5: 0.4126 - val_f1_6: 0.0833 - val_f1_7: 0.0000e+00 - val_f1_8: 0.9117 - val_f1_9: 0.8417 - val_f1_10: 0.9761 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 1.0000 - val_f1_14: 0.0000e+00 - val_f1_15: 0.6630 - val_f1_16: 0.9975 - val_f1_17: 0.6847 - val_f1_19: 0.9676 - val_f1_20: 0.9559 - val_f1_21: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.7584 - val_f1_26: 0.9589 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.9854 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.8300 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9895 - val_f1_38: 0.7747 - val_f1_39: 0.9744 - val_f1_40: 0.0000e+00 - val_f1_41: 0.1273 - val_f1_42: 0.8248 - val_f1_43: 0.6508 - val_f1_44: 0.8639\n"
          ]
        }
      ],
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "history = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=40, validation_data=(valid_sentences_X, cat_val_tags_y), callbacks=[callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_XNg1aXonOl"
      },
      "source": [
        "#History f1 for class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIeuPw8AorGd"
      },
      "outputs": [],
      "source": [
        "f1_val_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_val_list[i] = history.history['val_f1_{}'.format(i)][-1]\n",
        "f1_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_list[i] = history.history['f1_{}'.format(i)][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlQ8zRSMou1-"
      },
      "outputs": [],
      "source": [
        "index2tag = {value : key for (key, value) in tag2index.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKsg3Ui-owlf",
        "outputId": "f7bd8f0c-7d0c-4f2f-910f-a93c98306bf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: VBG --- F1: 0.6402647495269775\n",
            "Tag: NNS --- F1: 0.8915663957595825\n",
            "Tag: VBN --- F1: 0.738163948059082\n",
            "Tag: VBZ --- F1: 0.9426226615905762\n",
            "Tag: RP --- F1: 0.5074160695075989\n",
            "Tag: JJR --- F1: 0.15480472147464752\n",
            "Tag: FW --- F1: 0.0\n",
            "Tag: CD --- F1: 0.923042893409729\n",
            "Tag: NNP --- F1: 0.883076012134552\n",
            "Tag: DT --- F1: 0.976521909236908\n",
            "Tag: JJS --- F1: 0.0\n",
            "Tag: SYM --- F1: 0.0\n",
            "Tag: TO --- F1: 0.9986453652381897\n",
            "Tag: -LRB- --- F1: 0.0\n",
            "Tag: JJ --- F1: 0.7724694609642029\n",
            "Tag: $ --- F1: 0.9945385456085205\n",
            "Tag: RB --- F1: 0.7591555118560791\n",
            "Tag: PRP --- F1: 0.9646547436714172\n",
            "Tag: MD --- F1: 0.9789264798164368\n",
            "Tag: NNPS --- F1: 0.0\n",
            "Tag: WRB --- F1: 0.0\n",
            "Tag: WDT --- F1: 0.837536096572876\n",
            "Tag: IN --- F1: 0.9598546624183655\n",
            "Tag: UH --- F1: 0.0\n",
            "Tag: -RRB- --- F1: 0.0\n",
            "Tag: PDT --- F1: 0.0\n",
            "Tag: PRP$ --- F1: 0.9863884449005127\n",
            "Tag: # --- F1: 0.0\n",
            "Tag: RBR --- F1: 0.0\n",
            "Tag: NN --- F1: 0.8627248406410217\n",
            "Tag: WP$ --- F1: 0.0\n",
            "Tag: LS --- F1: 0.0\n",
            "Tag: POS --- F1: 0.989243745803833\n",
            "Tag: VBP --- F1: 0.8014297485351562\n",
            "Tag: CC --- F1: 0.9866000413894653\n",
            "Tag: RBS --- F1: 0.0\n",
            "Tag: EX --- F1: 0.16562499105930328\n",
            "Tag: VBD --- F1: 0.8795477151870728\n",
            "Tag: WP --- F1: 0.78561931848526\n",
            "Tag: VB --- F1: 0.9060536623001099\n"
          ]
        }
      ],
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- F1: {}'.format(index2tag[i], f1_list[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9wM9cNloyzc",
        "outputId": "d241a245-e4c4-4b61-92e4-14be55405802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: VBG --- Val_F1: 0.6329540610313416\n",
            "Tag: NNS --- Val_F1: 0.8528262972831726\n",
            "Tag: VBN --- Val_F1: 0.6919581890106201\n",
            "Tag: VBZ --- Val_F1: 0.9205448627471924\n",
            "Tag: RP --- Val_F1: 0.4125540852546692\n",
            "Tag: JJR --- Val_F1: 0.08331645280122757\n",
            "Tag: FW --- Val_F1: 0.0\n",
            "Tag: CD --- Val_F1: 0.9117259979248047\n",
            "Tag: NNP --- Val_F1: 0.8416542410850525\n",
            "Tag: DT --- Val_F1: 0.97613525390625\n",
            "Tag: JJS --- Val_F1: 0.0\n",
            "Tag: SYM --- Val_F1: 0.0\n",
            "Tag: TO --- Val_F1: 1.0\n",
            "Tag: -LRB- --- Val_F1: 0.0\n",
            "Tag: JJ --- Val_F1: 0.6629823446273804\n",
            "Tag: $ --- Val_F1: 0.9975429773330688\n",
            "Tag: RB --- Val_F1: 0.6846678256988525\n",
            "Tag: PRP --- Val_F1: 0.9676254391670227\n",
            "Tag: MD --- Val_F1: 0.9558601975440979\n",
            "Tag: NNPS --- Val_F1: 0.0\n",
            "Tag: WRB --- Val_F1: 0.0\n",
            "Tag: WDT --- Val_F1: 0.7584331035614014\n",
            "Tag: IN --- Val_F1: 0.9589329361915588\n",
            "Tag: UH --- Val_F1: 0.0\n",
            "Tag: -RRB- --- Val_F1: 0.0\n",
            "Tag: PDT --- Val_F1: 0.0\n",
            "Tag: PRP$ --- Val_F1: 0.9853793978691101\n",
            "Tag: # --- Val_F1: 0.0\n",
            "Tag: RBR --- Val_F1: 0.0\n",
            "Tag: NN --- Val_F1: 0.8300292491912842\n",
            "Tag: WP$ --- Val_F1: 0.0\n",
            "Tag: LS --- Val_F1: 0.0\n",
            "Tag: POS --- Val_F1: 0.9894912242889404\n",
            "Tag: VBP --- Val_F1: 0.7747256755828857\n",
            "Tag: CC --- Val_F1: 0.9743629693984985\n",
            "Tag: RBS --- Val_F1: 0.0\n",
            "Tag: EX --- Val_F1: 0.12727271020412445\n",
            "Tag: VBD --- Val_F1: 0.8248082995414734\n",
            "Tag: WP --- Val_F1: 0.6507688164710999\n",
            "Tag: VB --- Val_F1: 0.8639055490493774\n"
          ]
        }
      ],
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- Val_F1: {}'.format(index2tag[i], f1_val_list[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uv6mtuMo1Mh"
      },
      "source": [
        "#Infos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jot_8JtVo37_"
      },
      "source": [
        "#40 epochs, batch 128\n",
        "loss: 0.0370 - accuracy: 0.9899 - f1: 0.5241\n",
        "\n",
        "loss: 0.0373 - accuracy: 0.9896 - f1: 0.5214"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOQVTC5DSMvW"
      },
      "source": [
        "#Test Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9Y9N9RYoBNi"
      },
      "outputs": [],
      "source": [
        "test_sentences_X,  test_tags_y = [], []\n",
        "\n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        "\n",
        "for s in test_tags:\n",
        "    test_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFug9E5DoKWe"
      },
      "outputs": [],
      "source": [
        "y_val_pred = model.predict(test_sentences_X)\n",
        "\n",
        "y_pred = np.zeros((y_val_pred.shape[0], y_val_pred.shape[1]), dtype=int)\n",
        "for i in range(len(y_val_pred)):\n",
        "  for j in range(len(y_val_pred[i])):\n",
        "    y_pred[i][j] = np.argmax(y_val_pred[i][j])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lrb = [tag2index['-LRB-']]\n",
        "rrb = [tag2index['-RRB-']]\n",
        "canc = [tag2index['#']]\n",
        "dol = [tag2index['$']]\n",
        "\n",
        "punct_cat_classes = to_categorical([point, virg, weird_apex, single_apex, two_dots, pad, lrb, rrb, canc, dol], len(tag2index))\n",
        "\n",
        "cum_tags = np.zeros(len(tag2index))\n",
        "for i in punct_cat_classes:\n",
        "  cum_tags += i[0]\n",
        "where_tags = np.where(np.logical_not(cum_tags))\n",
        "no_punct_indexes = where_tags[0]"
      ],
      "metadata": {
        "id": "OpoOertF4Hzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Trdg6NLXSdGi"
      },
      "source": [
        "Using the f1_score from sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bsytk51_PLmR",
        "outputId": "8f4a6675-b437-4f5f-95c5-faba1cabbca9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5424090124111094"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "tags_flat = test_tags_y.flatten()\n",
        "pred_flat = y_pred.flatten()\n",
        "\n",
        "f1_model = f1_score(tags_flat, pred_flat, labels = no_punct_indexes, average='macro', zero_division=0)\n",
        "f1_model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8iUsOu-UvaQ4",
        "I3qoNG45vaQ6"
      ],
      "name": "A1_biLSTM256_2DENSE.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}