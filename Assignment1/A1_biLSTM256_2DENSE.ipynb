{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AirNicco8/NLP_Assignments/blob/main/Assignment1/A1_biLSTM256_2DENSE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Due to**: 23/12/2021 (dd/mm/yyyy)\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Part-of Speech (POS) tagging as Sequence Labelling using Recurrent Neural Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4_wqPdlBcKS"
      },
      "source": [
        "# Intro\n",
        "\n",
        "In this assignment  we will ask you to perform POS tagging using neural architectures\n",
        "\n",
        "You are asked to follow these steps:\n",
        "*   Download the corpora and split it in training and test sets, structuring a dataframe.\n",
        "*   Embed the words using GloVe embeddings\n",
        "*   Create a baseline model, using a simple neural architecture\n",
        "*   Experiment doing small modifications to the baseline model, choose hyperparameters using the validation set\n",
        "*   Evaluate your two best model\n",
        "*   Analyze the errors of your model\n",
        "\n",
        "\n",
        "**Task**: given a corpus of documents, predict the POS tag for each word\n",
        "\n",
        "**Corpus**:\n",
        "Ignore the numeric value in the third column, use only the words/symbols and its label. \n",
        "The corpus is available at:\n",
        "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
        "\n",
        "**Splits**: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
        "\n",
        "\n",
        "**Features**: you MUST use GloVe embeddings as the only input features to the model.\n",
        "\n",
        "**Splitting**: you can decide to split documents into sentences or not, the choice is yours.\n",
        "\n",
        "**I/O structure**: The input data will have three dimensions: 1-documents/sentences, 2-token, 3-features; for the output there are 2 possibilities: if you use one-hot encoding it will be 1-documents/sentences, 2-token labels, 3-classes, if you use a single integer that indicates the number of the class it will be 1-documents/sentences, 2-token labels.\n",
        "\n",
        "**Baseline**: two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top; the choice of hyper-parameters is yours.\n",
        "\n",
        "**Architectures**: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and adding an additional dense layer; do not mix these variantions.\n",
        "\n",
        "\n",
        "**Training and Experiments**: all the experiments must involve only the training and validation sets.\n",
        "\n",
        "**Evaluation**: in the end, only the two best models of your choice (according to the validation set) must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech. DO NOT CONSIDER THE PUNCTUATION CLASSES.\n",
        "\n",
        "**Metrics**: the metric you must use to evaluate your final model is the F1-macro, WITHOUT considering punctuation/symbols classes; during the training process you can use accuracy because you can't use the F1 metric unless you use a single (gigantic) batch because there is no way to aggregate \"partial\" F1 scores computed on mini-batches.\n",
        "\n",
        "**Discussion and Error Analysis** : verify and discuss if the results on the test sets are coherent with those on the validation set; analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
        "\n",
        "**Report**: you are asked to deliver the code of your experiments and a small pdf report of about 2 pages; the pdf must begin with the names of the people of your team and a small abstract (4-5 lines) that sums up your findings.\n",
        "\n",
        "# Out Of Vocabulary (OOV) terms\n",
        "\n",
        "How to handle words that are not in GloVe vocabulary?\n",
        "You can handle them as you want (random embedding, placeholder, whatever!), but they must be STATIC embeddings (you cannot train them).\n",
        "\n",
        "But there is a very important caveat! As usual, the element of the test set must not influence the elements of the other splits!\n",
        "\n",
        "So, when you compute new embeddings for train+validation, you must forget about test documents.\n",
        "The motivation is to emulate a real-world scenario, where you select and train a model in the first stage, without knowing nothing about the testing environment.\n",
        "\n",
        "For implementation convenience, you CAN use a single vocabulary file/matrix/whatever. The principle of the previous point is that the embeddings inside that file/matrix must be generated independently for train and test splits.\n",
        "\n",
        "Basically in a real-world scenario, this is what would happen:\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Training of the model(s)\n",
        "5. Compute embeddings for terms OOV2 of the validation split \n",
        "6. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "7. Validation of the model(s)\n",
        "8. Compute embeddings for terms OOV3 of the test split \n",
        "9. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2+OOV3\n",
        "10. Testing of the final model\n",
        "\n",
        "In this case, where we already have all the documents, we can simplify the process a bit, but the procedure must remain rigorous.\n",
        "\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Compute embeddings for terms OOV2 of the validation split \n",
        "5. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "6. Compute embeddings for terms OOV3 of the test split \n",
        "7. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2\n",
        "8. Training of the model(s)\n",
        "9. Validation of the model(s)\n",
        "10. Testing of the final model\n",
        "\n",
        "Step 2 and step 6 must be completely independent of each other, for what concerns the method and the documents. But they can rely on the previous vocabulary (V1 for step 2 and V3 for step 6)\n",
        "THEREFORE if a word is present both in the training set and the test split and not in the starting vocabulary, its embedding is computed in step 2) and it is not considered OOV anymore in step 6).\n",
        "\n",
        "# Report\n",
        "The report must not be just a copy and paste of graphs and tables!\n",
        "\n",
        "The report must not be longer than 2 pages and must contain:\n",
        "* The names of the member of your team\n",
        "* A short abstract (4-5 lines) that sum ups everything\n",
        "* A general description of the task you have addressed and how you have addressed it\n",
        "* A short description of the models you have used\n",
        "* Some tables that sum up your findings in validation and test and a discussion of those results\n",
        "* The most relevant findings of your error analysis\n",
        "\n",
        "# Evaluation Criterion\n",
        "\n",
        "The goal of this assignment is not to prove you can find best model ever, but to face a common task, structure it correctly, and follow a correct and rigorous experimental procedure.\n",
        "In other words, we don't care if you final models are awful as long as you have followed the correct procedure and wrote a decent report.\n",
        "\n",
        "The score of the assignment will be computed roughly as follows\n",
        "* 1 point for the general setting of the problem\n",
        "* 1 point for the handling of OOV terms\n",
        "* 1 point for the models\n",
        "* 1 point for train-validation-test procedure\n",
        "* 2 point for the discussion of the results, error analysis, and report\n",
        "\n",
        "This distribution of scores is tentative and we may decide to alter it at any moment.\n",
        "We also reserve the right to assign a small bonus (0.5 points) to any assignment that is particularly worthy. Similarly, in case of grave errors, we may decide to assign an equivalent malus (-0.5 points).\n",
        "\n",
        "# Contacts\n",
        "\n",
        "In case of any doubt, question, issue, or help we highly recommend you to check the [course useful material](https://virtuale.unibo.it/pluginfile.php/1036039/mod_resource/content/2/NLP_Course_Useful_Material.pdf) for additional information, and to use the Virtuale forums to discuss with other students.\n",
        "\n",
        "You can always contact us at the following email addresses. To increase the probability of a prompt response, we reccomend you to write to both the teaching assistants.\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Andrea Galassi -> a.galassi@unibo.it\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it\n",
        "\n",
        "\n",
        "# FAQ\n",
        "* You can use a non-trainable Embedding layer to load the glove embeddings\n",
        "* You can use any library of your choice to implement the networks. Two options are tensorflow/keras or pythorch. Both these libraries have all the classes you need to implement these simple architectures and there are plenty of tutorials around, where you can learn how to use them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usG1eafJvaQ4"
      },
      "source": [
        "# Preparation of the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iUsOu-UvaQ4"
      },
      "source": [
        "## Import the basic libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "emdubW66fnlD"
      },
      "outputs": [],
      "source": [
        "import os, shutil  #  file management\n",
        "import sys \n",
        "import pandas as pd  #  dataframe management\n",
        "import numpy as np  #  data manipulation\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request  #  download files\n",
        "import zipfile  #  unzip files\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6B-w2pqyfpF1"
      },
      "outputs": [],
      "source": [
        "from keras import metrics \n",
        "from tensorflow.python.keras.metrics import Metric\n",
        "from sklearn.metrics import f1_score\n",
        "import keras.backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFvajV2sni19",
        "outputId": "6f360e26-d607-4feb-e9e0-05e392ef26d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3qoNG45vaQ6"
      },
      "source": [
        "## Download the dataset\n",
        "Use the Dependency Treebank corpora from nltk. Download and extract it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3xn1MZPgA_v",
        "outputId": "7bed9a48-f95a-4912-c1f4-1d9ec84c59e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful download\n",
            "Successful extraction\n"
          ]
        }
      ],
      "source": [
        "dataset_folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\")\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "    os.makedirs(dataset_folder)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
        "\n",
        "dataset_path = os.path.join(dataset_folder, \"treebank.zip\")\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    urllib.request.urlretrieve(url, dataset_path)\n",
        "    print(\"Successful download\")\n",
        "\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_folder)\n",
        "print(\"Successful extraction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiO1v6SJmm37"
      },
      "source": [
        "# Pre Processing\n",
        "Reorder files and split train, validation and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LKbUsrVHhGAG"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"dependency_treebank\"\n",
        "\n",
        "\n",
        "folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\", dataset_name)\n",
        "\n",
        "pre_train = []\n",
        "pre_valid = []\n",
        "pre_test = []\n",
        "i = 1\n",
        "\n",
        "file_list = sorted(os.listdir(folder))\n",
        "\n",
        "for filename in file_list:\n",
        "  file_path = os.path.join(folder, filename)\n",
        "  if os.path.isfile(file_path):\n",
        "    # open the file\n",
        "      text = []\n",
        "      with open(file_path, mode='r', encoding='utf-8') as text_file:\n",
        "        text = text_file.read()\n",
        "        if i <= 100: # from 1 to 100 train set\n",
        "          pre_train.append(text)\n",
        "        elif i <= 150: # from 101 to 150 validation set\n",
        "          pre_valid.append(text)\n",
        "        else: # remainings are test set\n",
        "          pre_test.append(text)  \n",
        "  i+=1\n",
        "\n",
        "tmp_train = []\n",
        "tmp_valid = []\n",
        "tmp_test = []\n",
        "\n",
        "# split the contents of the files for the character '\\n'\n",
        "for paragraph in pre_train:\n",
        "   tmp_train.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_valid:\n",
        "   tmp_valid.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_test:\n",
        "   tmp_test.append(paragraph.split('\\n'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split each word with its tag"
      ],
      "metadata": {
        "id": "AR4rlQo5wL-H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GUgbTXi-F7Sa"
      },
      "outputs": [],
      "source": [
        "train = []\n",
        "valid = []\n",
        "test = []\n",
        "\n",
        "for i in tmp_train:\n",
        "  for j in i:\n",
        "    train.append(j.split('\\t'))\n",
        "\n",
        "for i in tmp_valid:\n",
        "  for j in i:\n",
        "    valid.append(j.split('\\t'))\n",
        "\n",
        "for i in tmp_test:\n",
        "  for j in i:\n",
        "    test.append(j.split('\\t'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the sentences by their respective tags by creating two lists of lists.\n",
        "\n",
        "One contains lists of words (which form sentences) and one contains lists of tags that refer to individual words"
      ],
      "metadata": {
        "id": "KB50HMDn18NX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bmzVaRuQFp0M"
      },
      "outputs": [],
      "source": [
        "train_sentences = []\n",
        "valid_sentences = []\n",
        "test_sentences = []\n",
        "\n",
        "train_tags = []\n",
        "valid_tags = []\n",
        "test_tags = []\n",
        "\n",
        "tmp_sentences = []\n",
        "tmp_tags = []\n",
        "for i in train:\n",
        "  if i[0] != '':\n",
        "    tmp_sentences.append(i[0])\n",
        "    tmp_tags.append(i[1])\n",
        "  else:\n",
        "    train_sentences.append(tmp_sentences)\n",
        "    train_tags.append(tmp_tags)\n",
        "    tmp_sentences = []\n",
        "    tmp_tags = []\n",
        "\n",
        "tmp_sentences = []\n",
        "tmp_tags = []\n",
        "for i in valid:\n",
        "  if i[0] != '':\n",
        "    tmp_sentences.append(i[0])\n",
        "    tmp_tags.append(i[1])\n",
        "  else:\n",
        "    valid_sentences.append(tmp_sentences)\n",
        "    valid_tags.append(tmp_tags)\n",
        "    tmp_sentences = []\n",
        "    tmp_tags = []\n",
        "\n",
        "tmp_sentences = []\n",
        "tmp_tags = []\n",
        "for i in test:\n",
        "  if i[0] != '':\n",
        "    tmp_sentences.append(i[0])\n",
        "    tmp_tags.append(i[1])\n",
        "  else:\n",
        "    test_sentences.append(tmp_sentences)\n",
        "    test_tags.append(tmp_tags)\n",
        "    tmp_sentences = []\n",
        "    tmp_tags = []"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We flatten the lists so that we have a 3 simple lists with the words in order"
      ],
      "metadata": {
        "id": "f8hhGpCK2yS7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tB65cUFLIeEr"
      },
      "outputs": [],
      "source": [
        "flat_train = [item for sublist in train_sentences for item in sublist]\n",
        "flat_valid = [item for sublist in valid_sentences for item in sublist]\n",
        "flat_test = [item for sublist in test_sentences for item in sublist]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4IenRMamtLg"
      },
      "source": [
        "# Tokenization\n",
        "We create the vocabulary index based on word frequency and then we take each word in the text and replaces it with its corresponding integer value from the dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TZO6ga1EGHdh"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "train_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "train_tokenizer.fit_on_texts(train_sentences)                    # fit tokeniser on data\n",
        "train_encoded = train_tokenizer.texts_to_sequences(train_sentences)\n",
        "\n",
        "valid_tokenizer = Tokenizer()       \n",
        "valid_tokenizer.fit_on_texts(valid_sentences)                  # instantiate tokeniser\n",
        "valid_encoded = valid_tokenizer.texts_to_sequences(valid_sentences)\n",
        "\n",
        "test_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "test_tokenizer.fit_on_texts(test_sentences)                    # fit tokeniser on data\n",
        "test_encoded = test_tokenizer.texts_to_sequences(test_sentences)  # use the tokeniser to encode input sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MSvhz4IvmziU"
      },
      "outputs": [],
      "source": [
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(train_tags)\n",
        "tags_encoded = tag_tokenizer.texts_to_sequences(train_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We pick the differences between the 3 vocabularies and we concatenate them"
      ],
      "metadata": {
        "id": "IYaZwQfqWoDq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FMsG0Yw1l50L"
      },
      "outputs": [],
      "source": [
        "vocabulary = list(set(train_tokenizer.word_index.keys()))\n",
        "vocabulary += list(set(valid_tokenizer.word_index.keys()) - \n",
        "                   set(train_tokenizer.word_index.keys()))\n",
        "vocabulary += list(set(test_tokenizer.word_index.keys()) - set(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dS9RwnOtJuas"
      },
      "outputs": [],
      "source": [
        "word_index = dict(zip(vocabulary, range(2, len(vocabulary)+3)))\n",
        "word_index['-PAD-'] = 0\n",
        "word_index['-OOV-'] = 1\n",
        "\n",
        "tags = set([item for sublist in train_tags for item in sublist])\n",
        "\n",
        "tag2index = {t: i + 1 for i, t in enumerate(tags)}\n",
        "tag2index['-PAD-'] = 0 "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and setup the GloVe Model"
      ],
      "metadata": {
        "id": "it-cE6dfR6Lj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoj4Zx51J2MU",
        "outputId": "93a45f71-1fde-41d4-c1d3-1c9110e7dbe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-20 17:45:45--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-12-20 17:45:45--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-12-20 17:45:46--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.00MB/s    in 2m 41s  \n",
            "\n",
            "2021-12-20 17:48:27 (5.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc-T6FcLKtS9",
        "outputId": "2206b529-7e53-46d7-c6b0-953fe21fcb4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "path_to_glove_file = os.path.join(os.getcwd(), \"glove.6B.100d.txt\")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2IN3Mh-m85T"
      },
      "source": [
        "# Embedding\n",
        "- padding\n",
        "- one hot encoding of tags\n",
        "- punctuation identification\n",
        "\n",
        "Create the embedding matrix which maps each word index to its GloVe embedding (100 dimensions).\n",
        "\n",
        "We had to consider what to do with the words that were not present in the downloaded embeddings: we chose to map them randomly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0mOStEPMeXt",
        "outputId": "8c595d6d-ed5f-41ba-8c4f-b884442c23b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 10271 words (678 misses)\n"
          ]
        }
      ],
      "source": [
        "num_tokens = len(vocabulary) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        embedding_matrix += np.random.uniform(low=-0.05,\n",
        "                                              high=0.05,\n",
        "                                              size=embedding_dim)\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup the embedding layer and the maximum length based on the sentences of each set"
      ],
      "metadata": {
        "id": "8aVXEf0skeQa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OpYrHs0RMtQJ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "f8EZM31Js06-"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = len(max(train_sentences, key=len))\n",
        "MAX_LENGTH2 = len(max(valid_sentences, key=len))\n",
        "MAX_LENGTH3 = len(max(test_sentences, key=len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "iSRa9SwTuNrX"
      },
      "outputs": [],
      "source": [
        "train_sentences_X, valid_sentences_X, train_tags_y, valid_tags_y, = [], [], [], []\n",
        " \n",
        "for sentence in train_sentences:\n",
        "    sentence_int = []\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            sentence_int.append(word_index[word.lower()])\n",
        "        except KeyError:\n",
        "            sentence_int.append(word_index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(sentence_int)\n",
        "\n",
        "for sentence in valid_sentences:\n",
        "    sentence_int = []\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            sentence_int.append(word_index[word.lower()])\n",
        "        except KeyError:\n",
        "            sentence_int.append(word_index['-OOV-'])\n",
        " \n",
        "    valid_sentences_X.append(sentence_int)\n",
        "\n",
        "\n",
        "for sentence in train_tags:\n",
        "    train_tags_y.append([tag2index[tag] for tag in sentence])\n",
        "\n",
        "for sentence in valid_tags:\n",
        "    valid_tags_y.append([tag2index[tag] for tag in sentence])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padded the sentences to the length of the longest one present in the train split (MAX_LENGTH)"
      ],
      "metadata": {
        "id": "kzvPLh_Snn4V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Yf-XlSrStQx3"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_sentences_X = pad_sequences(valid_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_tags_y = pad_sequences(valid_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the method to categorize and map tags into integer indexes"
      ],
      "metadata": {
        "id": "IxzClSXao14z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "nUtevCQrnskt"
      },
      "outputs": [],
      "source": [
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define categories for punctuation, symbols and padding"
      ],
      "metadata": {
        "id": "cYoq3Ui5pT9N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "eT8PjDIynuHG"
      },
      "outputs": [],
      "source": [
        "point = [tag2index['.']]\n",
        "virg = [tag2index[',']]\n",
        "weird_apex = [tag2index['``']]\n",
        "single_apex = [tag2index[\"''\"]]\n",
        "two_dots = [tag2index[':']]\n",
        "pad = [tag2index['-PAD-']]\n",
        "\n",
        "punct_cat_classes = to_categorical([point, virg, weird_apex, single_apex, two_dots, pad], len(tag2index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAqZ0Xkrnw_d",
        "outputId": "3fcb52d5-c6be-4f47-87e5-e5eb7a28061a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "cumulative_tags = np.zeros(46)\n",
        "for i in punct_cat_classes:\n",
        "  print(i[0])\n",
        "  cumulative_tags += i[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qzJNfc1xn7jA"
      },
      "outputs": [],
      "source": [
        "where_tags = np.where(np.logical_not(cumulative_tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "LrzwDP5Wn9TK"
      },
      "outputs": [],
      "source": [
        "no_punct_indexes = where_tags[0]\n",
        "n_classes = len(no_punct_indexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Ik32ea9dn-5m"
      },
      "outputs": [],
      "source": [
        "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "cat_val_tags_y = to_categorical(valid_tags_y, len(tag2index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbT_r953oBA5"
      },
      "source": [
        "#Mini Data Vizualization Inset\n",
        "Observe the distribution of the tags and their frequency\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "sPiOzlwroPuJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "YIyRpFGVoKlH"
      },
      "outputs": [],
      "source": [
        "distribution_tags = np.zeros(46)\n",
        "for i in cat_train_tags_y:\n",
        "  for t in i:\n",
        "    distribution_tags += t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "OAbGJViloL0O"
      },
      "outputs": [],
      "source": [
        "s = sum(distribution_tags[1:])\n",
        "norm = [float(i)/s for i in distribution_tags[1:]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "px2v7JyxoQrz",
        "outputId": "3197b598-6837-4914-d7a6-5893c7fba984"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.bar(np.arange(len(norm)),norm)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0m_PqWUoRsV"
      },
      "source": [
        "#F1 Metric\n",
        "Definition of the methods to calculate the F1 Metric\n",
        "\n",
        "##Precision metric.\n",
        "```\n",
        "Only computes a batch-wise average of precision.\n",
        "Computes the precision, a metric for multi-label classification of\n",
        "how many selected items are relevant.\n",
        "```\n",
        "\n",
        "##Recall metric.\n",
        "\n",
        "```\n",
        "Only computes a batch-wise average of recall.\n",
        "Computes the recall, a metric for multi-label classification of\n",
        "how many relevant items are selected.\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ZDaBrpk3oYXO"
      },
      "outputs": [],
      "source": [
        "def precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def f1_binary(y_true, y_pred):\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
        "\n",
        "def metric_wrapper(index):\n",
        "  def f1_for_class(y_true, y_pred):\n",
        "    #get only the desired class\n",
        "    true = y_true[:,:,index]\n",
        "    pred = y_pred[:,:,index]\n",
        "    #return dice per class\n",
        "    tmp = f1_binary(true,pred)\n",
        "    return tmp\n",
        "  f1_for_class.__name__ = 'f1_' + str(index)\n",
        "  return f1_for_class\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    result = 0.0\n",
        "    for class_id in no_punct_indexes:\n",
        "        y_true_single_class = y_true[:,:,class_id]\n",
        "        y_pred_single_class = y_pred[:,:,class_id]\n",
        "        f1_single = f1_binary(y_true_single_class, y_pred_single_class)\n",
        "        result += f1_single / float(n_classes)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQz388M4oaCW"
      },
      "source": [
        "#Model compile and fit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of the model, in this case we're using:\n",
        "\n",
        "*   Input Layer\n",
        "*   Embedding Layer\n",
        "*   Bidirectional LSTM with 256 units\n",
        "*   Two dense layer\n",
        "*   Activation layer with softmax activation function\n",
        "\n",
        "The model is compiled using *categorical_crossentropy* as loss,\n",
        "*Adam(0.001)* optimizer, *accuracy* and *macro_f1* as metrics.\n",
        "\n",
        "Then the trainingis done for 40 epochs on a batch size of 128.\n"
      ],
      "metadata": {
        "id": "sfli4cpdt1Qh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsVGByW_sdec",
        "outputId": "f733bae9-413b-4892-85b4-a45f35e2ac46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 249, 100)          1094900   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 249, 512)         731136    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 249, 46)           23598     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 249, 46)           2162      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,851,796\n",
            "Trainable params: 756,896\n",
            "Non-trainable params: 1,094,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        " \n",
        "\n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(embedding_layer)\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(Dense(len(tag2index)))\n",
        "model.add(Dense(len(tag2index)))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=Adam(0.001),\n",
        "             metrics=['accuracy', f1, [metric_wrapper(i) for i in no_punct_indexes]])\n",
        " \n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWIokkJwokrV",
        "outputId": "22f4d490-2242-4019-b4bb-6165e1cb5f91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "16/16 [==============================] - 100s 5s/step - loss: 0.7419 - accuracy: 0.8470 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.3528 - val_accuracy: 0.9169 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - 78s 5s/step - loss: 0.3180 - accuracy: 0.9170 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2979 - val_accuracy: 0.9238 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - 84s 5s/step - loss: 0.2861 - accuracy: 0.9276 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2747 - val_accuracy: 0.9304 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - 79s 5s/step - loss: 0.2657 - accuracy: 0.9340 - f1: 0.0000e+00 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0000e+00 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2584 - val_accuracy: 0.9375 - val_f1: 0.0000e+00 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0000e+00 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.2483 - accuracy: 0.9401 - f1: 2.6085e-04 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.0104 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2411 - val_accuracy: 0.9418 - val_f1: 7.4110e-04 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0296 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.2295 - accuracy: 0.9436 - f1: 0.0036 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.1440 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.0000e+00 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2215 - val_accuracy: 0.9444 - val_f1: 0.0052 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.2089 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.0000e+00 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - 83s 5s/step - loss: 0.2093 - accuracy: 0.9459 - f1: 0.0129 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.3592 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 8.9095e-04 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.1312 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0000e+00 - f1_36: 0.0000e+00 - f1_37: 0.0242 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.2007 - val_accuracy: 0.9480 - val_f1: 0.0205 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.3224 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0078 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.3872 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0000e+00 - val_f1_36: 0.0000e+00 - val_f1_37: 0.1015 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - 78s 5s/step - loss: 0.1886 - accuracy: 0.9504 - f1: 0.0384 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.4801 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0336 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.4670 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0033 - f1_36: 0.0000e+00 - f1_37: 0.5518 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1806 - val_accuracy: 0.9528 - val_f1: 0.0482 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.4346 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.1303 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.4955 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.0292 - val_f1_36: 0.0000e+00 - val_f1_37: 0.8396 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.1690 - accuracy: 0.9553 - f1: 0.0567 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.5407 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.1432 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.6382 - f1_31: 6.3131e-04 - f1_32: 0.0000e+00 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.0734 - f1_36: 0.0000e+00 - f1_37: 0.8730 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1620 - val_accuracy: 0.9578 - val_f1: 0.0666 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.5226 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.2470 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.7102 - val_f1_31: 0.0135 - val_f1_32: 0.0000e+00 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.2696 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9021 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - 78s 5s/step - loss: 0.1514 - accuracy: 0.9604 - f1: 0.0793 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.5846 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.2980 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.0195 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.3130 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.7472 - f1_31: 0.0391 - f1_32: 0.0012 - f1_33: 0.0000e+00 - f1_34: 0.0000e+00 - f1_35: 0.2622 - f1_36: 0.0000e+00 - f1_37: 0.9065 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1461 - val_accuracy: 0.9631 - val_f1: 0.0987 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.5660 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.4102 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.0134 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.7220 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.7567 - val_f1_31: 0.1033 - val_f1_32: 0.0158 - val_f1_33: 0.0000e+00 - val_f1_34: 0.0000e+00 - val_f1_35: 0.4449 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9138 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.1361 - accuracy: 0.9654 - f1: 0.1126 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.6375 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.4289 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.1778 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.8809 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.8028 - f1_31: 0.1703 - f1_32: 0.0611 - f1_33: 0.0196 - f1_34: 0.0000e+00 - f1_35: 0.4073 - f1_36: 0.0000e+00 - f1_37: 0.9099 - f1_38: 0.0000e+00 - f1_39: 0.0085 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1326 - val_accuracy: 0.9664 - val_f1: 0.1241 - val_f1_1: 0.0000e+00 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.6266 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.4802 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.2691 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.9703 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.8094 - val_f1_31: 0.1946 - val_f1_32: 0.1038 - val_f1_33: 0.0518 - val_f1_34: 0.0000e+00 - val_f1_35: 0.5392 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9124 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0062 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - 78s 5s/step - loss: 0.1230 - accuracy: 0.9690 - f1: 0.1430 - f1_1: 0.0000e+00 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.6840 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.5233 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.3910 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9820 - f1_24: 0.0000e+00 - f1_25: 0.0000e+00 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.8440 - f1_31: 0.3320 - f1_32: 0.1854 - f1_33: 0.1336 - f1_34: 0.0000e+00 - f1_35: 0.5129 - f1_36: 0.0000e+00 - f1_37: 0.9112 - f1_38: 0.1134 - f1_39: 0.1052 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1210 - val_accuracy: 0.9692 - val_f1: 0.1504 - val_f1_1: 0.0490 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.6362 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.6093 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.3719 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.9835 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0044 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.8642 - val_f1_31: 0.3233 - val_f1_32: 0.2534 - val_f1_33: 0.1796 - val_f1_34: 0.0000e+00 - val_f1_35: 0.5775 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9128 - val_f1_38: 0.1326 - val_f1_39: 0.1182 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.1120 - accuracy: 0.9717 - f1: 0.1885 - f1_1: 0.3316 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.7133 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.6019 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.5446 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9972 - f1_24: 0.0000e+00 - f1_25: 0.0320 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.8703 - f1_31: 0.4456 - f1_32: 0.3208 - f1_33: 0.3356 - f1_34: 0.0000e+00 - f1_35: 0.6006 - f1_36: 0.0000e+00 - f1_37: 0.9127 - f1_38: 0.5489 - f1_39: 0.2843 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1114 - val_accuracy: 0.9715 - val_f1: 0.2053 - val_f1_1: 0.6043 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.6669 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.6299 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.6764 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.9986 - val_f1_24: 0.0000e+00 - val_f1_25: 0.1573 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.8713 - val_f1_31: 0.4598 - val_f1_32: 0.3130 - val_f1_33: 0.3965 - val_f1_34: 0.0000e+00 - val_f1_35: 0.6406 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9091 - val_f1_38: 0.6083 - val_f1_39: 0.2795 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.1026 - accuracy: 0.9742 - f1: 0.2300 - f1_1: 0.8133 - f1_2: 0.0000e+00 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.7378 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.6472 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.6685 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9982 - f1_24: 0.0000e+00 - f1_25: 0.2265 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.8812 - f1_31: 0.5339 - f1_32: 0.3899 - f1_33: 0.5185 - f1_34: 0.0000e+00 - f1_35: 0.6599 - f1_36: 0.0000e+00 - f1_37: 0.9119 - f1_38: 0.7753 - f1_39: 0.4381 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.1030 - val_accuracy: 0.9738 - val_f1: 0.2394 - val_f1_1: 0.9761 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.6966 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.6474 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.7091 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.0000e+00 - val_f1_25: 0.4834 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.8823 - val_f1_31: 0.5587 - val_f1_32: 0.4165 - val_f1_33: 0.4857 - val_f1_34: 0.0000e+00 - val_f1_35: 0.6752 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9101 - val_f1_38: 0.7341 - val_f1_39: 0.3992 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0945 - accuracy: 0.9761 - f1: 0.2592 - f1_1: 0.9866 - f1_2: 0.0000e+00 - f1_3: 0.0040 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.7619 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.0746 - f1_11: 0.6817 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.7490 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9987 - f1_24: 0.0600 - f1_25: 0.5251 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.8945 - f1_31: 0.6031 - f1_32: 0.4614 - f1_33: 0.6186 - f1_34: 0.0000e+00 - f1_35: 0.6904 - f1_36: 0.0000e+00 - f1_37: 0.9097 - f1_38: 0.8195 - f1_39: 0.5296 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.0960 - val_accuracy: 0.9751 - val_f1: 0.2668 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0019 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.7061 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.3638 - val_f1_11: 0.6841 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.8245 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.0740 - val_f1_25: 0.5798 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.8946 - val_f1_31: 0.5336 - val_f1_32: 0.4880 - val_f1_33: 0.6072 - val_f1_34: 0.0000e+00 - val_f1_35: 0.7495 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9145 - val_f1_38: 0.7633 - val_f1_39: 0.4780 - val_f1_41: 0.0134 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0879 - accuracy: 0.9777 - f1: 0.2923 - f1_1: 0.9929 - f1_2: 0.0000e+00 - f1_3: 0.0171 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.7635 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.6369 - f1_11: 0.7070 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.8008 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9982 - f1_24: 0.2229 - f1_25: 0.6616 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.9000 - f1_31: 0.6522 - f1_32: 0.5101 - f1_33: 0.6725 - f1_34: 0.0000e+00 - f1_35: 0.7145 - f1_36: 0.0000e+00 - f1_37: 0.9166 - f1_38: 0.8272 - f1_39: 0.6080 - f1_41: 0.0919 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0000e+00 - val_loss: 0.0899 - val_accuracy: 0.9768 - val_f1: 0.2977 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0223 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.7400 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.7003 - val_f1_11: 0.6982 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.8508 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.2974 - val_f1_25: 0.7200 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.8957 - val_f1_31: 0.5805 - val_f1_32: 0.4763 - val_f1_33: 0.6687 - val_f1_34: 0.0000e+00 - val_f1_35: 0.7624 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9178 - val_f1_38: 0.7737 - val_f1_39: 0.5596 - val_f1_41: 0.2480 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0000e+00\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0820 - accuracy: 0.9792 - f1: 0.3138 - f1_1: 0.9945 - f1_2: 0.0000e+00 - f1_3: 0.0496 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.7835 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.7858 - f1_11: 0.7285 - f1_12: 0.0000e+00 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.8307 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9986 - f1_24: 0.3426 - f1_25: 0.7507 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.9070 - f1_31: 0.6805 - f1_32: 0.5353 - f1_33: 0.7211 - f1_34: 0.0000e+00 - f1_35: 0.7517 - f1_36: 0.0000e+00 - f1_37: 0.9181 - f1_38: 0.8275 - f1_39: 0.6483 - f1_41: 0.2914 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0000e+00 - f1_45: 0.0087 - val_loss: 0.0848 - val_accuracy: 0.9779 - val_f1: 0.3177 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.1895 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.6864 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.8252 - val_f1_11: 0.7188 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0047 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.8469 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.4499 - val_f1_25: 0.7875 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.9073 - val_f1_31: 0.6600 - val_f1_32: 0.5784 - val_f1_33: 0.6769 - val_f1_34: 0.0000e+00 - val_f1_35: 0.7656 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9260 - val_f1_38: 0.7737 - val_f1_39: 0.5987 - val_f1_41: 0.2878 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0285\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0771 - accuracy: 0.9803 - f1: 0.3293 - f1_1: 0.9921 - f1_2: 0.0000e+00 - f1_3: 0.1082 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.7918 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.8832 - f1_11: 0.7370 - f1_12: 0.0000e+00 - f1_13: 0.0020 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.8661 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9986 - f1_24: 0.4204 - f1_25: 0.7892 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.9119 - f1_31: 0.7203 - f1_32: 0.5564 - f1_33: 0.7501 - f1_34: 0.0000e+00 - f1_35: 0.7700 - f1_36: 0.0000e+00 - f1_37: 0.9249 - f1_38: 0.8262 - f1_39: 0.6809 - f1_41: 0.4045 - f1_42: 0.0000e+00 - f1_43: 0.0123 - f1_44: 0.0000e+00 - f1_45: 0.0280 - val_loss: 0.0797 - val_accuracy: 0.9791 - val_f1: 0.3306 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.0797 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.7237 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9113 - val_f1_11: 0.7579 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0319 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.8873 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.5193 - val_f1_25: 0.8127 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.9068 - val_f1_31: 0.6356 - val_f1_32: 0.5678 - val_f1_33: 0.7527 - val_f1_34: 0.0000e+00 - val_f1_35: 0.7840 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9311 - val_f1_38: 0.7730 - val_f1_39: 0.6062 - val_f1_41: 0.3894 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0819 - val_f1_44: 0.0000e+00 - val_f1_45: 0.0732\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0726 - accuracy: 0.9814 - f1: 0.3455 - f1_1: 0.9934 - f1_2: 0.0000e+00 - f1_3: 0.1715 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.7984 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9211 - f1_11: 0.7610 - f1_12: 0.0000e+00 - f1_13: 0.0102 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.8756 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9986 - f1_24: 0.4212 - f1_25: 0.8200 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.9165 - f1_31: 0.7295 - f1_32: 0.5765 - f1_33: 0.7593 - f1_34: 0.0000e+00 - f1_35: 0.7827 - f1_36: 0.0000e+00 - f1_37: 0.9302 - f1_38: 0.8241 - f1_39: 0.7005 - f1_41: 0.4628 - f1_42: 0.0000e+00 - f1_43: 0.2907 - f1_44: 0.0000e+00 - f1_45: 0.0782 - val_loss: 0.0761 - val_accuracy: 0.9799 - val_f1: 0.3481 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.1677 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.7409 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9346 - val_f1_11: 0.7604 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0613 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.8972 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.5342 - val_f1_25: 0.8365 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.9181 - val_f1_31: 0.6186 - val_f1_32: 0.5871 - val_f1_33: 0.7879 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8021 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9377 - val_f1_38: 0.7737 - val_f1_39: 0.6458 - val_f1_41: 0.4692 - val_f1_42: 0.0000e+00 - val_f1_43: 0.3271 - val_f1_44: 0.0000e+00 - val_f1_45: 0.1268\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - 78s 5s/step - loss: 0.0688 - accuracy: 0.9822 - f1: 0.3634 - f1_1: 0.9945 - f1_2: 0.0000e+00 - f1_3: 0.2252 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8085 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9350 - f1_11: 0.7734 - f1_12: 0.0000e+00 - f1_13: 0.0224 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.9078 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9987 - f1_24: 0.4819 - f1_25: 0.8534 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.9184 - f1_31: 0.7436 - f1_32: 0.6027 - f1_33: 0.7908 - f1_34: 0.0000e+00 - f1_35: 0.8006 - f1_36: 0.0000e+00 - f1_37: 0.9355 - f1_38: 0.8225 - f1_39: 0.7239 - f1_41: 0.6082 - f1_42: 0.0000e+00 - f1_43: 0.4659 - f1_44: 0.0000e+00 - f1_45: 0.1231 - val_loss: 0.0727 - val_accuracy: 0.9808 - val_f1: 0.3619 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.2681 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.7225 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9418 - val_f1_11: 0.7729 - val_f1_12: 0.0000e+00 - val_f1_13: 0.1019 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.9003 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.5753 - val_f1_25: 0.8527 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.9187 - val_f1_31: 0.6742 - val_f1_32: 0.6272 - val_f1_33: 0.7897 - val_f1_34: 0.0000e+00 - val_f1_35: 0.7950 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9418 - val_f1_38: 0.7737 - val_f1_39: 0.6732 - val_f1_41: 0.5020 - val_f1_42: 0.0000e+00 - val_f1_43: 0.4750 - val_f1_44: 0.0000e+00 - val_f1_45: 0.1734\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0654 - accuracy: 0.9831 - f1: 0.3772 - f1_1: 0.9934 - f1_2: 0.0000e+00 - f1_3: 0.2890 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8106 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9356 - f1_11: 0.7770 - f1_12: 0.0000e+00 - f1_13: 0.0944 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.9206 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9986 - f1_24: 0.4967 - f1_25: 0.8614 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.9251 - f1_31: 0.7644 - f1_32: 0.6313 - f1_33: 0.8094 - f1_34: 0.0000e+00 - f1_35: 0.8088 - f1_36: 0.0000e+00 - f1_37: 0.9386 - f1_38: 0.8240 - f1_39: 0.7389 - f1_41: 0.6556 - f1_42: 0.0000e+00 - f1_43: 0.5728 - f1_44: 0.0000e+00 - f1_45: 0.2424 - val_loss: 0.0702 - val_accuracy: 0.9815 - val_f1: 0.3677 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.2623 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.7785 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9438 - val_f1_11: 0.7608 - val_f1_12: 0.0000e+00 - val_f1_13: 0.0895 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.9164 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.5460 - val_f1_25: 0.8687 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.9231 - val_f1_31: 0.6636 - val_f1_32: 0.5735 - val_f1_33: 0.7745 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8201 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9455 - val_f1_38: 0.7717 - val_f1_39: 0.7178 - val_f1_41: 0.6416 - val_f1_42: 0.0000e+00 - val_f1_43: 0.4828 - val_f1_44: 0.0000e+00 - val_f1_45: 0.2295\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0625 - accuracy: 0.9838 - f1: 0.3883 - f1_1: 0.9930 - f1_2: 0.0000e+00 - f1_3: 0.3591 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8152 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9371 - f1_11: 0.7860 - f1_12: 0.0000e+00 - f1_13: 0.1343 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.9283 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9986 - f1_24: 0.5610 - f1_25: 0.8767 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.9294 - f1_31: 0.7796 - f1_32: 0.6356 - f1_33: 0.8099 - f1_34: 0.0000e+00 - f1_35: 0.8237 - f1_36: 0.0000e+00 - f1_37: 0.9419 - f1_38: 0.8256 - f1_39: 0.7600 - f1_41: 0.7076 - f1_42: 0.0000e+00 - f1_43: 0.6288 - f1_44: 0.0000e+00 - f1_45: 0.2992 - val_loss: 0.0667 - val_accuracy: 0.9824 - val_f1: 0.3829 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.3417 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.7770 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9532 - val_f1_11: 0.7822 - val_f1_12: 0.0000e+00 - val_f1_13: 0.1634 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_19: 0.9176 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.6140 - val_f1_25: 0.8691 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.9290 - val_f1_31: 0.6784 - val_f1_32: 0.6356 - val_f1_33: 0.8126 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8360 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9493 - val_f1_38: 0.7739 - val_f1_39: 0.7039 - val_f1_41: 0.6640 - val_f1_42: 0.0000e+00 - val_f1_43: 0.5896 - val_f1_44: 0.0000e+00 - val_f1_45: 0.3259\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0599 - accuracy: 0.9846 - f1: 0.3999 - f1_1: 0.9861 - f1_2: 0.0000e+00 - f1_3: 0.3686 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8188 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9436 - f1_11: 0.7932 - f1_12: 0.0000e+00 - f1_13: 0.2250 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_19: 0.9366 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9987 - f1_24: 0.5793 - f1_25: 0.8845 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.9329 - f1_31: 0.7911 - f1_32: 0.6544 - f1_33: 0.8291 - f1_34: 0.0000e+00 - f1_35: 0.8268 - f1_36: 0.0000e+00 - f1_37: 0.9436 - f1_38: 0.8311 - f1_39: 0.7689 - f1_41: 0.7442 - f1_42: 0.0000e+00 - f1_43: 0.7161 - f1_44: 0.0000e+00 - f1_45: 0.4212 - val_loss: 0.0646 - val_accuracy: 0.9832 - val_f1: 0.3973 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.4487 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.7628 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9603 - val_f1_11: 0.7509 - val_f1_12: 0.0000e+00 - val_f1_13: 0.2430 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0096 - val_f1_19: 0.9239 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.6399 - val_f1_25: 0.8717 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.9304 - val_f1_31: 0.7439 - val_f1_32: 0.6678 - val_f1_33: 0.8051 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8407 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9530 - val_f1_38: 0.7726 - val_f1_39: 0.7307 - val_f1_41: 0.7081 - val_f1_42: 0.0000e+00 - val_f1_43: 0.6799 - val_f1_44: 0.0000e+00 - val_f1_45: 0.4500\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0574 - accuracy: 0.9851 - f1: 0.4113 - f1_1: 0.9934 - f1_2: 0.0000e+00 - f1_3: 0.4494 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8251 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9457 - f1_11: 0.7937 - f1_12: 0.0000e+00 - f1_13: 0.2846 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0089 - f1_19: 0.9419 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9985 - f1_24: 0.6238 - f1_25: 0.8932 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.9351 - f1_31: 0.7977 - f1_32: 0.6606 - f1_33: 0.8382 - f1_34: 0.0000e+00 - f1_35: 0.8516 - f1_36: 0.0000e+00 - f1_37: 0.9462 - f1_38: 0.8315 - f1_39: 0.7811 - f1_41: 0.7864 - f1_42: 0.0000e+00 - f1_43: 0.8233 - f1_44: 0.0000e+00 - f1_45: 0.4408 - val_loss: 0.0630 - val_accuracy: 0.9832 - val_f1: 0.4050 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.4995 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.7098 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9594 - val_f1_11: 0.8076 - val_f1_12: 0.0000e+00 - val_f1_13: 0.2764 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0915 - val_f1_19: 0.9180 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.6798 - val_f1_25: 0.8678 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.9230 - val_f1_31: 0.7496 - val_f1_32: 0.6748 - val_f1_33: 0.8131 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8560 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9546 - val_f1_38: 0.7781 - val_f1_39: 0.7160 - val_f1_41: 0.7133 - val_f1_42: 0.0000e+00 - val_f1_43: 0.7121 - val_f1_44: 0.0000e+00 - val_f1_45: 0.5038\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0555 - accuracy: 0.9855 - f1: 0.4240 - f1_1: 0.9913 - f1_2: 0.0000e+00 - f1_3: 0.4766 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8217 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9475 - f1_11: 0.8064 - f1_12: 0.0000e+00 - f1_13: 0.3078 - f1_14: 0.0000e+00 - f1_15: 0.0662 - f1_16: 0.0000e+00 - f1_17: 0.1741 - f1_19: 0.9434 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9984 - f1_24: 0.6289 - f1_25: 0.8966 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.9362 - f1_31: 0.8128 - f1_32: 0.6748 - f1_33: 0.8484 - f1_34: 0.0000e+00 - f1_35: 0.8564 - f1_36: 0.0000e+00 - f1_37: 0.9501 - f1_38: 0.8523 - f1_39: 0.7935 - f1_41: 0.8152 - f1_42: 0.0000e+00 - f1_43: 0.8382 - f1_44: 0.0000e+00 - f1_45: 0.5244 - val_loss: 0.0603 - val_accuracy: 0.9843 - val_f1: 0.4209 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.4373 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.7961 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9710 - val_f1_11: 0.7785 - val_f1_12: 0.0000e+00 - val_f1_13: 0.3425 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0485 - val_f1_16: 0.0000e+00 - val_f1_17: 0.2781 - val_f1_19: 0.9294 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.6746 - val_f1_25: 0.8865 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.9328 - val_f1_31: 0.7568 - val_f1_32: 0.6525 - val_f1_33: 0.8272 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8525 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9575 - val_f1_38: 0.8325 - val_f1_39: 0.7741 - val_f1_41: 0.8110 - val_f1_42: 0.0000e+00 - val_f1_43: 0.8244 - val_f1_44: 0.0000e+00 - val_f1_45: 0.4737\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0532 - accuracy: 0.9862 - f1: 0.4356 - f1_1: 0.9938 - f1_2: 0.0000e+00 - f1_3: 0.5303 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8300 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9627 - f1_11: 0.8108 - f1_12: 0.0000e+00 - f1_13: 0.4239 - f1_14: 0.0000e+00 - f1_15: 0.0962 - f1_16: 0.0000e+00 - f1_17: 0.1972 - f1_19: 0.9513 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9986 - f1_24: 0.6684 - f1_25: 0.9033 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.9395 - f1_31: 0.8192 - f1_32: 0.6867 - f1_33: 0.8607 - f1_34: 0.0000e+00 - f1_35: 0.8619 - f1_36: 0.0000e+00 - f1_37: 0.9532 - f1_38: 0.8613 - f1_39: 0.8031 - f1_41: 0.8597 - f1_42: 0.0000e+00 - f1_43: 0.8701 - f1_44: 0.0000e+00 - f1_45: 0.5418 - val_loss: 0.0580 - val_accuracy: 0.9846 - val_f1: 0.4242 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.4650 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.7768 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9829 - val_f1_11: 0.8086 - val_f1_12: 0.0000e+00 - val_f1_13: 0.3831 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0485 - val_f1_16: 0.0000e+00 - val_f1_17: 0.1992 - val_f1_19: 0.9311 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.6957 - val_f1_25: 0.8877 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.9408 - val_f1_31: 0.7741 - val_f1_32: 0.6698 - val_f1_33: 0.8312 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8687 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9574 - val_f1_38: 0.8020 - val_f1_39: 0.7614 - val_f1_41: 0.8110 - val_f1_42: 0.0000e+00 - val_f1_43: 0.8587 - val_f1_44: 0.0000e+00 - val_f1_45: 0.5148\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0512 - accuracy: 0.9868 - f1: 0.4454 - f1_1: 0.9941 - f1_2: 0.0000e+00 - f1_3: 0.5487 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8390 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9732 - f1_11: 0.8189 - f1_12: 0.0000e+00 - f1_13: 0.4614 - f1_14: 0.0000e+00 - f1_15: 0.1455 - f1_16: 0.0000e+00 - f1_17: 0.2499 - f1_19: 0.9510 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9973 - f1_24: 0.6982 - f1_25: 0.9159 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.9439 - f1_31: 0.8279 - f1_32: 0.7016 - f1_33: 0.8614 - f1_34: 0.0000e+00 - f1_35: 0.8768 - f1_36: 0.0000e+00 - f1_37: 0.9546 - f1_38: 0.8895 - f1_39: 0.8110 - f1_41: 0.8702 - f1_42: 0.0000e+00 - f1_43: 0.8967 - f1_44: 0.0000e+00 - f1_45: 0.5878 - val_loss: 0.0570 - val_accuracy: 0.9848 - val_f1: 0.4404 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.5200 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.8034 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9847 - val_f1_11: 0.7775 - val_f1_12: 0.0000e+00 - val_f1_13: 0.3501 - val_f1_14: 0.0000e+00 - val_f1_15: 0.1894 - val_f1_16: 0.0000e+00 - val_f1_17: 0.4299 - val_f1_19: 0.9365 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.6958 - val_f1_25: 0.9024 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.9382 - val_f1_31: 0.7890 - val_f1_32: 0.6692 - val_f1_33: 0.8416 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8608 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9582 - val_f1_38: 0.8671 - val_f1_39: 0.7860 - val_f1_41: 0.9036 - val_f1_42: 0.0000e+00 - val_f1_43: 0.8691 - val_f1_44: 0.0000e+00 - val_f1_45: 0.5467\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0497 - accuracy: 0.9871 - f1: 0.4586 - f1_1: 0.9929 - f1_2: 0.0000e+00 - f1_3: 0.5751 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8452 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9746 - f1_11: 0.8228 - f1_12: 0.0000e+00 - f1_13: 0.4887 - f1_14: 0.0000e+00 - f1_15: 0.2435 - f1_16: 0.0000e+00 - f1_17: 0.4769 - f1_19: 0.9566 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9989 - f1_24: 0.7114 - f1_25: 0.9122 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.9448 - f1_31: 0.8271 - f1_32: 0.7088 - f1_33: 0.8663 - f1_34: 0.0000e+00 - f1_35: 0.8769 - f1_36: 0.0000e+00 - f1_37: 0.9544 - f1_38: 0.9093 - f1_39: 0.8231 - f1_41: 0.9071 - f1_42: 0.0000e+00 - f1_43: 0.9139 - f1_44: 0.0000e+00 - f1_45: 0.6133 - val_loss: 0.0552 - val_accuracy: 0.9855 - val_f1: 0.4515 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.6173 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.7922 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9847 - val_f1_11: 0.8132 - val_f1_12: 0.0000e+00 - val_f1_13: 0.4636 - val_f1_14: 0.0000e+00 - val_f1_15: 0.1575 - val_f1_16: 0.0000e+00 - val_f1_17: 0.6511 - val_f1_19: 0.9434 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.7215 - val_f1_25: 0.9018 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.9297 - val_f1_31: 0.7738 - val_f1_32: 0.6913 - val_f1_33: 0.8437 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8772 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9587 - val_f1_38: 0.8412 - val_f1_39: 0.7720 - val_f1_41: 0.8524 - val_f1_42: 0.0000e+00 - val_f1_43: 0.8931 - val_f1_44: 0.0000e+00 - val_f1_45: 0.5833\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0480 - accuracy: 0.9874 - f1: 0.4666 - f1_1: 0.9945 - f1_2: 0.0000e+00 - f1_3: 0.6084 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8456 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9784 - f1_11: 0.8295 - f1_12: 0.0000e+00 - f1_13: 0.5469 - f1_14: 0.0000e+00 - f1_15: 0.2800 - f1_16: 0.0000e+00 - f1_17: 0.5730 - f1_19: 0.9579 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9987 - f1_24: 0.7387 - f1_25: 0.9201 - f1_26: 0.0000e+00 - f1_27: 0.0000e+00 - f1_29: 0.9446 - f1_31: 0.8378 - f1_32: 0.7133 - f1_33: 0.8713 - f1_34: 0.0000e+00 - f1_35: 0.8827 - f1_36: 0.0000e+00 - f1_37: 0.9571 - f1_38: 0.9111 - f1_39: 0.8254 - f1_41: 0.8911 - f1_42: 0.0000e+00 - f1_43: 0.9290 - f1_44: 0.0000e+00 - f1_45: 0.6302 - val_loss: 0.0538 - val_accuracy: 0.9855 - val_f1: 0.4503 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.5242 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.8093 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9892 - val_f1_11: 0.8101 - val_f1_12: 0.0000e+00 - val_f1_13: 0.4386 - val_f1_14: 0.0000e+00 - val_f1_15: 0.1894 - val_f1_16: 0.0000e+00 - val_f1_17: 0.5358 - val_f1_19: 0.9392 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.6999 - val_f1_25: 0.9053 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.9447 - val_f1_31: 0.7770 - val_f1_32: 0.6731 - val_f1_33: 0.8483 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8797 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9610 - val_f1_38: 0.8853 - val_f1_39: 0.7988 - val_f1_41: 0.9008 - val_f1_42: 0.0000e+00 - val_f1_43: 0.8946 - val_f1_44: 0.0000e+00 - val_f1_45: 0.6114\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0466 - accuracy: 0.9879 - f1: 0.4718 - f1_1: 0.9903 - f1_2: 0.0000e+00 - f1_3: 0.6242 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8466 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9846 - f1_11: 0.8301 - f1_12: 0.0000e+00 - f1_13: 0.5269 - f1_14: 0.0000e+00 - f1_15: 0.3550 - f1_16: 0.0000e+00 - f1_17: 0.5820 - f1_19: 0.9562 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9986 - f1_24: 0.7447 - f1_25: 0.9183 - f1_26: 0.0096 - f1_27: 0.0000e+00 - f1_29: 0.9472 - f1_31: 0.8415 - f1_32: 0.7209 - f1_33: 0.8780 - f1_34: 0.0000e+00 - f1_35: 0.8905 - f1_36: 0.0000e+00 - f1_37: 0.9587 - f1_38: 0.9308 - f1_39: 0.8349 - f1_41: 0.9122 - f1_42: 0.0000e+00 - f1_43: 0.9439 - f1_44: 0.0000e+00 - f1_45: 0.6478 - val_loss: 0.0527 - val_accuracy: 0.9858 - val_f1: 0.4555 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.6570 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.7792 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9847 - val_f1_11: 0.8217 - val_f1_12: 0.0000e+00 - val_f1_13: 0.4827 - val_f1_14: 0.0000e+00 - val_f1_15: 0.2374 - val_f1_16: 0.0000e+00 - val_f1_17: 0.5228 - val_f1_19: 0.9465 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.7131 - val_f1_25: 0.9135 - val_f1_26: 0.0000e+00 - val_f1_27: 0.0000e+00 - val_f1_29: 0.9461 - val_f1_31: 0.8086 - val_f1_32: 0.6870 - val_f1_33: 0.8595 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8838 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9618 - val_f1_38: 0.8497 - val_f1_39: 0.8091 - val_f1_41: 0.8962 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9183 - val_f1_44: 0.0000e+00 - val_f1_45: 0.5428\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - 80s 5s/step - loss: 0.0452 - accuracy: 0.9880 - f1: 0.4767 - f1_1: 0.9952 - f1_2: 0.0000e+00 - f1_3: 0.6437 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8547 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9853 - f1_11: 0.8324 - f1_12: 0.0000e+00 - f1_13: 0.5818 - f1_14: 0.0000e+00 - f1_15: 0.3320 - f1_16: 0.0000e+00 - f1_17: 0.5494 - f1_19: 0.9585 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9987 - f1_24: 0.7656 - f1_25: 0.9221 - f1_26: 0.0857 - f1_27: 0.0000e+00 - f1_29: 0.9485 - f1_31: 0.8514 - f1_32: 0.7275 - f1_33: 0.8820 - f1_34: 0.0000e+00 - f1_35: 0.8898 - f1_36: 0.0000e+00 - f1_37: 0.9616 - f1_38: 0.9277 - f1_39: 0.8431 - f1_41: 0.9371 - f1_42: 0.0000e+00 - f1_43: 0.9574 - f1_44: 0.0000e+00 - f1_45: 0.6370 - val_loss: 0.0516 - val_accuracy: 0.9861 - val_f1: 0.4683 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.6156 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.7868 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9930 - val_f1_11: 0.8244 - val_f1_12: 0.0000e+00 - val_f1_13: 0.5559 - val_f1_14: 0.0000e+00 - val_f1_15: 0.2258 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7634 - val_f1_19: 0.9426 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.7290 - val_f1_25: 0.9105 - val_f1_26: 0.0107 - val_f1_27: 0.0000e+00 - val_f1_29: 0.9442 - val_f1_31: 0.7958 - val_f1_32: 0.6991 - val_f1_33: 0.8561 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8844 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9618 - val_f1_38: 0.9277 - val_f1_39: 0.8127 - val_f1_41: 0.9087 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9394 - val_f1_44: 0.0000e+00 - val_f1_45: 0.6472\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - 78s 5s/step - loss: 0.0439 - accuracy: 0.9885 - f1: 0.4857 - f1_1: 0.9932 - f1_2: 0.0000e+00 - f1_3: 0.6560 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8576 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9848 - f1_11: 0.8416 - f1_12: 0.0000e+00 - f1_13: 0.5913 - f1_14: 0.0000e+00 - f1_15: 0.4394 - f1_16: 0.0000e+00 - f1_17: 0.6691 - f1_19: 0.9633 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9987 - f1_24: 0.7756 - f1_25: 0.9229 - f1_26: 0.1042 - f1_27: 0.0000e+00 - f1_29: 0.9502 - f1_31: 0.8543 - f1_32: 0.7376 - f1_33: 0.8862 - f1_34: 0.0000e+00 - f1_35: 0.8865 - f1_36: 0.0000e+00 - f1_37: 0.9641 - f1_38: 0.9465 - f1_39: 0.8438 - f1_41: 0.9346 - f1_42: 0.0000e+00 - f1_43: 0.9619 - f1_44: 0.0000e+00 - f1_45: 0.6641 - val_loss: 0.0502 - val_accuracy: 0.9864 - val_f1: 0.4712 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.6513 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.8150 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9912 - val_f1_11: 0.8283 - val_f1_12: 0.0000e+00 - val_f1_13: 0.5232 - val_f1_14: 0.0000e+00 - val_f1_15: 0.2330 - val_f1_16: 0.0000e+00 - val_f1_17: 0.8009 - val_f1_19: 0.9442 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.7338 - val_f1_25: 0.9145 - val_f1_26: 0.0221 - val_f1_27: 0.0000e+00 - val_f1_29: 0.9453 - val_f1_31: 0.8062 - val_f1_32: 0.6969 - val_f1_33: 0.8538 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8870 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9632 - val_f1_38: 0.9089 - val_f1_39: 0.8113 - val_f1_41: 0.9425 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9280 - val_f1_44: 0.0000e+00 - val_f1_45: 0.6496\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0426 - accuracy: 0.9887 - f1: 0.4915 - f1_1: 0.9917 - f1_2: 0.0000e+00 - f1_3: 0.6730 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8629 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9871 - f1_11: 0.8454 - f1_12: 0.0000e+00 - f1_13: 0.6027 - f1_14: 0.0000e+00 - f1_15: 0.4362 - f1_16: 0.0000e+00 - f1_17: 0.7588 - f1_19: 0.9638 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9986 - f1_24: 0.7763 - f1_25: 0.9288 - f1_26: 0.1404 - f1_27: 0.0000e+00 - f1_29: 0.9499 - f1_31: 0.8601 - f1_32: 0.7467 - f1_33: 0.8977 - f1_34: 0.0000e+00 - f1_35: 0.8972 - f1_36: 0.0000e+00 - f1_37: 0.9645 - f1_38: 0.9491 - f1_39: 0.8451 - f1_41: 0.9578 - f1_42: 0.0000e+00 - f1_43: 0.9595 - f1_44: 0.0000e+00 - f1_45: 0.6671 - val_loss: 0.0494 - val_accuracy: 0.9865 - val_f1: 0.4778 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.6735 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.8130 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9930 - val_f1_11: 0.8344 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6017 - val_f1_14: 0.0000e+00 - val_f1_15: 0.2348 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7885 - val_f1_19: 0.9428 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.7637 - val_f1_25: 0.9161 - val_f1_26: 0.1507 - val_f1_27: 0.0000e+00 - val_f1_29: 0.9473 - val_f1_31: 0.8059 - val_f1_32: 0.6706 - val_f1_33: 0.8582 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8903 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9647 - val_f1_38: 0.9411 - val_f1_39: 0.8022 - val_f1_41: 0.9274 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9492 - val_f1_44: 0.0000e+00 - val_f1_45: 0.6437\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0418 - accuracy: 0.9887 - f1: 0.4978 - f1_1: 0.9942 - f1_2: 0.0000e+00 - f1_3: 0.6933 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8560 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9915 - f1_11: 0.8411 - f1_12: 0.0000e+00 - f1_13: 0.6196 - f1_14: 0.0000e+00 - f1_15: 0.3844 - f1_16: 0.0000e+00 - f1_17: 0.7925 - f1_19: 0.9676 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9986 - f1_24: 0.7779 - f1_25: 0.9280 - f1_26: 0.3379 - f1_27: 0.0000e+00 - f1_29: 0.9498 - f1_31: 0.8621 - f1_32: 0.7430 - f1_33: 0.8905 - f1_34: 0.0000e+00 - f1_35: 0.8987 - f1_36: 0.0000e+00 - f1_37: 0.9661 - f1_38: 0.9515 - f1_39: 0.8527 - f1_41: 0.9625 - f1_42: 0.0000e+00 - f1_43: 0.9623 - f1_44: 0.0000e+00 - f1_45: 0.6888 - val_loss: 0.0493 - val_accuracy: 0.9865 - val_f1: 0.4780 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.6619 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.7690 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9920 - val_f1_11: 0.8294 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6095 - val_f1_14: 0.0000e+00 - val_f1_15: 0.2290 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7391 - val_f1_19: 0.9474 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.7846 - val_f1_25: 0.9138 - val_f1_26: 0.1407 - val_f1_27: 0.0000e+00 - val_f1_29: 0.9481 - val_f1_31: 0.8266 - val_f1_32: 0.7240 - val_f1_33: 0.8599 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8904 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9655 - val_f1_38: 0.9505 - val_f1_39: 0.8204 - val_f1_41: 0.9126 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9678 - val_f1_44: 0.0000e+00 - val_f1_45: 0.6407\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - 79s 5s/step - loss: 0.0408 - accuracy: 0.9890 - f1: 0.5023 - f1_1: 0.9920 - f1_2: 0.0000e+00 - f1_3: 0.6879 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8636 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9884 - f1_11: 0.8467 - f1_12: 0.0000e+00 - f1_13: 0.6411 - f1_14: 0.0000e+00 - f1_15: 0.5055 - f1_16: 0.0000e+00 - f1_17: 0.8079 - f1_19: 0.9663 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9986 - f1_24: 0.7872 - f1_25: 0.9354 - f1_26: 0.2916 - f1_27: 0.0000e+00 - f1_29: 0.9513 - f1_31: 0.8659 - f1_32: 0.7565 - f1_33: 0.9018 - f1_34: 0.0000e+00 - f1_35: 0.9052 - f1_36: 0.0000e+00 - f1_37: 0.9680 - f1_38: 0.9625 - f1_39: 0.8582 - f1_41: 0.9619 - f1_42: 0.0000e+00 - f1_43: 0.9608 - f1_44: 0.0000e+00 - f1_45: 0.6862 - val_loss: 0.0478 - val_accuracy: 0.9867 - val_f1: 0.4840 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.6683 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.7918 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9930 - val_f1_11: 0.8372 - val_f1_12: 0.0000e+00 - val_f1_13: 0.5870 - val_f1_14: 0.0000e+00 - val_f1_15: 0.2567 - val_f1_16: 0.0000e+00 - val_f1_17: 0.7726 - val_f1_19: 0.9494 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0057 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.7472 - val_f1_25: 0.9120 - val_f1_26: 0.3118 - val_f1_27: 0.0000e+00 - val_f1_29: 0.9495 - val_f1_31: 0.8306 - val_f1_32: 0.7142 - val_f1_33: 0.8645 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8919 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9681 - val_f1_38: 0.9261 - val_f1_39: 0.7982 - val_f1_41: 0.9307 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9701 - val_f1_44: 0.0000e+00 - val_f1_45: 0.6864\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0396 - accuracy: 0.9893 - f1: 0.5093 - f1_1: 0.9944 - f1_2: 0.0000e+00 - f1_3: 0.7061 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8661 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9866 - f1_11: 0.8521 - f1_12: 0.0000e+00 - f1_13: 0.6548 - f1_14: 0.0000e+00 - f1_15: 0.4861 - f1_16: 0.0000e+00 - f1_17: 0.7851 - f1_19: 0.9665 - f1_20: 0.0000e+00 - f1_21: 0.0089 - f1_22: 0.0000e+00 - f1_23: 0.9987 - f1_24: 0.7899 - f1_25: 0.9311 - f1_26: 0.5231 - f1_27: 0.0000e+00 - f1_29: 0.9535 - f1_31: 0.8710 - f1_32: 0.7660 - f1_33: 0.9027 - f1_34: 0.0000e+00 - f1_35: 0.9077 - f1_36: 0.0000e+00 - f1_37: 0.9665 - f1_38: 0.9613 - f1_39: 0.8580 - f1_41: 0.9652 - f1_42: 0.0000e+00 - f1_43: 0.9717 - f1_44: 0.0000e+00 - f1_45: 0.6996 - val_loss: 0.0484 - val_accuracy: 0.9864 - val_f1: 0.4885 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.6432 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.8078 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9908 - val_f1_11: 0.7973 - val_f1_12: 0.0000e+00 - val_f1_13: 0.5336 - val_f1_14: 0.0000e+00 - val_f1_15: 0.2997 - val_f1_16: 0.0000e+00 - val_f1_17: 0.8376 - val_f1_19: 0.9631 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.7738 - val_f1_25: 0.9178 - val_f1_26: 0.3774 - val_f1_27: 0.0000e+00 - val_f1_29: 0.9492 - val_f1_31: 0.8288 - val_f1_32: 0.7202 - val_f1_33: 0.8764 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8864 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9651 - val_f1_38: 0.9569 - val_f1_39: 0.8351 - val_f1_41: 0.9677 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9648 - val_f1_44: 0.0000e+00 - val_f1_45: 0.6514\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0388 - accuracy: 0.9895 - f1: 0.5149 - f1_1: 0.9921 - f1_2: 0.0000e+00 - f1_3: 0.7057 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8711 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9827 - f1_11: 0.8522 - f1_12: 0.0000e+00 - f1_13: 0.6446 - f1_14: 0.0000e+00 - f1_15: 0.5434 - f1_16: 0.0000e+00 - f1_17: 0.8566 - f1_19: 0.9683 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.9986 - f1_24: 0.8034 - f1_25: 0.9305 - f1_26: 0.6071 - f1_27: 0.0000e+00 - f1_29: 0.9547 - f1_31: 0.8778 - f1_32: 0.7613 - f1_33: 0.9033 - f1_34: 0.0000e+00 - f1_35: 0.9053 - f1_36: 0.0000e+00 - f1_37: 0.9696 - f1_38: 0.9670 - f1_39: 0.8715 - f1_41: 0.9626 - f1_42: 0.0000e+00 - f1_43: 0.9632 - f1_44: 0.0000e+00 - f1_45: 0.7046 - val_loss: 0.0459 - val_accuracy: 0.9874 - val_f1: 0.4965 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.6865 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.8231 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9908 - val_f1_11: 0.8287 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6221 - val_f1_14: 0.0000e+00 - val_f1_15: 0.2550 - val_f1_16: 0.0000e+00 - val_f1_17: 0.8323 - val_f1_19: 0.9669 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0057 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.7772 - val_f1_25: 0.9249 - val_f1_26: 0.5303 - val_f1_27: 0.0000e+00 - val_f1_29: 0.9499 - val_f1_31: 0.8353 - val_f1_32: 0.7308 - val_f1_33: 0.8639 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8996 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9698 - val_f1_38: 0.9576 - val_f1_39: 0.8212 - val_f1_41: 0.9421 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9754 - val_f1_44: 0.0000e+00 - val_f1_45: 0.6717\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - 76s 5s/step - loss: 0.0376 - accuracy: 0.9898 - f1: 0.5196 - f1_1: 0.9948 - f1_2: 0.0000e+00 - f1_3: 0.7222 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8775 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9812 - f1_11: 0.8595 - f1_12: 0.0000e+00 - f1_13: 0.6504 - f1_14: 0.0000e+00 - f1_15: 0.4969 - f1_16: 0.0000e+00 - f1_17: 0.8655 - f1_19: 0.9735 - f1_20: 0.0417 - f1_21: 0.0156 - f1_22: 0.0000e+00 - f1_23: 0.9986 - f1_24: 0.8067 - f1_25: 0.9373 - f1_26: 0.6602 - f1_27: 0.0000e+00 - f1_29: 0.9575 - f1_31: 0.8799 - f1_32: 0.7692 - f1_33: 0.9034 - f1_34: 0.0000e+00 - f1_35: 0.9159 - f1_36: 0.0000e+00 - f1_37: 0.9694 - f1_38: 0.9750 - f1_39: 0.8733 - f1_41: 0.9718 - f1_42: 0.0000e+00 - f1_43: 0.9789 - f1_44: 0.0000e+00 - f1_45: 0.7098 - val_loss: 0.0464 - val_accuracy: 0.9871 - val_f1: 0.4969 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.7135 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.7881 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9912 - val_f1_11: 0.8377 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6533 - val_f1_14: 0.0000e+00 - val_f1_15: 0.2694 - val_f1_16: 0.0000e+00 - val_f1_17: 0.8375 - val_f1_19: 0.9654 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.7706 - val_f1_25: 0.9241 - val_f1_26: 0.5073 - val_f1_27: 0.0000e+00 - val_f1_29: 0.9495 - val_f1_31: 0.8139 - val_f1_32: 0.7269 - val_f1_33: 0.8737 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8945 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9688 - val_f1_38: 0.9495 - val_f1_39: 0.8271 - val_f1_41: 0.9361 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9800 - val_f1_44: 0.0000e+00 - val_f1_45: 0.7009\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0369 - accuracy: 0.9901 - f1: 0.5238 - f1_1: 0.9926 - f1_2: 0.0000e+00 - f1_3: 0.7285 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8806 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9880 - f1_11: 0.8609 - f1_12: 0.0000e+00 - f1_13: 0.6814 - f1_14: 0.0000e+00 - f1_15: 0.5745 - f1_16: 0.0000e+00 - f1_17: 0.8644 - f1_19: 0.9678 - f1_20: 0.0417 - f1_21: 0.0096 - f1_22: 0.0000e+00 - f1_23: 0.9986 - f1_24: 0.8226 - f1_25: 0.9394 - f1_26: 0.6854 - f1_27: 0.0000e+00 - f1_29: 0.9565 - f1_31: 0.8801 - f1_32: 0.7784 - f1_33: 0.9166 - f1_34: 0.0000e+00 - f1_35: 0.9121 - f1_36: 0.0000e+00 - f1_37: 0.9714 - f1_38: 0.9743 - f1_39: 0.8721 - f1_41: 0.9658 - f1_42: 0.0000e+00 - f1_43: 0.9758 - f1_44: 0.0000e+00 - f1_45: 0.7122 - val_loss: 0.0453 - val_accuracy: 0.9873 - val_f1: 0.4992 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.7026 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.8198 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9939 - val_f1_11: 0.8387 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6329 - val_f1_14: 0.0000e+00 - val_f1_15: 0.2694 - val_f1_16: 0.0000e+00 - val_f1_17: 0.8526 - val_f1_19: 0.9526 - val_f1_20: 0.0455 - val_f1_21: 0.0153 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.7539 - val_f1_25: 0.9247 - val_f1_26: 0.5335 - val_f1_27: 0.0000e+00 - val_f1_29: 0.9530 - val_f1_31: 0.8029 - val_f1_32: 0.7212 - val_f1_33: 0.8735 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8998 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9704 - val_f1_38: 0.9602 - val_f1_39: 0.8329 - val_f1_41: 0.9621 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9690 - val_f1_44: 0.0000e+00 - val_f1_45: 0.6890\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - 77s 5s/step - loss: 0.0359 - accuracy: 0.9901 - f1: 0.5285 - f1_1: 0.9942 - f1_2: 0.0000e+00 - f1_3: 0.7428 - f1_4: 0.0000e+00 - f1_6: 0.0000e+00 - f1_7: 0.8851 - f1_8: 0.0000e+00 - f1_9: 0.0000e+00 - f1_10: 0.9872 - f1_11: 0.8642 - f1_12: 0.0000e+00 - f1_13: 0.6805 - f1_14: 0.0000e+00 - f1_15: 0.5413 - f1_16: 0.0000e+00 - f1_17: 0.9057 - f1_19: 0.9710 - f1_20: 0.1042 - f1_21: 0.0096 - f1_22: 0.0000e+00 - f1_23: 0.9987 - f1_24: 0.8170 - f1_25: 0.9352 - f1_26: 0.7479 - f1_27: 0.0000e+00 - f1_29: 0.9582 - f1_31: 0.8824 - f1_32: 0.7810 - f1_33: 0.9130 - f1_34: 0.0000e+00 - f1_35: 0.9161 - f1_36: 0.0000e+00 - f1_37: 0.9704 - f1_38: 0.9748 - f1_39: 0.8794 - f1_41: 0.9723 - f1_42: 0.0000e+00 - f1_43: 0.9763 - f1_44: 0.0000e+00 - f1_45: 0.7317 - val_loss: 0.0444 - val_accuracy: 0.9876 - val_f1: 0.5040 - val_f1_1: 0.9975 - val_f1_2: 0.0000e+00 - val_f1_3: 0.7200 - val_f1_4: 0.0000e+00 - val_f1_6: 0.0000e+00 - val_f1_7: 0.8181 - val_f1_8: 0.0000e+00 - val_f1_9: 0.0000e+00 - val_f1_10: 0.9912 - val_f1_11: 0.8343 - val_f1_12: 0.0000e+00 - val_f1_13: 0.6478 - val_f1_14: 0.0000e+00 - val_f1_15: 0.3958 - val_f1_16: 0.0000e+00 - val_f1_17: 0.8211 - val_f1_19: 0.9669 - val_f1_20: 0.0455 - val_f1_21: 0.0228 - val_f1_22: 0.0000e+00 - val_f1_23: 1.0000 - val_f1_24: 0.7656 - val_f1_25: 0.9264 - val_f1_26: 0.5130 - val_f1_27: 0.0000e+00 - val_f1_29: 0.9518 - val_f1_31: 0.8365 - val_f1_32: 0.7367 - val_f1_33: 0.8803 - val_f1_34: 0.0000e+00 - val_f1_35: 0.8985 - val_f1_36: 0.0000e+00 - val_f1_37: 0.9716 - val_f1_38: 0.9707 - val_f1_39: 0.8411 - val_f1_41: 0.9467 - val_f1_42: 0.0000e+00 - val_f1_43: 0.9797 - val_f1_44: 0.0000e+00 - val_f1_45: 0.6803\n"
          ]
        }
      ],
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "history = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=40, validation_data=(valid_sentences_X, cat_val_tags_y), callbacks=[callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_XNg1aXonOl"
      },
      "source": [
        "#History f1 for class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "DIeuPw8AorGd"
      },
      "outputs": [],
      "source": [
        "f1_val_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_val_list[i] = history.history['val_f1_{}'.format(i)][-1]\n",
        "f1_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_list[i] = history.history['f1_{}'.format(i)][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "dlQ8zRSMou1-"
      },
      "outputs": [],
      "source": [
        "index2tag = {value : key for (key, value) in tag2index.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKsg3Ui-owlf",
        "outputId": "22bb1456-bbb8-4c15-b407-441ad9e6400b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: $ --- F1: 0.994209349155426\n",
            "Tag: LS --- F1: 0.0\n",
            "Tag: RB --- F1: 0.7427894473075867\n",
            "Tag: PDT --- F1: 0.0\n",
            "Tag: WRB --- F1: 0.0\n",
            "Tag: NNP --- F1: 0.8851285576820374\n",
            "Tag: SYM --- F1: 0.0\n",
            "Tag: NNPS --- F1: 0.0\n",
            "Tag: POS --- F1: 0.9871503710746765\n",
            "Tag: NN --- F1: 0.8641887307167053\n",
            "Tag: WP$ --- F1: 0.0\n",
            "Tag: VBG --- F1: 0.6804764866828918\n",
            "Tag: JJS --- F1: 0.0\n",
            "Tag: RP --- F1: 0.5413431525230408\n",
            "Tag: -RRB- --- F1: 0.0\n",
            "Tag: WDT --- F1: 0.9056588411331177\n",
            "Tag: PRP --- F1: 0.9710192084312439\n",
            "Tag: EX --- F1: 0.1041666567325592\n",
            "Tag: JJR --- F1: 0.009615384042263031\n",
            "Tag: # --- F1: 0.0\n",
            "Tag: TO --- F1: 0.9986948370933533\n",
            "Tag: VBP --- F1: 0.8170299530029297\n",
            "Tag: VBZ --- F1: 0.9352047443389893\n",
            "Tag: WP --- F1: 0.7479190230369568\n",
            "Tag: FW --- F1: 0.0\n",
            "Tag: IN --- F1: 0.9581913948059082\n",
            "Tag: NNS --- F1: 0.8824212551116943\n",
            "Tag: JJ --- F1: 0.7810029983520508\n",
            "Tag: VB --- F1: 0.9130306839942932\n",
            "Tag: UH --- F1: 0.0\n",
            "Tag: CD --- F1: 0.9161133766174316\n",
            "Tag: -LRB- --- F1: 0.0\n",
            "Tag: DT --- F1: 0.9704066514968872\n",
            "Tag: CC --- F1: 0.9747712016105652\n",
            "Tag: VBD --- F1: 0.8794181942939758\n",
            "Tag: MD --- F1: 0.9722835421562195\n",
            "Tag: RBS --- F1: 0.0\n",
            "Tag: PRP$ --- F1: 0.9763414263725281\n",
            "Tag: RBR --- F1: 0.0\n",
            "Tag: VBN --- F1: 0.7316558957099915\n"
          ]
        }
      ],
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- F1: {}'.format(index2tag[i], f1_list[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9wM9cNloyzc",
        "outputId": "c09a9762-321c-46d0-a06e-09c9773c0a9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: $ --- Val_F1: 0.9975429773330688\n",
            "Tag: LS --- Val_F1: 0.0\n",
            "Tag: RB --- Val_F1: 0.7200138568878174\n",
            "Tag: PDT --- Val_F1: 0.0\n",
            "Tag: WRB --- Val_F1: 0.0\n",
            "Tag: NNP --- Val_F1: 0.8180990815162659\n",
            "Tag: SYM --- Val_F1: 0.0\n",
            "Tag: NNPS --- Val_F1: 0.0\n",
            "Tag: POS --- Val_F1: 0.9911739826202393\n",
            "Tag: NN --- Val_F1: 0.8343343138694763\n",
            "Tag: WP$ --- Val_F1: 0.0\n",
            "Tag: VBG --- Val_F1: 0.6478449106216431\n",
            "Tag: JJS --- Val_F1: 0.0\n",
            "Tag: RP --- Val_F1: 0.3957819938659668\n",
            "Tag: -RRB- --- Val_F1: 0.0\n",
            "Tag: WDT --- Val_F1: 0.8210527300834656\n",
            "Tag: PRP --- Val_F1: 0.9669021964073181\n",
            "Tag: EX --- Val_F1: 0.045454543083906174\n",
            "Tag: JJR --- Val_F1: 0.02282695099711418\n",
            "Tag: # --- Val_F1: 0.0\n",
            "Tag: TO --- Val_F1: 1.0\n",
            "Tag: VBP --- Val_F1: 0.7656495571136475\n",
            "Tag: VBZ --- Val_F1: 0.9263761639595032\n",
            "Tag: WP --- Val_F1: 0.5130448937416077\n",
            "Tag: FW --- Val_F1: 0.0\n",
            "Tag: IN --- Val_F1: 0.9518236517906189\n",
            "Tag: NNS --- Val_F1: 0.8365123867988586\n",
            "Tag: JJ --- Val_F1: 0.7366695404052734\n",
            "Tag: VB --- Val_F1: 0.880270779132843\n",
            "Tag: UH --- Val_F1: 0.0\n",
            "Tag: CD --- Val_F1: 0.8985231518745422\n",
            "Tag: -LRB- --- Val_F1: 0.0\n",
            "Tag: DT --- Val_F1: 0.9715548157691956\n",
            "Tag: CC --- Val_F1: 0.9706600308418274\n",
            "Tag: VBD --- Val_F1: 0.8411031365394592\n",
            "Tag: MD --- Val_F1: 0.9467405676841736\n",
            "Tag: RBS --- Val_F1: 0.0\n",
            "Tag: PRP$ --- Val_F1: 0.9797472357749939\n",
            "Tag: RBR --- Val_F1: 0.0\n",
            "Tag: VBN --- Val_F1: 0.6802549958229065\n"
          ]
        }
      ],
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- Val_F1: {}'.format(index2tag[i], f1_val_list[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uv6mtuMo1Mh"
      },
      "source": [
        "#Infos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jot_8JtVo37_"
      },
      "source": [
        "#40 epochs, batch 128\n",
        "loss: 0.0370 - accuracy: 0.9899 - f1: 0.5241\n",
        "\n",
        "loss: 0.0373 - accuracy: 0.9896 - f1: 0.5214"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOQVTC5DSMvW"
      },
      "source": [
        "#Test Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "w9Y9N9RYoBNi"
      },
      "outputs": [],
      "source": [
        "test_sentences_X,  test_tags_y = [], []\n",
        "\n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        "\n",
        "for s in test_tags:\n",
        "    test_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "bFug9E5DoKWe"
      },
      "outputs": [],
      "source": [
        "y_val_pred = model.predict(test_sentences_X)\n",
        "\n",
        "y_pred = np.zeros((y_val_pred.shape[0], y_val_pred.shape[1]), dtype=int)\n",
        "for i in range(len(y_val_pred)):\n",
        "  for j in range(len(y_val_pred[i])):\n",
        "    y_pred[i][j] = np.argmax(y_val_pred[i][j])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lrb = [tag2index['-LRB-']]\n",
        "rrb = [tag2index['-RRB-']]\n",
        "canc = [tag2index['#']]\n",
        "dol = [tag2index['$']]\n",
        "\n",
        "punct_cat_classes = to_categorical([point, virg, weird_apex, single_apex, two_dots, pad, lrb, rrb, canc, dol], len(tag2index))\n",
        "\n",
        "cum_tags = np.zeros(len(tag2index))\n",
        "for i in punct_cat_classes:\n",
        "  cum_tags += i[0]\n",
        "where_tags = np.where(np.logical_not(cum_tags))\n",
        "no_punct_indexes = where_tags[0]"
      ],
      "metadata": {
        "id": "OpoOertF4Hzx"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Trdg6NLXSdGi"
      },
      "source": [
        "Using the f1_score from sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bsytk51_PLmR",
        "outputId": "2a4b72fc-a024-42b6-b5be-ac71f7da1a03"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.583316514506944"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "tags_flat = test_tags_y.flatten()\n",
        "pred_flat = y_pred.flatten()\n",
        "\n",
        "f1_model = f1_score(tags_flat, pred_flat, labels = no_punct_indexes, average='macro', zero_division=0)\n",
        "f1_model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "A1_biLSTM256_2DENSE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}