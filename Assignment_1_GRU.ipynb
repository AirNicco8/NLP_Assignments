{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1_GRU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AirNicco8/NLP_Assignments/blob/main/Assignment_1_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Due to**: 23/12/2021 (dd/mm/yyyy)\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Part-of Speech (POS) tagging as Sequence Labelling using Recurrent Neural Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4_wqPdlBcKS"
      },
      "source": [
        "# Intro\n",
        "\n",
        "In this assignment  we will ask you to perform POS tagging using neural architectures\n",
        "\n",
        "You are asked to follow these steps:\n",
        "*   Download the corpora and split it in training and test sets, structuring a dataframe.\n",
        "*   Embed the words using GloVe embeddings\n",
        "*   Create a baseline model, using a simple neural architecture\n",
        "*   Experiment doing small modifications to the baseline model, choose hyperparameters using the validation set\n",
        "*   Evaluate your two best model\n",
        "*   Analyze the errors of your model\n",
        "\n",
        "\n",
        "**Task**: given a corpus of documents, predict the POS tag for each word\n",
        "\n",
        "**Corpus**:\n",
        "Ignore the numeric value in the third column, use only the words/symbols and its label. \n",
        "The corpus is available at:\n",
        "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
        "\n",
        "**Splits**: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
        "\n",
        "\n",
        "**Features**: you MUST use GloVe embeddings as the only input features to the model.\n",
        "\n",
        "**Splitting**: you can decide to split documents into sentences or not, the choice is yours.\n",
        "\n",
        "**I/O structure**: The input data will have three dimensions: 1-documents/sentences, 2-token, 3-features; for the output there are 2 possibilities: if you use one-hot encoding it will be 1-documents/sentences, 2-token labels, 3-classes, if you use a single integer that indicates the number of the class it will be 1-documents/sentences, 2-token labels.\n",
        "\n",
        "**Baseline**: two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top; the choice of hyper-parameters is yours.\n",
        "\n",
        "**Architectures**: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and adding an additional dense layer; do not mix these variantions.\n",
        "\n",
        "\n",
        "**Training and Experiments**: all the experiments must involve only the training and validation sets.\n",
        "\n",
        "**Evaluation**: in the end, only the two best models of your choice (according to the validation set) must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech. DO NOT CONSIDER THE PUNCTUATION CLASSES.\n",
        "\n",
        "**Metrics**: the metric you must use to evaluate your final model is the F1-macro, WITHOUT considering punctuation/symbols classes; during the training process you can use accuracy because you can't use the F1 metric unless you use a single (gigantic) batch because there is no way to aggregate \"partial\" F1 scores computed on mini-batches.\n",
        "\n",
        "**Discussion and Error Analysis** : verify and discuss if the results on the test sets are coherent with those on the validation set; analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
        "\n",
        "**Report**: you are asked to deliver the code of your experiments and a small pdf report of about 2 pages; the pdf must begin with the names of the people of your team and a small abstract (4-5 lines) that sums up your findings.\n",
        "\n",
        "# Out Of Vocabulary (OOV) terms\n",
        "\n",
        "How to handle words that are not in GloVe vocabulary?\n",
        "You can handle them as you want (random embedding, placeholder, whatever!), but they must be STATIC embeddings (you cannot train them).\n",
        "\n",
        "But there is a very important caveat! As usual, the element of the test set must not influence the elements of the other splits!\n",
        "\n",
        "So, when you compute new embeddings for train+validation, you must forget about test documents.\n",
        "The motivation is to emulate a real-world scenario, where you select and train a model in the first stage, without knowing nothing about the testing environment.\n",
        "\n",
        "For implementation convenience, you CAN use a single vocabulary file/matrix/whatever. The principle of the previous point is that the embeddings inside that file/matrix must be generated independently for train and test splits.\n",
        "\n",
        "Basically in a real-world scenario, this is what would happen:\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Training of the model(s)\n",
        "5. Compute embeddings for terms OOV2 of the validation split \n",
        "6. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "7. Validation of the model(s)\n",
        "8. Compute embeddings for terms OOV3 of the test split \n",
        "9. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2+OOV3\n",
        "10. Testing of the final model\n",
        "\n",
        "In this case, where we already have all the documents, we can simplify the process a bit, but the procedure must remain rigorous.\n",
        "\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Compute embeddings for terms OOV2 of the validation split \n",
        "5. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "6. Compute embeddings for terms OOV3 of the test split \n",
        "7. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2\n",
        "8. Training of the model(s)\n",
        "9. Validation of the model(s)\n",
        "10. Testing of the final model\n",
        "\n",
        "Step 2 and step 6 must be completely independent of each other, for what concerns the method and the documents. But they can rely on the previous vocabulary (V1 for step 2 and V3 for step 6)\n",
        "THEREFORE if a word is present both in the training set and the test split and not in the starting vocabulary, its embedding is computed in step 2) and it is not considered OOV anymore in step 6).\n",
        "\n",
        "# Report\n",
        "The report must not be just a copy and paste of graphs and tables!\n",
        "\n",
        "The report must not be longer than 2 pages and must contain:\n",
        "* The names of the member of your team\n",
        "* A short abstract (4-5 lines) that sum ups everything\n",
        "* A general description of the task you have addressed and how you have addressed it\n",
        "* A short description of the models you have used\n",
        "* Some tables that sum up your findings in validation and test and a discussion of those results\n",
        "* The most relevant findings of your error analysis\n",
        "\n",
        "# Evaluation Criterion\n",
        "\n",
        "The goal of this assignment is not to prove you can find best model ever, but to face a common task, structure it correctly, and follow a correct and rigorous experimental procedure.\n",
        "In other words, we don't care if you final models are awful as long as you have followed the correct procedure and wrote a decent report.\n",
        "\n",
        "The score of the assignment will be computed roughly as follows\n",
        "* 1 point for the general setting of the problem\n",
        "* 1 point for the handling of OOV terms\n",
        "* 1 point for the models\n",
        "* 1 point for train-validation-test procedure\n",
        "* 2 point for the discussion of the results, error analysis, and report\n",
        "\n",
        "This distribution of scores is tentative and we may decide to alter it at any moment.\n",
        "We also reserve the right to assign a small bonus (0.5 points) to any assignment that is particularly worthy. Similarly, in case of grave errors, we may decide to assign an equivalent malus (-0.5 points).\n",
        "\n",
        "# Contacts\n",
        "\n",
        "In case of any doubt, question, issue, or help we highly recommend you to check the [course useful material](https://virtuale.unibo.it/pluginfile.php/1036039/mod_resource/content/2/NLP_Course_Useful_Material.pdf) for additional information, and to use the Virtuale forums to discuss with other students.\n",
        "\n",
        "You can always contact us at the following email addresses. To increase the probability of a prompt response, we reccomend you to write to both the teaching assistants.\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Andrea Galassi -> a.galassi@unibo.it\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it\n",
        "\n",
        "\n",
        "# FAQ\n",
        "* You can use a non-trainable Embedding layer to load the glove embeddings\n",
        "* You can use any library of your choice to implement the networks. Two options are tensorflow/keras or pythorch. Both these libraries have all the classes you need to implement these simple architectures and there are plenty of tutorials around, where you can learn how to use them."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Dataset"
      ],
      "metadata": {
        "id": "wW9PX3JdtDIE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emdubW66fnlD"
      },
      "source": [
        "import os, shutil  #  file management\n",
        "import sys \n",
        "import pandas as pd  #  dataframe management\n",
        "import numpy as np  #  data manipulation\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request  #  download files\n",
        "import zipfile  #  unzip files\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3Og8Jeg_LdG"
      },
      "source": [
        "from keras import metrics \n",
        "from tensorflow.python.keras.metrics import Metric\n",
        "from sklearn.metrics import f1_score\n",
        "import keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26J_gJt6yYle",
        "outputId": "0328ca04-1dde-4491-8723-049bba532944"
      },
      "source": [
        "!pip install -U keras"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3xn1MZPgA_v",
        "outputId": "e6a84d76-3b96-4716-d611-f36234536ff4"
      },
      "source": [
        "dataset_folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\")\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "    os.makedirs(dataset_folder)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
        "\n",
        "dataset_path = os.path.join(dataset_folder, \"treebank.zip\")\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    urllib.request.urlretrieve(url, dataset_path)\n",
        "    print(\"Successful download\")\n",
        "\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_folder)\n",
        "print(\"Successful extraction\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful download\n",
            "Successful extraction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre Processing\n",
        "- splitting"
      ],
      "metadata": {
        "id": "fLZ2apM5sVrQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKbUsrVHhGAG"
      },
      "source": [
        "dataset_name = \"dependency_treebank\"\n",
        "\n",
        "\n",
        "folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\", dataset_name)\n",
        "\n",
        "pre_train = []\n",
        "pre_valid = []\n",
        "pre_test = []\n",
        "i = 1\n",
        "\n",
        "file_list = sorted(os.listdir(folder))\n",
        "\n",
        "for filename in file_list:\n",
        "  file_path = os.path.join(folder, filename)\n",
        "  if os.path.isfile(file_path):\n",
        "    # open the file\n",
        "      text = []\n",
        "      with open(file_path, mode='r', encoding='utf-8') as text_file:\n",
        "        text = text_file.read()\n",
        "        if i <= 100:\n",
        "          pre_train.append(text)\n",
        "        elif i <= 150:\n",
        "          pre_valid.append(text)\n",
        "        else:\n",
        "          pre_test.append(text)  \n",
        "  i+=1\n",
        "\n",
        "tr = []\n",
        "val = []\n",
        "tes = []\n",
        "\n",
        "for paragraph in pre_train:\n",
        "   tr.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_valid:\n",
        "   val.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_test:\n",
        "   tes.append(paragraph.split('\\n'))\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUgbTXi-F7Sa"
      },
      "source": [
        "train = []\n",
        "valid = []\n",
        "test = []\n",
        "\n",
        "for i in tr:\n",
        "  for j in i:\n",
        "    train.append(j.split('\\t'))\n",
        "\n",
        "for i in val:\n",
        "  for j in i:\n",
        "    valid.append(j.split('\\t'))\n",
        "\n",
        "for i in tes:\n",
        "  for j in i:\n",
        "    test.append(j.split('\\t'))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmzVaRuQFp0M"
      },
      "source": [
        "train_sentences = []\n",
        "valid_sentences = []\n",
        "test_sentences = []\n",
        "train_tags = []\n",
        "valid_tags = []\n",
        "test_tags = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in train:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    train_sentences.append(s)\n",
        "    train_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in valid:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    valid_sentences.append(s)\n",
        "    valid_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in test:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    test_sentences.append(s)\n",
        "    test_tags.append(t)\n",
        "    s = []\n",
        "    t = []"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB65cUFLIeEr"
      },
      "source": [
        "flat_train = [item for sublist in train_sentences for item in sublist]\n",
        "flat_valid = [item for sublist in valid_sentences for item in sublist]\n",
        "flat_test = [item for sublist in test_sentences for item in sublist]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "- vocabularies\n",
        "- GloVe download"
      ],
      "metadata": {
        "id": "9jV2NMEFskis"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZO6ga1EGHdh"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "train_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "train_tokenizer.fit_on_texts(train_sentences)                    # fit tokeniser on data\n",
        "train_encoded = train_tokenizer.texts_to_sequences(train_sentences)\n",
        "\n",
        "valid_tokenizer = Tokenizer()       \n",
        "valid_tokenizer.fit_on_texts(valid_sentences)                  # instantiate tokeniser\n",
        "valid_encoded = valid_tokenizer.texts_to_sequences(valid_sentences)\n",
        "\n",
        "test_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "test_tokenizer.fit_on_texts(test_sentences)                    # fit tokeniser on data\n",
        "test_encoded = test_tokenizer.texts_to_sequences(test_sentences)  # use the tokeniser to encode input sequence"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsFbN0QOvaxl"
      },
      "source": [
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(train_tags)\n",
        "tags_encoded = tag_tokenizer.texts_to_sequences(train_tags)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMsG0Yw1l50L",
        "outputId": "60aee7bc-013e-4d5f-d93e-299ec0b40d03"
      },
      "source": [
        "voc = list(set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(valid_tokenizer.word_index.keys()) - set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(test_tokenizer.word_index.keys()) - set(voc))\n",
        "print(len(voc))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7404\n",
            "9901\n",
            "10947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS9RwnOtJuas"
      },
      "source": [
        "word_index = dict(zip(voc, range(2, len(voc)+3)))\n",
        "word_index['-PAD-'] = 0\n",
        "word_index['-OOV-'] = 1\n",
        "\n",
        "tags = set([item for sublist in train_tags for item in sublist])\n",
        "\n",
        "tag2index = {t: i + 1 for i, t in enumerate(tags)}\n",
        "tag2index['-PAD-'] = 0 "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoj4Zx51J2MU",
        "outputId": "a37a1479-a826-4301-c476-ef77ed95c670"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-13 14:54:33--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-12-13 14:54:34--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-12-13 14:54:34--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.21MB/s    in 2m 40s  \n",
            "\n",
            "2021-12-13 14:57:14 (5.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc-T6FcLKtS9",
        "outputId": "86e81e40-2a28-4d80-aea6-29ee48592fbd"
      },
      "source": [
        "path_to_glove_file = os.path.join(os.getcwd(), \"glove.6B.100d.txt\")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding\n",
        "- matrix creation\n",
        "- padding\n",
        "- one hot encoding of tags\n",
        "- punctuation identification"
      ],
      "metadata": {
        "id": "cLSzyxRdswoT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0mOStEPMeXt",
        "outputId": "3360d5ca-bf32-4b4c-9541-ac7a52c6c69d"
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        embedding_matrix += np.random.uniform(low=-0.05, high=0.05, size=embedding_dim)\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 10271 words (678 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwoJ4hbYXFPq"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpYrHs0RMtQJ"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8EZM31Js06-",
        "outputId": "54c61580-ecfd-470c-b281-ab539cb449b0"
      },
      "source": [
        "MAX_LENGTH = len(max(train_sentences, key=len))\n",
        "print(MAX_LENGTH)\n",
        "MAX_LENGTH2 = len(max(valid_sentences, key=len))\n",
        "print(MAX_LENGTH2)\n",
        "MAX_LENGTH3 = len(max(test_sentences, key=len))\n",
        "print(MAX_LENGTH3)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "249\n",
            "81\n",
            "58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSRa9SwTuNrX"
      },
      "source": [
        "train_sentences_X, valid_sentences_X, test_sentences_X, train_tags_y, valid_tags_y, test_tags_y = [], [], [], [], [], []\n",
        " \n",
        "for s in train_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        "\n",
        "for s in valid_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    valid_sentences_X.append(s_int)\n",
        "\n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        " \n",
        "for s in train_tags:\n",
        "    train_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "for s in valid_tags:\n",
        "    valid_tags_y.append([tag2index[t] for t in s])\n",
        " \n",
        "for s in test_tags:\n",
        "    test_tags_y.append([tag2index[t] for t in s])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf-XlSrStQx3"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_sentences_X = pad_sequences(valid_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_tags_y = pad_sequences(valid_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjZfgeu_zf0R"
      },
      "source": [
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztPl0A0xsA4s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d21f912-2ce5-4c07-eea0-8bbccc2d9918"
      },
      "source": [
        "point = [tag2index['.']]\n",
        "virg = [tag2index[',']]\n",
        "weird_apex = [tag2index['``']]\n",
        "single_apex = [tag2index[\"''\"]]\n",
        "two_dots = [tag2index[':']]\n",
        "pad = [tag2index['-PAD-']]\n",
        "\n",
        "punct_cat_classes = to_categorical([point, virg, weird_apex, single_apex, two_dots, pad], len(tag2index))\n",
        "punct_cat_classes.shape"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 1, 46)"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cum_tags = np.zeros(46)\n",
        "for i in punct_cat_classes:\n",
        "  print(i[0])\n",
        "  cum_tags += i[0]"
      ],
      "metadata": {
        "id": "RfwjtkoWeTzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "where_tags = np.where(np.logical_not(cum_tags))\n",
        "where_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3R9TJTIPN1B",
        "outputId": "a6cd0d84-3919-4d31-9728-18810b91a03d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 16, 17, 18,\n",
              "        19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37,\n",
              "        38, 39, 40, 42, 43, 44, 45]),)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_punct_indexes = where_tags[0]\n",
        "n_classes = len(no_punct_indexes)"
      ],
      "metadata": {
        "id": "ckabPVYKF8X5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgujFxeOzjq0",
        "outputId": "b9041597-31e6-4753-8b28-5b6b9a8200e0"
      },
      "source": [
        "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "cat_val_tags_y = to_categorical(valid_tags_y, len(tag2index))\n",
        "print(len(cat_train_tags_y), len(cat_val_tags_y))\n",
        "print(len(train_sentences_X), len(valid_sentences_X))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1963 1299\n",
            "1963 1299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mini Data Vizualization Inset"
      ],
      "metadata": {
        "id": "IHEHfOiTtd-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_tags = np.zeros(46)\n",
        "for i in cat_train_tags_y:\n",
        "  for t in i:\n",
        "    distribution_tags += t"
      ],
      "metadata": {
        "id": "pBsBceD6nhcS"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = sum(distribution_tags[1:])\n",
        "norm = [float(i)/s for i in distribution_tags[1:]]"
      ],
      "metadata": {
        "id": "fJsKW-69n83M"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k=1\n",
        "for i in norm:\n",
        "  print('Tag: {} --- distribution value: {}'.format(index2tag[k], i))\n",
        "  k += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8VLJ-kcoLcN",
        "outputId": "358fe471-a6d0-4fc8-a3d2-3133c68485a6"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: : --- distribution value: 0.006187177971112425\n",
            "Tag: CC --- distribution value: 0.024094095785116985\n",
            "Tag: TO --- distribution value: 0.021707914519807418\n",
            "Tag: WP --- distribution value: 0.0029774474195455695\n",
            "Tag: SYM --- distribution value: 2.1116648365571416e-05\n",
            "Tag: EX --- distribution value: 0.0010347157699129993\n",
            "Tag: LS --- distribution value: 0.00021116648365571416\n",
            "Tag: NNPS --- distribution value: 0.0020060815947292848\n",
            "Tag: -LRB- --- distribution value: 0.0010980657150097136\n",
            "Tag: JJ --- distribution value: 0.06318101190978968\n",
            "Tag: VBN --- distribution value: 0.02177126446490413\n",
            "Tag: VBP --- distribution value: 0.01535180336177042\n",
            "Tag: UH --- distribution value: 2.1116648365571416e-05\n",
            "Tag: VBZ --- distribution value: 0.023925162598192416\n",
            "Tag: `` --- distribution value: 0.00863670918151871\n",
            "Tag: JJR --- distribution value: 0.003315313793394712\n",
            "Tag: VB --- distribution value: 0.025234394796857844\n",
            "Tag: -RRB- --- distribution value: 0.001161415660106428\n",
            "Tag: RP --- distribution value: 0.0029563307711799984\n",
            "Tag: NNP --- distribution value: 0.10984880479770251\n",
            "Tag: PRP$ --- distribution value: 0.00863670918151871\n",
            "Tag: WDT --- distribution value: 0.004307796266576569\n",
            "Tag: . --- distribution value: 0.041367514148154406\n",
            "Tag: RB --- distribution value: 0.03146380606470141\n",
            "Tag: RBR --- distribution value: 0.0018160317594391419\n",
            "Tag: DT --- distribution value: 0.0860714587380691\n",
            "Tag: $ --- distribution value: 0.007221893741025424\n",
            "Tag: IN --- distribution value: 0.10456964270630965\n",
            "Tag: NNS --- distribution value: 0.06343441169017654\n",
            "Tag: NN --- distribution value: 0.13240138525213277\n",
            "Tag: FW --- distribution value: 4.223329673114283e-05\n",
            "Tag: JJS --- distribution value: 0.0019638482979981416\n",
            "Tag: '' --- distribution value: 0.008425542697862996\n",
            "Tag: # --- distribution value: 2.1116648365571416e-05\n",
            "Tag: RBS --- distribution value: 0.0004012163189458569\n",
            "Tag: VBD --- distribution value: 0.032688571669904555\n",
            "Tag: WP$ --- distribution value: 0.0001266998901934285\n",
            "Tag: PDT --- distribution value: 0.00019004983529014274\n",
            "Tag: WRB --- distribution value: 0.0019427316496325702\n",
            "Tag: MD --- distribution value: 0.008721175774980995\n",
            "Tag: , --- distribution value: 0.054290902947884113\n",
            "Tag: POS --- distribution value: 0.00851000929132528\n",
            "Tag: VBG --- distribution value: 0.01611200270293099\n",
            "Tag: CD --- distribution value: 0.03038685699805727\n",
            "Tag: PRP --- distribution value: 0.020145282540755132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt "
      ],
      "metadata": {
        "id": "zClpdjU5pZuI"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(np.arange(len(norm)),norm)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "_q5KWcNxoqz9",
        "outputId": "2fff921c-99bc-4f1f-d65a-47c9b60eeb59"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F1 Metric"
      ],
      "metadata": {
        "id": "90P5a9zauA9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric.\n",
        "\n",
        "    Only computes a batch-wise average of precision.\n",
        "\n",
        "    Computes the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric.\n",
        "\n",
        "    Only computes a batch-wise average of recall.\n",
        "\n",
        "    Computes the recall, a metric for multi-label classification of\n",
        "    how many relevant items are selected.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def f1_binary(y_true, y_pred):\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
        "\n",
        "def metric_wrapper(index):\n",
        "  def f1_for_class(y_true, y_pred):\n",
        "    #get only the desired class\n",
        "    true = y_true[:,:,index]\n",
        "    pred = y_pred[:,:,index]\n",
        "    #return dice per class\n",
        "    tmp = f1_binary(true,pred)\n",
        "    return tmp\n",
        "  f1_for_class.__name__ = 'f1_' + str(index)\n",
        "  return f1_for_class\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    result = 0.0\n",
        "    for class_id in no_punct_indexes:\n",
        "        y_true_single_class = y_true[:,:,class_id]\n",
        "        y_pred_single_class = y_pred[:,:,class_id]\n",
        "        f1_single = f1_binary(y_true_single_class, y_pred_single_class)\n",
        "        result += f1_single / float(n_classes)\n",
        "    return result"
      ],
      "metadata": {
        "id": "kL2FW8K7DSdy"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model compile and fit"
      ],
      "metadata": {
        "id": "ijdL-mXeuEhU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsVGByW_sdec",
        "outputId": "1019a1a3-799b-4f2d-8154-a0e910b90a55"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, GRU, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "gru = tf.keras.layers.GRU(64, return_sequences=True)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(embedding_layer)\n",
        "model.add(gru)\n",
        "model.add(TimeDistributed(Dense(len(tag2index), activation='softmax')))\n",
        "#model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=Adam(0.001),\n",
        "             metrics=['accuracy', f1, [metric_wrapper(i) for i in no_punct_indexes]])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 249, 100)          1094900   \n",
            "                                                                 \n",
            " gru_19 (GRU)                (None, 249, 64)           31872     \n",
            "                                                                 \n",
            " time_distributed_19 (TimeDi  (None, 249, 46)          2990      \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,129,762\n",
            "Trainable params: 34,862\n",
            "Non-trainable params: 1,094,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYKlazz3zr9P"
      },
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "history = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=40, validation_data=(valid_sentences_X, cat_val_tags_y), callbacks=[callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# History f1 for class"
      ],
      "metadata": {
        "id": "hxtPdjUiuKr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_val_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_val_list[i] = history.history['val_f1_{}'.format(i)][-1]\n",
        "f1_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_list[i] = history.history['f1_{}'.format(i)][-1]"
      ],
      "metadata": {
        "id": "4_KTbH5qiUnj"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index2tag = {value : key for (key, value) in tag2index.items()}\n",
        "index2tag"
      ],
      "metadata": {
        "id": "vGiXvzehlI_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- F1: {}'.format(index2tag[i], f1_list[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldEgVDA5mC2s",
        "outputId": "94e4dd36-97ba-4f47-e288-6973cc3b3c88"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: -PAD- --- F1: 0.9998751878738403\n",
            "Tag: CC --- F1: 0.7655251622200012\n",
            "Tag: TO --- F1: 0.9980541467666626\n",
            "Tag: WP --- F1: 0.0\n",
            "Tag: SYM --- F1: 0.0\n",
            "Tag: EX --- F1: 0.0\n",
            "Tag: LS --- F1: 0.0\n",
            "Tag: NNPS --- F1: 0.0\n",
            "Tag: -LRB- --- F1: 0.0\n",
            "Tag: JJ --- F1: 0.10476727783679962\n",
            "Tag: VBN --- F1: 0.0\n",
            "Tag: VBP --- F1: 0.0\n",
            "Tag: UH --- F1: 0.0\n",
            "Tag: VBZ --- F1: 0.4033684730529785\n",
            "Tag: JJR --- F1: 0.0\n",
            "Tag: VB --- F1: 0.28399667143821716\n",
            "Tag: -RRB- --- F1: 0.0\n",
            "Tag: RP --- F1: 0.0\n",
            "Tag: NNP --- F1: 0.6397297382354736\n",
            "Tag: PRP$ --- F1: 0.0\n",
            "Tag: WDT --- F1: 0.0\n",
            "Tag: RB --- F1: 0.0\n",
            "Tag: RBR --- F1: 0.0\n",
            "Tag: DT --- F1: 0.9329425692558289\n",
            "Tag: $ --- F1: 0.012323757633566856\n",
            "Tag: IN --- F1: 0.8643227815628052\n",
            "Tag: NNS --- F1: 0.4101976752281189\n",
            "Tag: NN --- F1: 0.5766741633415222\n",
            "Tag: FW --- F1: 0.0\n",
            "Tag: JJS --- F1: 0.0\n",
            "Tag: # --- F1: 0.0\n",
            "Tag: RBS --- F1: 0.0\n",
            "Tag: VBD --- F1: 0.49564415216445923\n",
            "Tag: WP$ --- F1: 0.0\n",
            "Tag: PDT --- F1: 0.0\n",
            "Tag: WRB --- F1: 0.0\n",
            "Tag: MD --- F1: 0.0\n",
            "Tag: POS --- F1: 0.0\n",
            "Tag: VBG --- F1: 0.0\n",
            "Tag: CD --- F1: 0.6715078949928284\n",
            "Tag: PRP --- F1: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- Val_F1: {}'.format(index2tag[i], f1_val_list[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9V6FASVZkmKo",
        "outputId": "514d299f-dcd0-498f-f73b-d5e56100ffa3"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: -PAD- --- F1: 0.9998584389686584\n",
            "Tag: CC --- F1: 0.7346917986869812\n",
            "Tag: TO --- F1: 1.0\n",
            "Tag: WP --- F1: 0.0\n",
            "Tag: SYM --- F1: 0.0\n",
            "Tag: EX --- F1: 0.0\n",
            "Tag: LS --- F1: 0.0\n",
            "Tag: NNPS --- F1: 0.0\n",
            "Tag: -LRB- --- F1: 0.0\n",
            "Tag: JJ --- F1: 0.05839594453573227\n",
            "Tag: VBN --- F1: 0.0\n",
            "Tag: VBP --- F1: 0.0\n",
            "Tag: UH --- F1: 0.0\n",
            "Tag: VBZ --- F1: 0.4804961085319519\n",
            "Tag: JJR --- F1: 0.0\n",
            "Tag: VB --- F1: 0.23628777265548706\n",
            "Tag: -RRB- --- F1: 0.0\n",
            "Tag: RP --- F1: 0.0\n",
            "Tag: NNP --- F1: 0.5538280010223389\n",
            "Tag: PRP$ --- F1: 0.0\n",
            "Tag: WDT --- F1: 0.0\n",
            "Tag: RB --- F1: 0.0\n",
            "Tag: RBR --- F1: 0.0\n",
            "Tag: DT --- F1: 0.9338130354881287\n",
            "Tag: $ --- F1: 0.0\n",
            "Tag: IN --- F1: 0.8572620749473572\n",
            "Tag: NNS --- F1: 0.396915078163147\n",
            "Tag: NN --- F1: 0.589436948299408\n",
            "Tag: FW --- F1: 0.0\n",
            "Tag: JJS --- F1: 0.0\n",
            "Tag: # --- F1: 0.0\n",
            "Tag: RBS --- F1: 0.0\n",
            "Tag: VBD --- F1: 0.48273754119873047\n",
            "Tag: WP$ --- F1: 0.0\n",
            "Tag: PDT --- F1: 0.0\n",
            "Tag: WRB --- F1: 0.0\n",
            "Tag: MD --- F1: 0.0\n",
            "Tag: POS --- F1: 0.0\n",
            "Tag: VBG --- F1: 0.0\n",
            "Tag: CD --- F1: 0.663867175579071\n",
            "Tag: PRP --- F1: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Infos"
      ],
      "metadata": {
        "id": "5NOy6OW7uRTa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SquZv3o-nDv"
      },
      "source": [
        "40 epochs, batch 128\n",
        "- loss: 0.0893/0.0929 - accuracy: 0.9754 - f1: 0.2916\n",
        "- val_loss: 0.0990/0.1006 - val_accuracy: 0.9721 - val_f1: 0.2836\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
        "\n",
        "List of tags used in this dataset"
      ],
      "metadata": {
        "id": "D7oqH_ZvmxTZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndqb8pzkgRAi"
      },
      "source": [
        "You don't have to compute the whole set of new Glove Embeddings, you must load the pre-trained ones.\n",
        "\n",
        "For the baseline, it must have only two trainable layers: the BiLSTM and the Dense/FC one. The Dense layer is the \"classification head\" with softmax activation. You must not add an additional dense layer on top of the baseline. You can use the embedding layer before the BiLSTM, but it must be not trainable.\n",
        "\n",
        "For the application of the Dense Layer, it is recommended to use a Time-Distributed Dense. In any case, doing otherwise is NOT considered an error.\n",
        "\n",
        "There is a typo regarding the vocabularies for OOV: V4=V1+OOV1+OOV2+OOV3\n",
        "\n",
        "Since in this specific case we already know the test set, it is possible to use an Embeddings Matrix with all the words from each split, and without any word that is not present in the documents. It's important to generate the OOVs embeddings separately, but it's not a problem to put them in the same matrix: the test set OOVs will never be used during training and so they won't affect the rest of the network in any way.\n",
        "\n",
        "Evaluation: for the early stopping you need to use accuracy since it's not possible to distribute and aggregate F1 across batches, but for any other evaluation, including choosing the best model on validation set, you must use F1.\n",
        "\n",
        "Punctuation: you must keep the punctuation in the documents, since it may be helpful for the model, you simply must ignore it when you perform the evaluation of the model, not considering the punctuation classes among the ones you use to compute F1 macro score.\n"
      ]
    }
  ]
}