{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_biLSTM256_LSTM256+Relu.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Due to**: 23/12/2021 (dd/mm/yyyy)\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Part-of Speech (POS) tagging as Sequence Labelling using Recurrent Neural Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4_wqPdlBcKS"
      },
      "source": [
        "# Intro\n",
        "\n",
        "In this assignment  we will ask you to perform POS tagging using neural architectures\n",
        "\n",
        "You are asked to follow these steps:\n",
        "*   Download the corpora and split it in training and test sets, structuring a dataframe.\n",
        "*   Embed the words using GloVe embeddings\n",
        "*   Create a baseline model, using a simple neural architecture\n",
        "*   Experiment doing small modifications to the baseline model, choose hyperparameters using the validation set\n",
        "*   Evaluate your two best model\n",
        "*   Analyze the errors of your model\n",
        "\n",
        "\n",
        "**Task**: given a corpus of documents, predict the POS tag for each word\n",
        "\n",
        "**Corpus**:\n",
        "Ignore the numeric value in the third column, use only the words/symbols and its label. \n",
        "The corpus is available at:\n",
        "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
        "\n",
        "**Splits**: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
        "\n",
        "\n",
        "**Features**: you MUST use GloVe embeddings as the only input features to the model.\n",
        "\n",
        "**Splitting**: you can decide to split documents into sentences or not, the choice is yours.\n",
        "\n",
        "**I/O structure**: The input data will have three dimensions: 1-documents/sentences, 2-token, 3-features; for the output there are 2 possibilities: if you use one-hot encoding it will be 1-documents/sentences, 2-token labels, 3-classes, if you use a single integer that indicates the number of the class it will be 1-documents/sentences, 2-token labels.\n",
        "\n",
        "**Baseline**: two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top; the choice of hyper-parameters is yours.\n",
        "\n",
        "**Architectures**: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and adding an additional dense layer; do not mix these variantions.\n",
        "\n",
        "\n",
        "**Training and Experiments**: all the experiments must involve only the training and validation sets.\n",
        "\n",
        "**Evaluation**: in the end, only the two best models of your choice (according to the validation set) must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech. DO NOT CONSIDER THE PUNCTUATION CLASSES.\n",
        "\n",
        "**Metrics**: the metric you must use to evaluate your final model is the F1-macro, WITHOUT considering punctuation/symbols classes; during the training process you can use accuracy because you can't use the F1 metric unless you use a single (gigantic) batch because there is no way to aggregate \"partial\" F1 scores computed on mini-batches.\n",
        "\n",
        "**Discussion and Error Analysis** : verify and discuss if the results on the test sets are coherent with those on the validation set; analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
        "\n",
        "**Report**: you are asked to deliver the code of your experiments and a small pdf report of about 2 pages; the pdf must begin with the names of the people of your team and a small abstract (4-5 lines) that sums up your findings.\n",
        "\n",
        "# Out Of Vocabulary (OOV) terms\n",
        "\n",
        "How to handle words that are not in GloVe vocabulary?\n",
        "You can handle them as you want (random embedding, placeholder, whatever!), but they must be STATIC embeddings (you cannot train them).\n",
        "\n",
        "But there is a very important caveat! As usual, the element of the test set must not influence the elements of the other splits!\n",
        "\n",
        "So, when you compute new embeddings for train+validation, you must forget about test documents.\n",
        "The motivation is to emulate a real-world scenario, where you select and train a model in the first stage, without knowing nothing about the testing environment.\n",
        "\n",
        "For implementation convenience, you CAN use a single vocabulary file/matrix/whatever. The principle of the previous point is that the embeddings inside that file/matrix must be generated independently for train and test splits.\n",
        "\n",
        "Basically in a real-world scenario, this is what would happen:\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Training of the model(s)\n",
        "5. Compute embeddings for terms OOV2 of the validation split \n",
        "6. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "7. Validation of the model(s)\n",
        "8. Compute embeddings for terms OOV3 of the test split \n",
        "9. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2+OOV3\n",
        "10. Testing of the final model\n",
        "\n",
        "In this case, where we already have all the documents, we can simplify the process a bit, but the procedure must remain rigorous.\n",
        "\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Compute embeddings for terms OOV2 of the validation split \n",
        "5. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "6. Compute embeddings for terms OOV3 of the test split \n",
        "7. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2\n",
        "8. Training of the model(s)\n",
        "9. Validation of the model(s)\n",
        "10. Testing of the final model\n",
        "\n",
        "Step 2 and step 6 must be completely independent of each other, for what concerns the method and the documents. But they can rely on the previous vocabulary (V1 for step 2 and V3 for step 6)\n",
        "THEREFORE if a word is present both in the training set and the test split and not in the starting vocabulary, its embedding is computed in step 2) and it is not considered OOV anymore in step 6).\n",
        "\n",
        "# Report\n",
        "The report must not be just a copy and paste of graphs and tables!\n",
        "\n",
        "The report must not be longer than 2 pages and must contain:\n",
        "* The names of the member of your team\n",
        "* A short abstract (4-5 lines) that sum ups everything\n",
        "* A general description of the task you have addressed and how you have addressed it\n",
        "* A short description of the models you have used\n",
        "* Some tables that sum up your findings in validation and test and a discussion of those results\n",
        "* The most relevant findings of your error analysis\n",
        "\n",
        "# Evaluation Criterion\n",
        "\n",
        "The goal of this assignment is not to prove you can find best model ever, but to face a common task, structure it correctly, and follow a correct and rigorous experimental procedure.\n",
        "In other words, we don't care if you final models are awful as long as you have followed the correct procedure and wrote a decent report.\n",
        "\n",
        "The score of the assignment will be computed roughly as follows\n",
        "* 1 point for the general setting of the problem\n",
        "* 1 point for the handling of OOV terms\n",
        "* 1 point for the models\n",
        "* 1 point for train-validation-test procedure\n",
        "* 2 point for the discussion of the results, error analysis, and report\n",
        "\n",
        "This distribution of scores is tentative and we may decide to alter it at any moment.\n",
        "We also reserve the right to assign a small bonus (0.5 points) to any assignment that is particularly worthy. Similarly, in case of grave errors, we may decide to assign an equivalent malus (-0.5 points).\n",
        "\n",
        "# Contacts\n",
        "\n",
        "In case of any doubt, question, issue, or help we highly recommend you to check the [course useful material](https://virtuale.unibo.it/pluginfile.php/1036039/mod_resource/content/2/NLP_Course_Useful_Material.pdf) for additional information, and to use the Virtuale forums to discuss with other students.\n",
        "\n",
        "You can always contact us at the following email addresses. To increase the probability of a prompt response, we reccomend you to write to both the teaching assistants.\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Andrea Galassi -> a.galassi@unibo.it\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it\n",
        "\n",
        "\n",
        "# FAQ\n",
        "* You can use a non-trainable Embedding layer to load the glove embeddings\n",
        "* You can use any library of your choice to implement the networks. Two options are tensorflow/keras or pythorch. Both these libraries have all the classes you need to implement these simple architectures and there are plenty of tutorials around, where you can learn how to use them."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports & Dataset"
      ],
      "metadata": {
        "id": "ZyyNdk2kx74A"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emdubW66fnlD"
      },
      "source": [
        "import os, shutil  #  file management\n",
        "import sys \n",
        "import pandas as pd  #  dataframe management\n",
        "import numpy as np  #  data manipulation\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request  #  download files\n",
        "import zipfile  #  unzip files\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B-w2pqyfpF1"
      },
      "source": [
        "from keras import metrics \n",
        "from tensorflow.python.keras.metrics import Metric\n",
        "from sklearn.metrics import f1_score\n",
        "import keras.backend as K"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS4AbOOdymmK",
        "outputId": "18c1cd70-f372-49e5-e852-0a264087d79e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3xn1MZPgA_v",
        "outputId": "877011dc-0439-47f7-fec5-18c14f2f4d57"
      },
      "source": [
        "dataset_folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\")\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "    os.makedirs(dataset_folder)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
        "\n",
        "dataset_path = os.path.join(dataset_folder, \"treebank.zip\")\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    urllib.request.urlretrieve(url, dataset_path)\n",
        "    print(\"Successful download\")\n",
        "\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_folder)\n",
        "print(\"Successful extraction\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful download\n",
            "Successful extraction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre Processing\n",
        "- splitting"
      ],
      "metadata": {
        "id": "Kef3IUYVyDTf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKbUsrVHhGAG"
      },
      "source": [
        "dataset_name = \"dependency_treebank\"\n",
        "\n",
        "\n",
        "folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\", dataset_name)\n",
        "\n",
        "pre_train = []\n",
        "pre_valid = []\n",
        "pre_test = []\n",
        "i = 1\n",
        "\n",
        "file_list = sorted(os.listdir(folder))\n",
        "\n",
        "for filename in file_list:\n",
        "  file_path = os.path.join(folder, filename)\n",
        "  if os.path.isfile(file_path):\n",
        "    # open the file\n",
        "      text = []\n",
        "      with open(file_path, mode='r', encoding='utf-8') as text_file:\n",
        "        text = text_file.read()\n",
        "        if i <= 100:\n",
        "          pre_train.append(text)\n",
        "        elif i <= 150:\n",
        "          pre_valid.append(text)\n",
        "        else:\n",
        "          pre_test.append(text)  \n",
        "  i+=1\n",
        "\n",
        "tr = []\n",
        "val = []\n",
        "tes = []\n",
        "\n",
        "for paragraph in pre_train:\n",
        "   tr.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_valid:\n",
        "   val.append(paragraph.split('\\n'))\n",
        "for paragraph in pre_test:\n",
        "   tes.append(paragraph.split('\\n'))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUgbTXi-F7Sa"
      },
      "source": [
        "train = []\n",
        "valid = []\n",
        "test = []\n",
        "\n",
        "for i in tr:\n",
        "  for j in i:\n",
        "    train.append(j.split('\\t'))\n",
        "\n",
        "for i in val:\n",
        "  for j in i:\n",
        "    valid.append(j.split('\\t'))\n",
        "\n",
        "for i in tes:\n",
        "  for j in i:\n",
        "    test.append(j.split('\\t'))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmzVaRuQFp0M"
      },
      "source": [
        "train_sentences = []\n",
        "valid_sentences = []\n",
        "test_sentences = []\n",
        "train_tags = []\n",
        "valid_tags = []\n",
        "test_tags = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in train:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    train_sentences.append(s)\n",
        "    train_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in valid:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    valid_sentences.append(s)\n",
        "    valid_tags.append(t)\n",
        "    s = []\n",
        "    t = []\n",
        "\n",
        "s = []\n",
        "t = []\n",
        "for i in test:\n",
        "  if i[0] != '':\n",
        "    s.append(i[0])\n",
        "    t.append(i[1])\n",
        "  else:\n",
        "    test_sentences.append(s)\n",
        "    test_tags.append(t)\n",
        "    s = []\n",
        "    t = []"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB65cUFLIeEr"
      },
      "source": [
        "flat_train = [item for sublist in train_sentences for item in sublist]\n",
        "flat_valid = [item for sublist in valid_sentences for item in sublist]\n",
        "flat_test = [item for sublist in test_sentences for item in sublist]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "- vocabularies\n",
        "- GloVe download"
      ],
      "metadata": {
        "id": "Lyy9cTRUyHU7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZO6ga1EGHdh"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "train_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "train_tokenizer.fit_on_texts(train_sentences)                    # fit tokeniser on data\n",
        "train_encoded = train_tokenizer.texts_to_sequences(train_sentences)\n",
        "\n",
        "valid_tokenizer = Tokenizer()       \n",
        "valid_tokenizer.fit_on_texts(valid_sentences)                  # instantiate tokeniser\n",
        "valid_encoded = valid_tokenizer.texts_to_sequences(valid_sentences)\n",
        "\n",
        "test_tokenizer = Tokenizer()                     # instantiate tokeniser\n",
        "test_tokenizer.fit_on_texts(test_sentences)                    # fit tokeniser on data\n",
        "test_encoded = test_tokenizer.texts_to_sequences(test_sentences)  # use the tokeniser to encode input sequence"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(train_tags)\n",
        "tags_encoded = tag_tokenizer.texts_to_sequences(train_tags)"
      ],
      "metadata": {
        "id": "oacPqxM4yQeE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMsG0Yw1l50L",
        "outputId": "1fedb49e-5208-42ce-b6e3-4c3e18ff3675"
      },
      "source": [
        "voc = list(set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(valid_tokenizer.word_index.keys()) - set(train_tokenizer.word_index.keys()))\n",
        "print(len(voc))\n",
        "voc += list(set(test_tokenizer.word_index.keys()) - set(voc))\n",
        "print(len(voc))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7404\n",
            "9901\n",
            "10947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS9RwnOtJuas"
      },
      "source": [
        "word_index = dict(zip(voc, range(2, len(voc)+3)))\n",
        "word_index['-PAD-'] = 0\n",
        "word_index['-OOV-'] = 1\n",
        "\n",
        "tags = set([item for sublist in train_tags for item in sublist])\n",
        "\n",
        "tag2index = {t: i + 1 for i, t in enumerate(tags)}\n",
        "tag2index['-PAD-'] = 0 "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoj4Zx51J2MU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad84575f-41c9-45be-a5af-ecebaa8cfbf1"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-15 09:01:57--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-12-15 09:01:57--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-12-15 09:01:57--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.82MB/s    in 2m 43s  \n",
            "\n",
            "2021-12-15 09:04:41 (5.04 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc-T6FcLKtS9",
        "outputId": "b2d9ca0e-ff50-4f3a-8cfe-b18bded44141"
      },
      "source": [
        "path_to_glove_file = os.path.join(os.getcwd(), \"glove.6B.100d.txt\")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding\n",
        "- matrix creation\n",
        "- padding\n",
        "- one hot encoding of tags\n",
        "- punctuation identification"
      ],
      "metadata": {
        "id": "AsiM2CUZyZfr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0mOStEPMeXt",
        "outputId": "72b4639e-4fe1-4a86-9bbb-359ba3717a83"
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        embedding_matrix += np.random.uniform(low=-0.05, high=0.05, size=embedding_dim)\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 10271 words (678 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpYrHs0RMtQJ"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8EZM31Js06-",
        "outputId": "e0cdf1be-98ad-4582-ec40-1ba33bf2f239"
      },
      "source": [
        "MAX_LENGTH = len(max(train_sentences, key=len))\n",
        "print(MAX_LENGTH)\n",
        "MAX_LENGTH2 = len(max(valid_sentences, key=len))\n",
        "print(MAX_LENGTH2)\n",
        "MAX_LENGTH3 = len(max(test_sentences, key=len))\n",
        "print(MAX_LENGTH3)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "249\n",
            "81\n",
            "58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSRa9SwTuNrX"
      },
      "source": [
        "train_sentences_X, valid_sentences_X, test_sentences_X, train_tags_y, valid_tags_y, test_tags_y = [], [], [], [], [], []\n",
        " \n",
        "for s in train_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        "\n",
        "for s in valid_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    valid_sentences_X.append(s_int)\n",
        "\n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word_index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word_index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        " \n",
        "for s in train_tags:\n",
        "    train_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "for s in valid_tags:\n",
        "    valid_tags_y.append([tag2index[t] for t in s])\n",
        " \n",
        "for s in test_tags:\n",
        "    test_tags_y.append([tag2index[t] for t in s])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf-XlSrStQx3"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_sentences_X = pad_sequences(valid_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "valid_tags_y = pad_sequences(valid_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)"
      ],
      "metadata": {
        "id": "BZ67yadRywqk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "point = [tag2index['.']]\n",
        "virg = [tag2index[',']]\n",
        "weird_apex = [tag2index['``']]\n",
        "single_apex = [tag2index[\"''\"]]\n",
        "two_dots = [tag2index[':']]\n",
        "pad = [tag2index['-PAD-']]\n",
        "\n",
        "punct_cat_classes = to_categorical([point, virg, weird_apex, single_apex, two_dots, pad], len(tag2index))\n",
        "punct_cat_classes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXU_7O8ZyyHk",
        "outputId": "6ac55031-996b-4f9b-ef94-e79899c9baa8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 1, 46)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cum_tags = np.zeros(46)\n",
        "for i in punct_cat_classes:\n",
        "  print(i[0])\n",
        "  cum_tags += i[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjDGzbi_y0HR",
        "outputId": "dcf65343-7da6-4270-80b4-5f0663b7dba3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "where_tags = np.where(np.logical_not(cum_tags))\n",
        "where_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KK8ZmhZy1rN",
        "outputId": "8d79252d-4235-4232-9c44-e8f9e9d428ab"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 1,  3,  4,  5,  6,  7,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20,\n",
              "        21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 36, 37, 38, 39,\n",
              "        40, 41, 42, 43, 44, 45]),)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_punct_indexes = where_tags[0]\n",
        "n_classes = len(no_punct_indexes)"
      ],
      "metadata": {
        "id": "AgcCedVXy27C"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "cat_val_tags_y = to_categorical(valid_tags_y, len(tag2index))\n",
        "print(len(cat_train_tags_y), len(cat_val_tags_y))\n",
        "print(len(train_sentences_X), len(valid_sentences_X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOHNSPtNy4_O",
        "outputId": "a3ef1805-9f4c-4b7a-c853-ad3e098f87f8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1963 1299\n",
            "1963 1299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mini Data Vizualization Inset"
      ],
      "metadata": {
        "id": "jNvdAUJ5y6L5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_tags = np.zeros(46)\n",
        "for i in cat_train_tags_y:\n",
        "  for t in i:\n",
        "    distribution_tags += t"
      ],
      "metadata": {
        "id": "4AfwZ6sty_en"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = sum(distribution_tags[1:])\n",
        "norm = [float(i)/s for i in distribution_tags[1:]]"
      ],
      "metadata": {
        "id": "D8i92wNZzBzb"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt "
      ],
      "metadata": {
        "id": "447DKuJQzE_D"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(np.arange(len(norm)),norm)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "PJjB1H0vzGAF",
        "outputId": "edeca7c6-f694-4967-e89d-50e6e7f37745"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#F1 Metric"
      ],
      "metadata": {
        "id": "1r2SQVe1zHWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric.\n",
        "\n",
        "    Only computes a batch-wise average of precision.\n",
        "\n",
        "    Computes the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric.\n",
        "\n",
        "    Only computes a batch-wise average of recall.\n",
        "\n",
        "    Computes the recall, a metric for multi-label classification of\n",
        "    how many relevant items are selected.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def f1_binary(y_true, y_pred):\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
        "\n",
        "def metric_wrapper(index):\n",
        "  def f1_for_class(y_true, y_pred):\n",
        "    #get only the desired class\n",
        "    true = y_true[:,:,index]\n",
        "    pred = y_pred[:,:,index]\n",
        "    #return dice per class\n",
        "    tmp = f1_binary(true,pred)\n",
        "    return tmp\n",
        "  f1_for_class.__name__ = 'f1_' + str(index)\n",
        "  return f1_for_class\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    result = 0.0\n",
        "    for class_id in no_punct_indexes:\n",
        "        y_true_single_class = y_true[:,:,class_id]\n",
        "        y_pred_single_class = y_pred[:,:,class_id]\n",
        "        f1_single = f1_binary(y_true_single_class, y_pred_single_class)\n",
        "        result += f1_single / float(n_classes)\n",
        "    return result"
      ],
      "metadata": {
        "id": "kL2FW8K7DSdy"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model compile and fit"
      ],
      "metadata": {
        "id": "08uv0pTyzNK7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsVGByW_sdec",
        "outputId": "3d82e386-925c-42c5-80fe-a008969da89b"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(embedding_layer)\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(Dense(len(tag2index)))\n",
        "model.add(Activation('relu'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=Adam(0.001),\n",
        "              metrics=['accuracy', f1, [metric_wrapper(i) for i in no_punct_indexes]])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 249, 100)          1094900   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 249, 512)         731136    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 249, 256)          787456    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 249, 46)           11822     \n",
            "                                                                 \n",
            " activation (Activation)     (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,625,314\n",
            "Trainable params: 1,530,414\n",
            "Non-trainable params: 1,094,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "history = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=40, validation_data=(valid_sentences_X, cat_val_tags_y), callbacks=[callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1YfuDOVzS_R",
        "outputId": "f41ee7d4-3f7a-4fe7-dfbb-7895d46eb28d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "16/16 [==============================] - 147s 9s/step - loss: 0.5538 - accuracy: 0.9094 - f1: 0.0306 - f1_1: 0.2038 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.1237 - f1_6: 0.0000e+00 - f1_7: 0.0634 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0590 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0351 - f1_26: 0.0530 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.1243 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.2375 - f1_34: 0.0641 - f1_36: 0.1665 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0494 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0451 - f1_45: 0.0000e+00 - val_loss: 0.5490 - val_accuracy: 0.9117 - val_f1: 0.0339 - val_f1_1: 0.1730 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.1173 - val_f1_6: 0.1060 - val_f1_7: 0.0581 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0793 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0333 - val_f1_26: 0.0650 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.1290 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2537 - val_f1_34: 0.0631 - val_f1_36: 0.1699 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0571 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0515 - val_f1_45: 0.0000e+00\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - 147s 9s/step - loss: 0.5517 - accuracy: 0.9158 - f1: 0.0342 - f1_1: 0.2055 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.1242 - f1_6: 0.1341 - f1_7: 0.0648 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0614 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0351 - f1_26: 0.0533 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.1215 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.2393 - f1_34: 0.0643 - f1_36: 0.1673 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0534 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0450 - f1_45: 0.0000e+00 - val_loss: 0.5463 - val_accuracy: 0.9199 - val_f1: 0.0344 - val_f1_1: 0.1731 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.1172 - val_f1_6: 0.1285 - val_f1_7: 0.0594 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0792 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0337 - val_f1_26: 0.0657 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.1258 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2497 - val_f1_34: 0.0627 - val_f1_36: 0.1685 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0596 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0518 - val_f1_45: 0.0000e+00\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - 145s 9s/step - loss: 0.5493 - accuracy: 0.9179 - f1: 0.0321 - f1_1: 0.2053 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.1248 - f1_6: 0.0359 - f1_7: 0.0664 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0606 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0366 - f1_26: 0.0552 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.1235 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.2404 - f1_34: 0.0656 - f1_36: 0.1686 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0568 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0457 - f1_45: 0.0000e+00 - val_loss: 0.5438 - val_accuracy: 0.9202 - val_f1: 0.0325 - val_f1_1: 0.1732 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.1172 - val_f1_6: 0.0351 - val_f1_7: 0.0605 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0805 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0342 - val_f1_26: 0.0661 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.1283 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2517 - val_f1_34: 0.0630 - val_f1_36: 0.1691 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0711 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0518 - val_f1_45: 0.0000e+00\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - 146s 9s/step - loss: 0.5468 - accuracy: 0.9187 - f1: 0.0340 - f1_1: 0.2041 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.1238 - f1_6: 0.0880 - f1_7: 0.0669 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0609 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0387 - f1_26: 0.0547 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.1255 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.2413 - f1_34: 0.0648 - f1_36: 0.1687 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.0768 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0455 - f1_45: 0.0000e+00 - val_loss: 0.5415 - val_accuracy: 0.9206 - val_f1: 0.0339 - val_f1_1: 0.1732 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.1172 - val_f1_6: 0.0598 - val_f1_7: 0.0607 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0805 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0387 - val_f1_26: 0.0664 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.1292 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2563 - val_f1_34: 0.0635 - val_f1_36: 0.1706 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.0898 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0519 - val_f1_45: 0.0000e+00\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - 150s 9s/step - loss: 0.5455 - accuracy: 0.9194 - f1: 0.0349 - f1_1: 0.2037 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.1244 - f1_6: 0.0733 - f1_7: 0.0669 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0608 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0125 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0482 - f1_26: 0.0554 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.1237 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.2406 - f1_34: 0.0640 - f1_36: 0.1686 - f1_37: 0.0044 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.1025 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0468 - f1_45: 0.0000e+00 - val_loss: 0.5403 - val_accuracy: 0.9206 - val_f1: 0.0361 - val_f1_1: 0.1731 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.1174 - val_f1_6: 0.0477 - val_f1_7: 0.0617 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0800 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0455 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0461 - val_f1_26: 0.0671 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.1291 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2575 - val_f1_34: 0.0637 - val_f1_36: 0.1741 - val_f1_37: 0.0153 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.1137 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0522 - val_f1_45: 0.0000e+00\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - 149s 9s/step - loss: 0.5421 - accuracy: 0.9198 - f1: 0.0381 - f1_1: 0.2053 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.1234 - f1_6: 0.1852 - f1_7: 0.0674 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0617 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0529 - f1_26: 0.0576 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.1246 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.2421 - f1_34: 0.0640 - f1_36: 0.1707 - f1_37: 0.0142 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.1081 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0455 - f1_45: 0.0000e+00 - val_loss: 0.5369 - val_accuracy: 0.9213 - val_f1: 0.0380 - val_f1_1: 0.1731 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.1173 - val_f1_6: 0.0564 - val_f1_7: 0.0619 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0800 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0487 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0548 - val_f1_26: 0.0742 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.1291 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2575 - val_f1_34: 0.0638 - val_f1_36: 0.1738 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.1757 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0526 - val_f1_45: 0.0000e+00\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - 151s 10s/step - loss: 0.5388 - accuracy: 0.9204 - f1: 0.0395 - f1_1: 0.2056 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.1240 - f1_6: 0.1904 - f1_7: 0.0678 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0616 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0627 - f1_26: 0.0727 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.1247 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.2426 - f1_34: 0.0638 - f1_36: 0.1706 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.1459 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0475 - f1_45: 0.0000e+00 - val_loss: 0.5337 - val_accuracy: 0.9219 - val_f1: 0.0396 - val_f1_1: 0.1734 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.1173 - val_f1_6: 0.1759 - val_f1_7: 0.0612 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0800 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0529 - val_f1_26: 0.0930 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.1292 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2575 - val_f1_34: 0.0640 - val_f1_36: 0.1742 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.1466 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0570 - val_f1_45: 0.0000e+00\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - 149s 9s/step - loss: 0.5361 - accuracy: 0.9215 - f1: 0.0426 - f1_1: 0.2054 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.1231 - f1_6: 0.2033 - f1_7: 0.0677 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0612 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0360 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0603 - f1_26: 0.1030 - f1_27: 0.0000e+00 - f1_28: 0.0000e+00 - f1_30: 0.1230 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.2423 - f1_34: 0.0657 - f1_36: 0.1736 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.1811 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0568 - f1_45: 0.0000e+00 - val_loss: 0.5298 - val_accuracy: 0.9227 - val_f1: 0.0431 - val_f1_1: 0.1733 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.1174 - val_f1_6: 0.1734 - val_f1_7: 0.0617 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0799 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0706 - val_f1_26: 0.1348 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.1292 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2575 - val_f1_34: 0.0662 - val_f1_36: 0.1773 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.1990 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0819 - val_f1_45: 0.0000e+00\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - 147s 9s/step - loss: 0.5217 - accuracy: 0.9220 - f1: 0.0455 - f1_1: 0.2060 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.1225 - f1_6: 0.1916 - f1_7: 0.0723 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0600 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0356 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0709 - f1_26: 0.1340 - f1_27: 0.0231 - f1_28: 0.0000e+00 - f1_30: 0.1238 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.2421 - f1_34: 0.0711 - f1_36: 0.1775 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.2129 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0761 - f1_45: 0.0000e+00 - val_loss: 0.5149 - val_accuracy: 0.9234 - val_f1: 0.0561 - val_f1_1: 0.1735 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.1179 - val_f1_6: 0.2690 - val_f1_7: 0.0654 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0797 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.1208 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0736 - val_f1_26: 0.2206 - val_f1_27: 0.0297 - val_f1_28: 0.0000e+00 - val_f1_30: 0.1293 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2575 - val_f1_34: 0.0688 - val_f1_36: 0.1766 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.3680 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0933 - val_f1_45: 0.0000e+00\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - 149s 9s/step - loss: 0.4944 - accuracy: 0.9227 - f1: 0.0535 - f1_1: 0.2030 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.1257 - f1_6: 0.2130 - f1_7: 0.0832 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0605 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0773 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0723 - f1_26: 0.2039 - f1_27: 0.0543 - f1_28: 0.0000e+00 - f1_30: 0.1249 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.2439 - f1_34: 0.0813 - f1_36: 0.2069 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.2881 - f1_41: 0.0000e+00 - f1_42: 0.0308 - f1_43: 0.0000e+00 - f1_44: 0.0725 - f1_45: 0.0000e+00 - val_loss: 0.4794 - val_accuracy: 0.9240 - val_f1: 0.0546 - val_f1_1: 0.1736 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.1189 - val_f1_6: 0.1695 - val_f1_7: 0.0910 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0787 - val_f1_13: 0.0687 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0708 - val_f1_26: 0.1733 - val_f1_27: 0.1164 - val_f1_28: 0.0000e+00 - val_f1_30: 0.1296 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2574 - val_f1_34: 0.0865 - val_f1_36: 0.2135 - val_f1_37: 0.0119 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.2915 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0447 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0897 - val_f1_45: 0.0000e+00\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - 148s 9s/step - loss: 0.4761 - accuracy: 0.9241 - f1: 0.0538 - f1_1: 0.2049 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.1241 - f1_6: 0.1979 - f1_7: 0.0861 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0597 - f1_13: 0.0121 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0334 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0920 - f1_26: 0.1236 - f1_27: 0.1623 - f1_28: 0.0000e+00 - f1_30: 0.1245 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.2430 - f1_34: 0.0990 - f1_36: 0.1863 - f1_37: 0.0127 - f1_38: 0.0000e+00 - f1_39: 0.0579 - f1_40: 0.1977 - f1_41: 0.0000e+00 - f1_42: 0.0507 - f1_43: 0.0000e+00 - f1_44: 0.0823 - f1_45: 0.0000e+00 - val_loss: 0.4669 - val_accuracy: 0.9242 - val_f1: 0.0513 - val_f1_1: 0.1746 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.1176 - val_f1_6: 0.1851 - val_f1_7: 0.0705 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0785 - val_f1_13: 0.0310 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.1418 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0495 - val_f1_26: 0.1434 - val_f1_27: 0.0959 - val_f1_28: 0.0000e+00 - val_f1_30: 0.1303 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2575 - val_f1_34: 0.0827 - val_f1_36: 0.1805 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0098 - val_f1_40: 0.1446 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0622 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0954 - val_f1_45: 0.0000e+00\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - 147s 9s/step - loss: 0.4661 - accuracy: 0.9238 - f1: 0.0644 - f1_1: 0.2111 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.1254 - f1_6: 0.2749 - f1_7: 0.1008 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0531 - f1_13: 0.0125 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0666 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0700 - f1_26: 0.2073 - f1_27: 0.1615 - f1_28: 0.0000e+00 - f1_30: 0.1300 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.2423 - f1_34: 0.1264 - f1_36: 0.2626 - f1_37: 0.0050 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.3370 - f1_41: 0.0000e+00 - f1_42: 0.1266 - f1_43: 0.0000e+00 - f1_44: 0.0616 - f1_45: 0.0000e+00 - val_loss: 0.4590 - val_accuracy: 0.9248 - val_f1: 0.0603 - val_f1_1: 0.2073 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.1262 - val_f1_6: 0.1827 - val_f1_7: 0.1099 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0705 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0367 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0373 - val_f1_26: 0.1912 - val_f1_27: 0.1048 - val_f1_28: 0.0000e+00 - val_f1_30: 0.1402 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2574 - val_f1_34: 0.1083 - val_f1_36: 0.2692 - val_f1_37: 0.0093 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0090 - val_f1_40: 0.3538 - val_f1_41: 0.0000e+00 - val_f1_42: 0.1495 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0501 - val_f1_45: 0.0000e+00\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - 151s 10s/step - loss: 0.4758 - accuracy: 0.9164 - f1: 0.0647 - f1_1: 0.2311 - f1_3: 0.0000e+00 - f1_4: 0.0000e+00 - f1_5: 0.1356 - f1_6: 0.1972 - f1_7: 0.1394 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0556 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0406 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0309 - f1_26: 0.2223 - f1_27: 0.1720 - f1_28: 0.0000e+00 - f1_30: 0.1493 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.2433 - f1_34: 0.1250 - f1_36: 0.2354 - f1_37: 0.0162 - f1_38: 0.0000e+00 - f1_39: 0.0219 - f1_40: 0.3512 - f1_41: 0.0000e+00 - f1_42: 0.1226 - f1_43: 0.0000e+00 - f1_44: 0.1002 - f1_45: 0.0000e+00 - val_loss: 0.4875 - val_accuracy: 0.9102 - val_f1: 0.0622 - val_f1_1: 0.1759 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0000e+00 - val_f1_5: 0.1207 - val_f1_6: 0.0000e+00 - val_f1_7: 0.0724 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0746 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0295 - val_f1_26: 0.2157 - val_f1_27: 0.1514 - val_f1_28: 0.0000e+00 - val_f1_30: 0.1462 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2571 - val_f1_34: 0.1450 - val_f1_36: 0.1941 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0273 - val_f1_40: 0.6514 - val_f1_41: 0.0000e+00 - val_f1_42: 0.1472 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0800 - val_f1_45: 0.0000e+00\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - 156s 10s/step - loss: 0.4702 - accuracy: 0.9122 - f1: 0.0629 - f1_1: 0.2052 - f1_3: 0.0000e+00 - f1_4: 0.0308 - f1_5: 0.1244 - f1_6: 0.1563 - f1_7: 0.0788 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0607 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0315 - f1_26: 0.2176 - f1_27: 0.1766 - f1_28: 0.0000e+00 - f1_30: 0.1476 - f1_31: 0.0208 - f1_32: 0.0000e+00 - f1_33: 0.2423 - f1_34: 0.1426 - f1_36: 0.1917 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0536 - f1_40: 0.4291 - f1_41: 0.0000e+00 - f1_42: 0.1512 - f1_43: 0.0000e+00 - f1_44: 0.0544 - f1_45: 0.0000e+00 - val_loss: 0.4533 - val_accuracy: 0.9220 - val_f1: 0.0612 - val_f1_1: 0.1732 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0375 - val_f1_5: 0.1181 - val_f1_6: 0.2806 - val_f1_7: 0.0852 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0788 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0307 - val_f1_26: 0.1655 - val_f1_27: 0.0528 - val_f1_28: 0.0000e+00 - val_f1_30: 0.1537 - val_f1_31: 0.0303 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2575 - val_f1_34: 0.1118 - val_f1_36: 0.1961 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.1721 - val_f1_40: 0.1988 - val_f1_41: 0.0000e+00 - val_f1_42: 0.1673 - val_f1_43: 0.0000e+00 - val_f1_44: 0.1383 - val_f1_45: 0.0000e+00\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - 148s 9s/step - loss: 0.4464 - accuracy: 0.9251 - f1: 0.0648 - f1_1: 0.2057 - f1_3: 0.0000e+00 - f1_4: 0.0526 - f1_5: 0.1275 - f1_6: 0.3084 - f1_7: 0.1092 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0627 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0332 - f1_26: 0.1731 - f1_27: 0.1087 - f1_28: 0.0000e+00 - f1_30: 0.1526 - f1_31: 0.0270 - f1_32: 0.0000e+00 - f1_33: 0.2437 - f1_34: 0.1220 - f1_36: 0.2510 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.1224 - f1_40: 0.2276 - f1_41: 0.0000e+00 - f1_42: 0.1545 - f1_43: 0.0000e+00 - f1_44: 0.1084 - f1_45: 0.0000e+00 - val_loss: 0.4385 - val_accuracy: 0.9278 - val_f1: 0.0718 - val_f1_1: 0.1798 - val_f1_3: 0.0000e+00 - val_f1_4: 0.0724 - val_f1_5: 0.1236 - val_f1_6: 0.2524 - val_f1_7: 0.1354 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0856 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0313 - val_f1_26: 0.2357 - val_f1_27: 0.1497 - val_f1_28: 0.0000e+00 - val_f1_30: 0.1658 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2574 - val_f1_34: 0.1267 - val_f1_36: 0.2951 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.1540 - val_f1_40: 0.3043 - val_f1_41: 0.0000e+00 - val_f1_42: 0.1815 - val_f1_43: 0.0000e+00 - val_f1_44: 0.1223 - val_f1_45: 0.0000e+00\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - 148s 9s/step - loss: 0.4326 - accuracy: 0.9315 - f1: 0.0809 - f1_1: 0.2400 - f1_3: 0.0000e+00 - f1_4: 0.1447 - f1_5: 0.1410 - f1_6: 0.3178 - f1_7: 0.1656 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0643 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0613 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0412 - f1_26: 0.2147 - f1_27: 0.2119 - f1_28: 0.0000e+00 - f1_30: 0.1960 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.2426 - f1_34: 0.1265 - f1_36: 0.3213 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.1615 - f1_40: 0.3110 - f1_41: 0.0000e+00 - f1_42: 0.2083 - f1_43: 0.0000e+00 - f1_44: 0.0671 - f1_45: 0.0000e+00 - val_loss: 0.4229 - val_accuracy: 0.9379 - val_f1: 0.0808 - val_f1_1: 0.2664 - val_f1_3: 0.0000e+00 - val_f1_4: 0.1511 - val_f1_5: 0.1470 - val_f1_6: 0.3219 - val_f1_7: 0.0489 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0880 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0107 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0642 - val_f1_26: 0.2274 - val_f1_27: 0.1683 - val_f1_28: 0.0000e+00 - val_f1_30: 0.2384 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2576 - val_f1_34: 0.1501 - val_f1_36: 0.3524 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.1925 - val_f1_40: 0.3345 - val_f1_41: 0.0000e+00 - val_f1_42: 0.1635 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0484 - val_f1_45: 0.0000e+00\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - 147s 9s/step - loss: 0.4253 - accuracy: 0.9351 - f1: 0.0909 - f1_1: 0.3427 - f1_3: 0.0000e+00 - f1_4: 0.2132 - f1_5: 0.1423 - f1_6: 0.3415 - f1_7: 0.0878 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0864 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0114 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0100 - f1_26: 0.1989 - f1_27: 0.2014 - f1_28: 0.0000e+00 - f1_30: 0.2573 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.2488 - f1_34: 0.1684 - f1_36: 0.3813 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0782 - f1_40: 0.5452 - f1_41: 0.0000e+00 - f1_42: 0.2172 - f1_43: 0.0000e+00 - f1_44: 0.1031 - f1_45: 0.0000e+00 - val_loss: 0.4320 - val_accuracy: 0.9262 - val_f1: 0.1135 - val_f1_1: 0.3052 - val_f1_3: 0.0000e+00 - val_f1_4: 0.1907 - val_f1_5: 0.1181 - val_f1_6: 0.3471 - val_f1_7: 0.2576 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.1000 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.3977 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0025 - val_f1_26: 0.2827 - val_f1_27: 0.1676 - val_f1_28: 0.0000e+00 - val_f1_30: 0.2718 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2687 - val_f1_34: 0.1576 - val_f1_36: 0.4287 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.1737 - val_f1_40: 0.7443 - val_f1_41: 0.0000e+00 - val_f1_42: 0.2739 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0538 - val_f1_45: 0.0000e+00\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - 146s 9s/step - loss: 0.4562 - accuracy: 0.9103 - f1: 0.0918 - f1_1: 0.2692 - f1_3: 0.0000e+00 - f1_4: 0.1753 - f1_5: 0.1255 - f1_6: 0.1821 - f1_7: 0.2528 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0885 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.1120 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0541 - f1_26: 0.2959 - f1_27: 0.1908 - f1_28: 0.0000e+00 - f1_30: 0.2149 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.2440 - f1_34: 0.1444 - f1_36: 0.3011 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.1198 - f1_40: 0.6628 - f1_41: 0.0000e+00 - f1_42: 0.1946 - f1_43: 0.0000e+00 - f1_44: 0.0430 - f1_45: 0.0000e+00 - val_loss: 0.4551 - val_accuracy: 0.9108 - val_f1: 0.0865 - val_f1_1: 0.1743 - val_f1_3: 0.0000e+00 - val_f1_4: 0.2256 - val_f1_5: 0.1218 - val_f1_6: 0.3265 - val_f1_7: 0.2751 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0825 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0630 - val_f1_26: 0.3265 - val_f1_27: 0.1628 - val_f1_28: 0.0000e+00 - val_f1_30: 0.1866 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2574 - val_f1_34: 0.1555 - val_f1_36: 0.2034 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.6531 - val_f1_41: 0.0000e+00 - val_f1_42: 0.1984 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0483 - val_f1_45: 0.0000e+00\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - 142s 9s/step - loss: 0.4494 - accuracy: 0.9140 - f1: 0.0837 - f1_1: 0.2068 - f1_3: 0.0000e+00 - f1_4: 0.2623 - f1_5: 0.1293 - f1_6: 0.3368 - f1_7: 0.2617 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0744 - f1_13: 0.0244 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.1393 - f1_26: 0.2343 - f1_27: 0.1854 - f1_28: 0.0000e+00 - f1_30: 0.1665 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.2427 - f1_34: 0.1675 - f1_36: 0.2342 - f1_37: 0.0065 - f1_38: 0.0000e+00 - f1_39: 0.1135 - f1_40: 0.3813 - f1_41: 0.0000e+00 - f1_42: 0.1370 - f1_43: 0.0000e+00 - f1_44: 0.0426 - f1_45: 0.0000e+00 - val_loss: 0.4385 - val_accuracy: 0.9204 - val_f1: 0.0805 - val_f1_1: 0.1807 - val_f1_3: 0.0000e+00 - val_f1_4: 0.2112 - val_f1_5: 0.1238 - val_f1_6: 0.2787 - val_f1_7: 0.1567 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.1248 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0543 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.1128 - val_f1_26: 0.2589 - val_f1_27: 0.1363 - val_f1_28: 0.0000e+00 - val_f1_30: 0.1732 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2584 - val_f1_34: 0.1406 - val_f1_36: 0.2486 - val_f1_37: 0.0353 - val_f1_38: 0.0000e+00 - val_f1_39: 0.1793 - val_f1_40: 0.3160 - val_f1_41: 0.0000e+00 - val_f1_42: 0.1810 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0503 - val_f1_45: 0.0000e+00\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - 144s 9s/step - loss: 0.4352 - accuracy: 0.9260 - f1: 0.0937 - f1_1: 0.2387 - f1_3: 0.0000e+00 - f1_4: 0.2368 - f1_5: 0.1337 - f1_6: 0.2892 - f1_7: 0.2168 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.1325 - f1_13: 0.0094 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.2690 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.1575 - f1_26: 0.2123 - f1_27: 0.1955 - f1_28: 0.0000e+00 - f1_30: 0.1851 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.2450 - f1_34: 0.1399 - f1_36: 0.2747 - f1_37: 0.0283 - f1_38: 0.0000e+00 - f1_39: 0.2015 - f1_40: 0.2428 - f1_41: 0.0000e+00 - f1_42: 0.2950 - f1_43: 0.0000e+00 - f1_44: 0.0444 - f1_45: 0.0000e+00 - val_loss: 0.4294 - val_accuracy: 0.9287 - val_f1: 0.0943 - val_f1_1: 0.2535 - val_f1_3: 0.0000e+00 - val_f1_4: 0.2294 - val_f1_5: 0.1311 - val_f1_6: 0.2540 - val_f1_7: 0.2708 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.2009 - val_f1_13: 0.0168 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.1102 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.1222 - val_f1_26: 0.2342 - val_f1_27: 0.1760 - val_f1_28: 0.0000e+00 - val_f1_30: 0.2048 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2631 - val_f1_34: 0.1498 - val_f1_36: 0.2986 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.2386 - val_f1_40: 0.2341 - val_f1_41: 0.0000e+00 - val_f1_42: 0.3349 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0507 - val_f1_45: 0.0000e+00\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - 146s 9s/step - loss: 0.4257 - accuracy: 0.9344 - f1: 0.1028 - f1_1: 0.2950 - f1_3: 0.0000e+00 - f1_4: 0.2308 - f1_5: 0.1635 - f1_6: 0.3909 - f1_7: 0.3125 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.2054 - f1_13: 0.0213 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.1576 - f1_26: 0.2615 - f1_27: 0.2172 - f1_28: 0.0000e+00 - f1_30: 0.2316 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.2588 - f1_34: 0.1648 - f1_36: 0.3285 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.2023 - f1_40: 0.3571 - f1_41: 0.0000e+00 - f1_42: 0.2696 - f1_43: 0.0000e+00 - f1_44: 0.0454 - f1_45: 0.0000e+00 - val_loss: 0.4146 - val_accuracy: 0.9359 - val_f1: 0.1042 - val_f1_1: 0.2658 - val_f1_3: 0.0000e+00 - val_f1_4: 0.1367 - val_f1_5: 0.1924 - val_f1_6: 0.4636 - val_f1_7: 0.2526 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.2645 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0640 - val_f1_26: 0.3067 - val_f1_27: 0.1892 - val_f1_28: 0.0000e+00 - val_f1_30: 0.2613 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.3031 - val_f1_34: 0.1610 - val_f1_36: 0.3456 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.1081 - val_f1_40: 0.4609 - val_f1_41: 0.0000e+00 - val_f1_42: 0.3331 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0581 - val_f1_45: 0.0000e+00\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - 162s 10s/step - loss: 0.4138 - accuracy: 0.9403 - f1: 0.1115 - f1_1: 0.3905 - f1_3: 0.0000e+00 - f1_4: 0.2745 - f1_5: 0.1964 - f1_6: 0.4408 - f1_7: 0.2166 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.2845 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0125 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0000e+00 - f1_24: 0.0000e+00 - f1_25: 0.0604 - f1_26: 0.2783 - f1_27: 0.2492 - f1_28: 0.0000e+00 - f1_30: 0.3047 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.3714 - f1_34: 0.1887 - f1_36: 0.3376 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0743 - f1_40: 0.4345 - f1_41: 0.0000e+00 - f1_42: 0.2802 - f1_43: 0.0000e+00 - f1_44: 0.0650 - f1_45: 0.0000e+00 - val_loss: 0.4097 - val_accuracy: 0.9438 - val_f1: 0.1136 - val_f1_1: 0.5052 - val_f1_3: 0.0000e+00 - val_f1_4: 0.1111 - val_f1_5: 0.2100 - val_f1_6: 0.4859 - val_f1_7: 0.0129 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.4661 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.0000e+00 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.3080 - val_f1_27: 0.1677 - val_f1_28: 0.0000e+00 - val_f1_30: 0.3420 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.4385 - val_f1_34: 0.1955 - val_f1_36: 0.5326 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0409 - val_f1_40: 0.5725 - val_f1_41: 0.0000e+00 - val_f1_42: 0.1138 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0394 - val_f1_45: 0.0000e+00\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - 157s 10s/step - loss: 0.3948 - accuracy: 0.9325 - f1: 0.1132 - f1_1: 0.4495 - f1_3: 0.0000e+00 - f1_4: 0.3068 - f1_5: 0.1629 - f1_6: 0.3399 - f1_7: 0.1103 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.2762 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.0752 - f1_24: 0.0000e+00 - f1_25: 0.1005 - f1_26: 0.2360 - f1_27: 0.1811 - f1_28: 0.0000e+00 - f1_30: 0.3080 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.3471 - f1_34: 0.2152 - f1_36: 0.4317 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0604 - f1_40: 0.7265 - f1_41: 0.0000e+00 - f1_42: 0.1520 - f1_43: 0.0000e+00 - f1_44: 0.0503 - f1_45: 0.0000e+00 - val_loss: 0.3952 - val_accuracy: 0.9096 - val_f1: 0.0835 - val_f1_1: 0.1998 - val_f1_3: 0.0000e+00 - val_f1_4: 0.3712 - val_f1_5: 0.1082 - val_f1_6: 0.1580 - val_f1_7: 0.2707 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0809 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.1847 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0000e+00 - val_f1_26: 0.0929 - val_f1_27: 0.0000e+00 - val_f1_28: 0.0000e+00 - val_f1_30: 0.2819 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2600 - val_f1_34: 0.1970 - val_f1_36: 0.2138 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0000e+00 - val_f1_40: 0.8675 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0526 - val_f1_45: 0.0000e+00\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - 150s 9s/step - loss: 0.3771 - accuracy: 0.9124 - f1: 0.0728 - f1_1: 0.2255 - f1_3: 0.0000e+00 - f1_4: 0.3449 - f1_5: 0.1169 - f1_6: 0.2089 - f1_7: 0.1313 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0616 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.1908 - f1_24: 0.0000e+00 - f1_25: 0.0543 - f1_26: 0.1219 - f1_27: 0.1250 - f1_28: 0.0000e+00 - f1_30: 0.1746 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.2444 - f1_34: 0.1375 - f1_36: 0.2033 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0000e+00 - f1_40: 0.5031 - f1_41: 0.0000e+00 - f1_42: 0.0000e+00 - f1_43: 0.0000e+00 - f1_44: 0.0698 - f1_45: 0.0000e+00 - val_loss: 0.3555 - val_accuracy: 0.9177 - val_f1: 0.0714 - val_f1_1: 0.1866 - val_f1_3: 0.0000e+00 - val_f1_4: 0.2060 - val_f1_5: 0.1126 - val_f1_6: 0.5515 - val_f1_7: 0.0983 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0811 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.1964 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0920 - val_f1_26: 0.1027 - val_f1_27: 0.0996 - val_f1_28: 0.0000e+00 - val_f1_30: 0.1402 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2574 - val_f1_34: 0.1146 - val_f1_36: 0.1920 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.1722 - val_f1_40: 0.1917 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0000e+00 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0605 - val_f1_45: 0.0000e+00\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - 142s 9s/step - loss: 0.3344 - accuracy: 0.9203 - f1: 0.0773 - f1_1: 0.2159 - f1_3: 0.0000e+00 - f1_4: 0.2198 - f1_5: 0.1200 - f1_6: 0.4774 - f1_7: 0.0941 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0627 - f1_13: 0.0000e+00 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0259 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.1965 - f1_24: 0.0000e+00 - f1_25: 0.1583 - f1_26: 0.1819 - f1_27: 0.1753 - f1_28: 0.0000e+00 - f1_30: 0.1390 - f1_31: 0.0044 - f1_32: 0.0000e+00 - f1_33: 0.2434 - f1_34: 0.1554 - f1_36: 0.2212 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.0424 - f1_40: 0.2682 - f1_41: 0.0000e+00 - f1_42: 0.0371 - f1_43: 0.0000e+00 - f1_44: 0.0530 - f1_45: 0.0000e+00 - val_loss: 0.3104 - val_accuracy: 0.9230 - val_f1: 0.0836 - val_f1_1: 0.1832 - val_f1_3: 0.0000e+00 - val_f1_4: 0.1808 - val_f1_5: 0.1126 - val_f1_6: 0.2844 - val_f1_7: 0.0848 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.0908 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.3165 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0818 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.1984 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0736 - val_f1_26: 0.2924 - val_f1_27: 0.1373 - val_f1_28: 0.0000e+00 - val_f1_30: 0.1466 - val_f1_31: 0.0205 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2572 - val_f1_34: 0.1609 - val_f1_36: 0.2188 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.0320 - val_f1_40: 0.3608 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0482 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0630 - val_f1_45: 0.0000e+00\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - 140s 9s/step - loss: 0.3049 - accuracy: 0.9248 - f1: 0.0794 - f1_1: 0.2350 - f1_3: 0.0000e+00 - f1_4: 0.2434 - f1_5: 0.1214 - f1_6: 0.4034 - f1_7: 0.1233 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.0788 - f1_13: 0.0281 - f1_14: 0.0000e+00 - f1_15: 0.1781 - f1_16: 0.0000e+00 - f1_17: 0.0503 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.1955 - f1_24: 0.0000e+00 - f1_25: 0.0637 - f1_26: 0.1690 - f1_27: 0.0885 - f1_28: 0.0000e+00 - f1_30: 0.1498 - f1_31: 0.0167 - f1_32: 0.0000e+00 - f1_33: 0.2437 - f1_34: 0.1413 - f1_36: 0.2246 - f1_37: 0.0048 - f1_38: 0.0000e+00 - f1_39: 0.0509 - f1_40: 0.2288 - f1_41: 0.0000e+00 - f1_42: 0.0782 - f1_43: 0.0000e+00 - f1_44: 0.0578 - f1_45: 0.0000e+00 - val_loss: 0.2978 - val_accuracy: 0.9265 - val_f1: 0.0922 - val_f1_1: 0.2072 - val_f1_3: 0.0000e+00 - val_f1_4: 0.3231 - val_f1_5: 0.1172 - val_f1_6: 0.4047 - val_f1_7: 0.1589 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.1094 - val_f1_13: 0.0111 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0595 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.2007 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0654 - val_f1_26: 0.2110 - val_f1_27: 0.0891 - val_f1_28: 0.0000e+00 - val_f1_30: 0.1847 - val_f1_31: 0.1698 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2596 - val_f1_34: 0.1540 - val_f1_36: 0.2330 - val_f1_37: 0.0256 - val_f1_38: 0.0000e+00 - val_f1_39: 0.1758 - val_f1_40: 0.2613 - val_f1_41: 0.0000e+00 - val_f1_42: 0.1815 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0855 - val_f1_45: 0.0000e+00\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - 140s 9s/step - loss: 0.2920 - accuracy: 0.9313 - f1: 0.0993 - f1_1: 0.2633 - f1_3: 0.0000e+00 - f1_4: 0.2655 - f1_5: 0.1239 - f1_6: 0.4732 - f1_7: 0.1596 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.2298 - f1_13: 0.0098 - f1_14: 0.0000e+00 - f1_15: 0.0179 - f1_16: 0.0000e+00 - f1_17: 0.0495 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.1999 - f1_24: 0.0000e+00 - f1_25: 0.0800 - f1_26: 0.1918 - f1_27: 0.1437 - f1_28: 0.0000e+00 - f1_30: 0.2088 - f1_31: 0.0446 - f1_32: 0.0000e+00 - f1_33: 0.2470 - f1_34: 0.1450 - f1_36: 0.2656 - f1_37: 0.0310 - f1_38: 0.0000e+00 - f1_39: 0.1887 - f1_40: 0.3178 - f1_41: 0.0000e+00 - f1_42: 0.2281 - f1_43: 0.0000e+00 - f1_44: 0.0860 - f1_45: 0.0000e+00 - val_loss: 0.2864 - val_accuracy: 0.9318 - val_f1: 0.1063 - val_f1_1: 0.2686 - val_f1_3: 0.0000e+00 - val_f1_4: 0.1582 - val_f1_5: 0.1172 - val_f1_6: 0.6260 - val_f1_7: 0.1674 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.4851 - val_f1_13: 0.0266 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0158 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.2007 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0725 - val_f1_26: 0.2097 - val_f1_27: 0.1257 - val_f1_28: 0.0000e+00 - val_f1_30: 0.2181 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2638 - val_f1_34: 0.1027 - val_f1_36: 0.2790 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.2531 - val_f1_40: 0.3464 - val_f1_41: 0.0000e+00 - val_f1_42: 0.2189 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0967 - val_f1_45: 0.0000e+00\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - 136s 9s/step - loss: 0.2789 - accuracy: 0.9379 - f1: 0.1173 - f1_1: 0.2914 - f1_3: 0.0000e+00 - f1_4: 0.2637 - f1_5: 0.1240 - f1_6: 0.6363 - f1_7: 0.2281 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.3628 - f1_13: 0.0583 - f1_14: 0.0000e+00 - f1_15: 0.0089 - f1_16: 0.0000e+00 - f1_17: 0.0443 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.2027 - f1_24: 0.0000e+00 - f1_25: 0.1167 - f1_26: 0.2353 - f1_27: 0.1888 - f1_28: 0.0000e+00 - f1_30: 0.2225 - f1_31: 0.0078 - f1_32: 0.0000e+00 - f1_33: 0.2552 - f1_34: 0.1366 - f1_36: 0.3154 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.2189 - f1_40: 0.3934 - f1_41: 0.0000e+00 - f1_42: 0.2848 - f1_43: 0.0000e+00 - f1_44: 0.0949 - f1_45: 0.0000e+00 - val_loss: 0.2727 - val_accuracy: 0.9386 - val_f1: 0.1199 - val_f1_1: 0.2908 - val_f1_3: 0.0000e+00 - val_f1_4: 0.2569 - val_f1_5: 0.1177 - val_f1_6: 0.5521 - val_f1_7: 0.2453 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.4841 - val_f1_13: 0.0426 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.2070 - val_f1_24: 0.0000e+00 - val_f1_25: 0.1215 - val_f1_26: 0.2617 - val_f1_27: 0.1528 - val_f1_28: 0.0000e+00 - val_f1_30: 0.2311 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.2990 - val_f1_34: 0.1536 - val_f1_36: 0.3434 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.2776 - val_f1_40: 0.4038 - val_f1_41: 0.0000e+00 - val_f1_42: 0.2470 - val_f1_43: 0.0000e+00 - val_f1_44: 0.1092 - val_f1_45: 0.0000e+00\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - 136s 9s/step - loss: 0.2683 - accuracy: 0.9416 - f1: 0.1344 - f1_1: 0.3582 - f1_3: 0.0000e+00 - f1_4: 0.3687 - f1_5: 0.1269 - f1_6: 0.6231 - f1_7: 0.2873 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.4720 - f1_13: 0.0534 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.2092 - f1_24: 0.0000e+00 - f1_25: 0.1692 - f1_26: 0.2522 - f1_27: 0.2193 - f1_28: 0.0000e+00 - f1_30: 0.2292 - f1_31: 0.0444 - f1_32: 0.0000e+00 - f1_33: 0.3337 - f1_34: 0.1756 - f1_36: 0.3789 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.2346 - f1_40: 0.4325 - f1_41: 0.0000e+00 - f1_42: 0.2966 - f1_43: 0.0000e+00 - f1_44: 0.1098 - f1_45: 0.0000e+00 - val_loss: 0.2651 - val_accuracy: 0.9451 - val_f1: 0.1381 - val_f1_1: 0.3123 - val_f1_3: 0.0000e+00 - val_f1_4: 0.3371 - val_f1_5: 0.1264 - val_f1_6: 0.5146 - val_f1_7: 0.2884 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.2912 - val_f1_13: 0.0611 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.2214 - val_f1_24: 0.0000e+00 - val_f1_25: 0.1307 - val_f1_26: 0.3614 - val_f1_27: 0.2734 - val_f1_28: 0.0000e+00 - val_f1_30: 0.2973 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.4080 - val_f1_34: 0.1892 - val_f1_36: 0.3575 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.2884 - val_f1_40: 0.6463 - val_f1_41: 0.0000e+00 - val_f1_42: 0.2730 - val_f1_43: 0.0000e+00 - val_f1_44: 0.1471 - val_f1_45: 0.0000e+00\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - 137s 9s/step - loss: 0.2572 - accuracy: 0.9496 - f1: 0.1525 - f1_1: 0.4162 - f1_3: 0.0000e+00 - f1_4: 0.3787 - f1_5: 0.1735 - f1_6: 0.5499 - f1_7: 0.2884 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.4862 - f1_13: 0.0262 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.3448 - f1_24: 0.0000e+00 - f1_25: 0.1318 - f1_26: 0.2791 - f1_27: 0.2754 - f1_28: 0.0000e+00 - f1_30: 0.3047 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.4056 - f1_34: 0.2098 - f1_36: 0.5502 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.2339 - f1_40: 0.5700 - f1_41: 0.0000e+00 - f1_42: 0.3558 - f1_43: 0.0000e+00 - f1_44: 0.1194 - f1_45: 0.0000e+00 - val_loss: 0.2569 - val_accuracy: 0.9472 - val_f1: 0.1679 - val_f1_1: 0.4154 - val_f1_3: 0.0000e+00 - val_f1_4: 0.4199 - val_f1_5: 0.1948 - val_f1_6: 0.5068 - val_f1_7: 0.2687 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.5975 - val_f1_13: 0.0079 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.4066 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0896 - val_f1_26: 0.3775 - val_f1_27: 0.2983 - val_f1_28: 0.0000e+00 - val_f1_30: 0.3034 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.4331 - val_f1_34: 0.2512 - val_f1_36: 0.7263 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.1225 - val_f1_40: 0.7964 - val_f1_41: 0.0000e+00 - val_f1_42: 0.4656 - val_f1_43: 0.0000e+00 - val_f1_44: 0.0358 - val_f1_45: 0.0000e+00\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - 140s 9s/step - loss: 0.2611 - accuracy: 0.9453 - f1: 0.1532 - f1_1: 0.4024 - f1_3: 0.0000e+00 - f1_4: 0.3708 - f1_5: 0.1494 - f1_6: 0.5373 - f1_7: 0.2432 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.5042 - f1_13: 0.0737 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0452 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.3680 - f1_24: 0.0000e+00 - f1_25: 0.1239 - f1_26: 0.3151 - f1_27: 0.3061 - f1_28: 0.0000e+00 - f1_30: 0.3545 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.4026 - f1_34: 0.2236 - f1_36: 0.5175 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.2146 - f1_40: 0.7015 - f1_41: 0.0000e+00 - f1_42: 0.1602 - f1_43: 0.0000e+00 - f1_44: 0.1146 - f1_45: 0.0000e+00 - val_loss: 0.2500 - val_accuracy: 0.9493 - val_f1: 0.1526 - val_f1_1: 0.3335 - val_f1_3: 0.0000e+00 - val_f1_4: 0.3552 - val_f1_5: 0.1259 - val_f1_6: 0.6399 - val_f1_7: 0.2428 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.5753 - val_f1_13: 0.1564 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.3714 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0067 - val_f1_26: 0.3129 - val_f1_27: 0.1537 - val_f1_28: 0.0000e+00 - val_f1_30: 0.2709 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.4168 - val_f1_34: 0.2126 - val_f1_36: 0.4811 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.3498 - val_f1_40: 0.5755 - val_f1_41: 0.0000e+00 - val_f1_42: 0.2778 - val_f1_43: 0.0000e+00 - val_f1_44: 0.2449 - val_f1_45: 0.0000e+00\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - 146s 9s/step - loss: 0.2510 - accuracy: 0.9504 - f1: 0.1652 - f1_1: 0.4061 - f1_3: 0.0000e+00 - f1_4: 0.3950 - f1_5: 0.2007 - f1_6: 0.5658 - f1_7: 0.2626 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.5520 - f1_13: 0.0658 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.3863 - f1_24: 0.0000e+00 - f1_25: 0.0943 - f1_26: 0.3711 - f1_27: 0.2490 - f1_28: 0.0000e+00 - f1_30: 0.2605 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.4104 - f1_34: 0.2632 - f1_36: 0.5736 - f1_37: 0.0646 - f1_38: 0.0000e+00 - f1_39: 0.2619 - f1_40: 0.7470 - f1_41: 0.0000e+00 - f1_42: 0.2768 - f1_43: 0.0000e+00 - f1_44: 0.2032 - f1_45: 0.0000e+00 - val_loss: 0.2509 - val_accuracy: 0.9503 - val_f1: 0.1468 - val_f1_1: 0.4329 - val_f1_3: 0.0000e+00 - val_f1_4: 0.4104 - val_f1_5: 0.2320 - val_f1_6: 0.4058 - val_f1_7: 0.1148 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.5553 - val_f1_13: 0.0083 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.3801 - val_f1_24: 0.0000e+00 - val_f1_25: 0.1217 - val_f1_26: 0.4193 - val_f1_27: 0.1144 - val_f1_28: 0.0000e+00 - val_f1_30: 0.2741 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.4346 - val_f1_34: 0.2723 - val_f1_36: 0.4881 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.1647 - val_f1_40: 0.6914 - val_f1_41: 0.0000e+00 - val_f1_42: 0.0901 - val_f1_43: 0.0000e+00 - val_f1_44: 0.2611 - val_f1_45: 0.0000e+00\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - 147s 9s/step - loss: 0.2427 - accuracy: 0.9525 - f1: 0.1598 - f1_1: 0.4735 - f1_3: 0.0000e+00 - f1_4: 0.4202 - f1_5: 0.2478 - f1_6: 0.4479 - f1_7: 0.3014 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.4368 - f1_13: 0.0284 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.4523 - f1_24: 0.0000e+00 - f1_25: 0.1073 - f1_26: 0.3341 - f1_27: 0.2678 - f1_28: 0.0000e+00 - f1_30: 0.3663 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.4259 - f1_34: 0.2726 - f1_36: 0.6509 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.1661 - f1_40: 0.5540 - f1_41: 0.0000e+00 - f1_42: 0.2681 - f1_43: 0.0000e+00 - f1_44: 0.1722 - f1_45: 0.0000e+00 - val_loss: 0.2514 - val_accuracy: 0.9470 - val_f1: 0.1753 - val_f1_1: 0.5151 - val_f1_3: 0.0000e+00 - val_f1_4: 0.3551 - val_f1_5: 0.2462 - val_f1_6: 0.8369 - val_f1_7: 0.3486 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.6118 - val_f1_13: 0.0922 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.3489 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0552 - val_f1_26: 0.4265 - val_f1_27: 0.2672 - val_f1_28: 0.0000e+00 - val_f1_30: 0.3485 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.4475 - val_f1_34: 0.1136 - val_f1_36: 0.7235 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.3629 - val_f1_40: 0.5091 - val_f1_41: 0.0000e+00 - val_f1_42: 0.2678 - val_f1_43: 0.0000e+00 - val_f1_44: 0.1350 - val_f1_45: 0.0000e+00\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - 141s 9s/step - loss: 0.2558 - accuracy: 0.9446 - f1: 0.1726 - f1_1: 0.4587 - f1_3: 0.0000e+00 - f1_4: 0.4080 - f1_5: 0.2412 - f1_6: 0.7479 - f1_7: 0.3486 - f1_9: 0.0000e+00 - f1_10: 0.0000e+00 - f1_11: 0.0000e+00 - f1_12: 0.4536 - f1_13: 0.0584 - f1_14: 0.0000e+00 - f1_15: 0.0000e+00 - f1_16: 0.0000e+00 - f1_17: 0.0000e+00 - f1_18: 0.0000e+00 - f1_20: 0.0000e+00 - f1_21: 0.0000e+00 - f1_22: 0.0000e+00 - f1_23: 0.4260 - f1_24: 0.0000e+00 - f1_25: 0.1103 - f1_26: 0.4109 - f1_27: 0.2241 - f1_28: 0.0000e+00 - f1_30: 0.2940 - f1_31: 0.0000e+00 - f1_32: 0.0000e+00 - f1_33: 0.4125 - f1_34: 0.0737 - f1_36: 0.6906 - f1_37: 0.0000e+00 - f1_38: 0.0000e+00 - f1_39: 0.2213 - f1_40: 0.8090 - f1_41: 0.0000e+00 - f1_42: 0.3374 - f1_43: 0.0000e+00 - f1_44: 0.1781 - f1_45: 0.0000e+00 - val_loss: 0.2472 - val_accuracy: 0.9500 - val_f1: 0.1723 - val_f1_1: 0.4053 - val_f1_3: 0.0000e+00 - val_f1_4: 0.4498 - val_f1_5: 0.2171 - val_f1_6: 0.7641 - val_f1_7: 0.0771 - val_f1_9: 0.0000e+00 - val_f1_10: 0.0000e+00 - val_f1_11: 0.0000e+00 - val_f1_12: 0.6059 - val_f1_13: 0.0000e+00 - val_f1_14: 0.0000e+00 - val_f1_15: 0.0000e+00 - val_f1_16: 0.0000e+00 - val_f1_17: 0.0000e+00 - val_f1_18: 0.0000e+00 - val_f1_20: 0.0000e+00 - val_f1_21: 0.0000e+00 - val_f1_22: 0.0000e+00 - val_f1_23: 0.4752 - val_f1_24: 0.0000e+00 - val_f1_25: 0.0728 - val_f1_26: 0.3083 - val_f1_27: 0.2463 - val_f1_28: 0.0000e+00 - val_f1_30: 0.3448 - val_f1_31: 0.0000e+00 - val_f1_32: 0.0000e+00 - val_f1_33: 0.4372 - val_f1_34: 0.0883 - val_f1_36: 0.6060 - val_f1_37: 0.0000e+00 - val_f1_38: 0.0000e+00 - val_f1_39: 0.2453 - val_f1_40: 0.8084 - val_f1_41: 0.0000e+00 - val_f1_42: 0.4924 - val_f1_43: 0.0000e+00 - val_f1_44: 0.2470 - val_f1_45: 0.0000e+00\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - 139s 9s/step - loss: nan - accuracy: 0.9427 - f1: nan - f1_1: nan - f1_3: nan - f1_4: nan - f1_5: nan - f1_6: nan - f1_7: nan - f1_9: nan - f1_10: nan - f1_11: nan - f1_12: nan - f1_13: nan - f1_14: nan - f1_15: nan - f1_16: nan - f1_17: nan - f1_18: nan - f1_20: nan - f1_21: nan - f1_22: nan - f1_23: nan - f1_24: nan - f1_25: nan - f1_26: nan - f1_27: nan - f1_28: nan - f1_30: nan - f1_31: nan - f1_32: nan - f1_33: nan - f1_34: nan - f1_36: nan - f1_37: nan - f1_38: nan - f1_39: nan - f1_40: nan - f1_41: nan - f1_42: nan - f1_43: nan - f1_44: nan - f1_45: nan - val_loss: nan - val_accuracy: 0.9036 - val_f1: nan - val_f1_1: nan - val_f1_3: nan - val_f1_4: nan - val_f1_5: nan - val_f1_6: nan - val_f1_7: nan - val_f1_9: nan - val_f1_10: nan - val_f1_11: nan - val_f1_12: nan - val_f1_13: nan - val_f1_14: nan - val_f1_15: nan - val_f1_16: nan - val_f1_17: nan - val_f1_18: nan - val_f1_20: nan - val_f1_21: nan - val_f1_22: nan - val_f1_23: nan - val_f1_24: nan - val_f1_25: nan - val_f1_26: nan - val_f1_27: nan - val_f1_28: nan - val_f1_30: nan - val_f1_31: nan - val_f1_32: nan - val_f1_33: nan - val_f1_34: nan - val_f1_36: nan - val_f1_37: nan - val_f1_38: nan - val_f1_39: nan - val_f1_40: nan - val_f1_41: nan - val_f1_42: nan - val_f1_43: nan - val_f1_44: nan - val_f1_45: nan\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - 141s 9s/step - loss: nan - accuracy: 0.9031 - f1: nan - f1_1: nan - f1_3: nan - f1_4: nan - f1_5: nan - f1_6: nan - f1_7: nan - f1_9: nan - f1_10: nan - f1_11: nan - f1_12: nan - f1_13: nan - f1_14: nan - f1_15: nan - f1_16: nan - f1_17: nan - f1_18: nan - f1_20: nan - f1_21: nan - f1_22: nan - f1_23: nan - f1_24: nan - f1_25: nan - f1_26: nan - f1_27: nan - f1_28: nan - f1_30: nan - f1_31: nan - f1_32: nan - f1_33: nan - f1_34: nan - f1_36: nan - f1_37: nan - f1_38: nan - f1_39: nan - f1_40: nan - f1_41: nan - f1_42: nan - f1_43: nan - f1_44: nan - f1_45: nan - val_loss: nan - val_accuracy: 0.9036 - val_f1: nan - val_f1_1: nan - val_f1_3: nan - val_f1_4: nan - val_f1_5: nan - val_f1_6: nan - val_f1_7: nan - val_f1_9: nan - val_f1_10: nan - val_f1_11: nan - val_f1_12: nan - val_f1_13: nan - val_f1_14: nan - val_f1_15: nan - val_f1_16: nan - val_f1_17: nan - val_f1_18: nan - val_f1_20: nan - val_f1_21: nan - val_f1_22: nan - val_f1_23: nan - val_f1_24: nan - val_f1_25: nan - val_f1_26: nan - val_f1_27: nan - val_f1_28: nan - val_f1_30: nan - val_f1_31: nan - val_f1_32: nan - val_f1_33: nan - val_f1_34: nan - val_f1_36: nan - val_f1_37: nan - val_f1_38: nan - val_f1_39: nan - val_f1_40: nan - val_f1_41: nan - val_f1_42: nan - val_f1_43: nan - val_f1_44: nan - val_f1_45: nan\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - 138s 9s/step - loss: nan - accuracy: 0.9031 - f1: nan - f1_1: nan - f1_3: nan - f1_4: nan - f1_5: nan - f1_6: nan - f1_7: nan - f1_9: nan - f1_10: nan - f1_11: nan - f1_12: nan - f1_13: nan - f1_14: nan - f1_15: nan - f1_16: nan - f1_17: nan - f1_18: nan - f1_20: nan - f1_21: nan - f1_22: nan - f1_23: nan - f1_24: nan - f1_25: nan - f1_26: nan - f1_27: nan - f1_28: nan - f1_30: nan - f1_31: nan - f1_32: nan - f1_33: nan - f1_34: nan - f1_36: nan - f1_37: nan - f1_38: nan - f1_39: nan - f1_40: nan - f1_41: nan - f1_42: nan - f1_43: nan - f1_44: nan - f1_45: nan - val_loss: nan - val_accuracy: 0.9036 - val_f1: nan - val_f1_1: nan - val_f1_3: nan - val_f1_4: nan - val_f1_5: nan - val_f1_6: nan - val_f1_7: nan - val_f1_9: nan - val_f1_10: nan - val_f1_11: nan - val_f1_12: nan - val_f1_13: nan - val_f1_14: nan - val_f1_15: nan - val_f1_16: nan - val_f1_17: nan - val_f1_18: nan - val_f1_20: nan - val_f1_21: nan - val_f1_22: nan - val_f1_23: nan - val_f1_24: nan - val_f1_25: nan - val_f1_26: nan - val_f1_27: nan - val_f1_28: nan - val_f1_30: nan - val_f1_31: nan - val_f1_32: nan - val_f1_33: nan - val_f1_34: nan - val_f1_36: nan - val_f1_37: nan - val_f1_38: nan - val_f1_39: nan - val_f1_40: nan - val_f1_41: nan - val_f1_42: nan - val_f1_43: nan - val_f1_44: nan - val_f1_45: nan\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - 139s 9s/step - loss: nan - accuracy: 0.9031 - f1: nan - f1_1: nan - f1_3: nan - f1_4: nan - f1_5: nan - f1_6: nan - f1_7: nan - f1_9: nan - f1_10: nan - f1_11: nan - f1_12: nan - f1_13: nan - f1_14: nan - f1_15: nan - f1_16: nan - f1_17: nan - f1_18: nan - f1_20: nan - f1_21: nan - f1_22: nan - f1_23: nan - f1_24: nan - f1_25: nan - f1_26: nan - f1_27: nan - f1_28: nan - f1_30: nan - f1_31: nan - f1_32: nan - f1_33: nan - f1_34: nan - f1_36: nan - f1_37: nan - f1_38: nan - f1_39: nan - f1_40: nan - f1_41: nan - f1_42: nan - f1_43: nan - f1_44: nan - f1_45: nan - val_loss: nan - val_accuracy: 0.9036 - val_f1: nan - val_f1_1: nan - val_f1_3: nan - val_f1_4: nan - val_f1_5: nan - val_f1_6: nan - val_f1_7: nan - val_f1_9: nan - val_f1_10: nan - val_f1_11: nan - val_f1_12: nan - val_f1_13: nan - val_f1_14: nan - val_f1_15: nan - val_f1_16: nan - val_f1_17: nan - val_f1_18: nan - val_f1_20: nan - val_f1_21: nan - val_f1_22: nan - val_f1_23: nan - val_f1_24: nan - val_f1_25: nan - val_f1_26: nan - val_f1_27: nan - val_f1_28: nan - val_f1_30: nan - val_f1_31: nan - val_f1_32: nan - val_f1_33: nan - val_f1_34: nan - val_f1_36: nan - val_f1_37: nan - val_f1_38: nan - val_f1_39: nan - val_f1_40: nan - val_f1_41: nan - val_f1_42: nan - val_f1_43: nan - val_f1_44: nan - val_f1_45: nan\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - 139s 9s/step - loss: nan - accuracy: 0.9031 - f1: nan - f1_1: nan - f1_3: nan - f1_4: nan - f1_5: nan - f1_6: nan - f1_7: nan - f1_9: nan - f1_10: nan - f1_11: nan - f1_12: nan - f1_13: nan - f1_14: nan - f1_15: nan - f1_16: nan - f1_17: nan - f1_18: nan - f1_20: nan - f1_21: nan - f1_22: nan - f1_23: nan - f1_24: nan - f1_25: nan - f1_26: nan - f1_27: nan - f1_28: nan - f1_30: nan - f1_31: nan - f1_32: nan - f1_33: nan - f1_34: nan - f1_36: nan - f1_37: nan - f1_38: nan - f1_39: nan - f1_40: nan - f1_41: nan - f1_42: nan - f1_43: nan - f1_44: nan - f1_45: nan - val_loss: nan - val_accuracy: 0.9036 - val_f1: nan - val_f1_1: nan - val_f1_3: nan - val_f1_4: nan - val_f1_5: nan - val_f1_6: nan - val_f1_7: nan - val_f1_9: nan - val_f1_10: nan - val_f1_11: nan - val_f1_12: nan - val_f1_13: nan - val_f1_14: nan - val_f1_15: nan - val_f1_16: nan - val_f1_17: nan - val_f1_18: nan - val_f1_20: nan - val_f1_21: nan - val_f1_22: nan - val_f1_23: nan - val_f1_24: nan - val_f1_25: nan - val_f1_26: nan - val_f1_27: nan - val_f1_28: nan - val_f1_30: nan - val_f1_31: nan - val_f1_32: nan - val_f1_33: nan - val_f1_34: nan - val_f1_36: nan - val_f1_37: nan - val_f1_38: nan - val_f1_39: nan - val_f1_40: nan - val_f1_41: nan - val_f1_42: nan - val_f1_43: nan - val_f1_44: nan - val_f1_45: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#History f1 for class"
      ],
      "metadata": {
        "id": "nHVbDXRvzUxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_val_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_val_list[i] = history.history['val_f1_{}'.format(i)][-1]\n",
        "f1_list = np.zeros(len(tag2index))\n",
        "for i in no_punct_indexes:\n",
        "  f1_list[i] = history.history['f1_{}'.format(i)][-1]"
      ],
      "metadata": {
        "id": "P0iqPN0rzYz3"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index2tag = {value : key for (key, value) in tag2index.items()}\n",
        "index2tag"
      ],
      "metadata": {
        "id": "_QIMWiCRzbGO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c4ec1da-76dc-47c2-bbe7-df59fb265656"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '-PAD-',\n",
              " 1: 'NNP',\n",
              " 2: '``',\n",
              " 3: 'CC',\n",
              " 4: 'PRP',\n",
              " 5: 'JJ',\n",
              " 6: 'PRP$',\n",
              " 7: 'VBD',\n",
              " 8: \"''\",\n",
              " 9: '$',\n",
              " 10: '#',\n",
              " 11: 'LS',\n",
              " 12: 'CD',\n",
              " 13: 'JJR',\n",
              " 14: 'RBR',\n",
              " 15: 'WDT',\n",
              " 16: 'EX',\n",
              " 17: 'WP',\n",
              " 18: 'FW',\n",
              " 19: ',',\n",
              " 20: 'SYM',\n",
              " 21: 'UH',\n",
              " 22: 'RBS',\n",
              " 23: 'IN',\n",
              " 24: 'PDT',\n",
              " 25: 'VBG',\n",
              " 26: 'VB',\n",
              " 27: 'VBP',\n",
              " 28: 'RP',\n",
              " 29: ':',\n",
              " 30: 'NNS',\n",
              " 31: '-LRB-',\n",
              " 32: 'POS',\n",
              " 33: 'NN',\n",
              " 34: 'RB',\n",
              " 35: '.',\n",
              " 36: 'DT',\n",
              " 37: 'NNPS',\n",
              " 38: 'JJS',\n",
              " 39: 'MD',\n",
              " 40: 'TO',\n",
              " 41: '-RRB-',\n",
              " 42: 'VBZ',\n",
              " 43: 'WRB',\n",
              " 44: 'VBN',\n",
              " 45: 'WP$'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- F1: {}'.format(index2tag[i], f1_list[i]))"
      ],
      "metadata": {
        "id": "eWJPvqyZzdCE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd61ee8d-1cef-46d4-e014-bbfd06dbd34b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: NNP --- F1: 0.20244266092777252\n",
            "Tag: CC --- F1: 0.0\n",
            "Tag: PRP --- F1: 0.0\n",
            "Tag: JJ --- F1: 0.12329528480768204\n",
            "Tag: PRP$ --- F1: 0.005208331160247326\n",
            "Tag: VBD --- F1: 0.0632685050368309\n",
            "Tag: $ --- F1: 0.0\n",
            "Tag: # --- F1: 0.0\n",
            "Tag: LS --- F1: 0.0\n",
            "Tag: CD --- F1: 0.05850984528660774\n",
            "Tag: JJR --- F1: 0.0\n",
            "Tag: RBR --- F1: 0.0\n",
            "Tag: WDT --- F1: 0.008425826206803322\n",
            "Tag: EX --- F1: 0.0\n",
            "Tag: WP --- F1: 0.0\n",
            "Tag: FW --- F1: 0.0\n",
            "Tag: SYM --- F1: 0.0\n",
            "Tag: UH --- F1: 0.0\n",
            "Tag: RBS --- F1: 0.0\n",
            "Tag: IN --- F1: 0.0\n",
            "Tag: PDT --- F1: 0.0\n",
            "Tag: VBG --- F1: 0.03524082899093628\n",
            "Tag: VB --- F1: 0.052886269986629486\n",
            "Tag: VBP --- F1: 0.0\n",
            "Tag: RP --- F1: 0.0\n",
            "Tag: NNS --- F1: 0.1212531253695488\n",
            "Tag: -LRB- --- F1: 0.0\n",
            "Tag: POS --- F1: 0.0\n",
            "Tag: NN --- F1: 0.23830042779445648\n",
            "Tag: RB --- F1: 0.06477608531713486\n",
            "Tag: DT --- F1: 0.16692525148391724\n",
            "Tag: NNPS --- F1: 0.0\n",
            "Tag: JJS --- F1: 0.0\n",
            "Tag: MD --- F1: 0.0\n",
            "Tag: TO --- F1: 0.04744458571076393\n",
            "Tag: -RRB- --- F1: 0.0\n",
            "Tag: VBZ --- F1: 0.0\n",
            "Tag: WRB --- F1: 0.0\n",
            "Tag: VBN --- F1: 0.04426443204283714\n",
            "Tag: WP$ --- F1: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in no_punct_indexes:\n",
        "  print('Tag: {} --- Val_F1: {}'.format(index2tag[i], f1_val_list[i]))"
      ],
      "metadata": {
        "id": "H4o57lkyzeUg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "367390ea-dbee-42ea-ef11-ebe0fc157250"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: NNP --- Val_F1: 0.17277957499027252\n",
            "Tag: CC --- Val_F1: 0.0\n",
            "Tag: PRP --- Val_F1: 0.0\n",
            "Tag: JJ --- Val_F1: 0.11727093905210495\n",
            "Tag: PRP$ --- Val_F1: 0.0\n",
            "Tag: VBD --- Val_F1: 0.05787144973874092\n",
            "Tag: $ --- Val_F1: 0.0\n",
            "Tag: # --- Val_F1: 0.0\n",
            "Tag: LS --- Val_F1: 0.0\n",
            "Tag: CD --- Val_F1: 0.0777989998459816\n",
            "Tag: JJR --- Val_F1: 0.0\n",
            "Tag: RBR --- Val_F1: 0.0\n",
            "Tag: WDT --- Val_F1: 0.0\n",
            "Tag: EX --- Val_F1: 0.0\n",
            "Tag: WP --- Val_F1: 0.0\n",
            "Tag: FW --- Val_F1: 0.0\n",
            "Tag: SYM --- Val_F1: 0.0\n",
            "Tag: UH --- Val_F1: 0.0\n",
            "Tag: RBS --- Val_F1: 0.0\n",
            "Tag: IN --- Val_F1: 0.0\n",
            "Tag: PDT --- Val_F1: 0.0\n",
            "Tag: VBG --- Val_F1: 0.03300979360938072\n",
            "Tag: VB --- Val_F1: 0.06465950608253479\n",
            "Tag: VBP --- Val_F1: 0.0\n",
            "Tag: RP --- Val_F1: 0.0\n",
            "Tag: NNS --- Val_F1: 0.12882493436336517\n",
            "Tag: -LRB- --- Val_F1: 0.0\n",
            "Tag: POS --- Val_F1: 0.0\n",
            "Tag: NN --- Val_F1: 0.2533676326274872\n",
            "Tag: RB --- Val_F1: 0.06300192326307297\n",
            "Tag: DT --- Val_F1: 0.1696164458990097\n",
            "Tag: NNPS --- Val_F1: 0.0\n",
            "Tag: JJS --- Val_F1: 0.0\n",
            "Tag: MD --- Val_F1: 0.0\n",
            "Tag: TO --- Val_F1: 0.05304573476314545\n",
            "Tag: -RRB- --- Val_F1: 0.0\n",
            "Tag: VBZ --- Val_F1: 0.0\n",
            "Tag: WRB --- Val_F1: 0.0\n",
            "Tag: VBN --- Val_F1: 0.05027509108185768\n",
            "Tag: WP$ --- Val_F1: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Infos"
      ],
      "metadata": {
        "id": "Z6_SvLtQzfxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#40 epochs, batch 128"
      ],
      "metadata": {
        "id": "9_XqTndszhdN"
      }
    }
  ]
}